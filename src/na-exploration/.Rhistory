x = data.frame(Infg=seq(-10, 35, 0.5))
head(predict(cyto.lda, x)$posterior)
cyto.LDA.A = predict(cyto.lda, x)$posterior[,1] # posterior probability for class A
cyto.LDA.B = predict(cyto.lda, x)$posterior[,2] # posterior probability for class B
predict(cyto.lda, x)$class
lines(x[,1], cyto.LDA.A, type='l', col='blue', xlab='x', ylab='estimated posterior',
main="LDA")
lines(x[,1], cyto.LDA.B, type='l', col='red')
abline(h = 0.5)
#! We can control the cost of misclassification by moving this line above or below
#! Like to be sure when we assing obs to certain groups
abline(h=0.7, col="gray",lty="dashed")
abline(h=0.2, col="gray",lty="dashed")
text(x=28,y=0.22,"other possible tresholds",cex=0.5, col="gray")
text(x=28,y=0.72,"other possible tresholds",cex=0.5, col="gray")
legend(-10, 0.9, legend=c('P(A|X=x)', 'P(B|X=x)'), fill=c('blue','red'), cex = 0.7)
# set prior probabilities
# Pay attention to the order in which the levels appear in factor(group)!
cyto.lda.1 = lda(group ~ Infg, prior=c(0.95,0.05))
#! The priors are specified in the order of the factor levels
factor(cyto$group)
unique(cyto$group)
cyto.lda.1
x = data.frame(Infg=seq(-10, 35, 0.5))
cyto.LDA.A.1 = predict(cyto.lda.1, x)$posterior[,1] # posterior probability for class A
cyto.LDA.B.1 = predict(cyto.lda.1, x)$posterior[,2] # posterior probability for class B
plot  (x[,1], cyto.LDA.A.1, type='l', col='blue', xlab='x', ylab='estimated posterior',
main="LDA", ylim=c(0,1))
points(x[,1], cyto.LDA.B.1, type='l', col='red')
abline(h = 0.5)
legend(-10, 0.9, legend=c('P(A|X=x)', 'P(B|X=x)'), fill=c('blue','red'), cex = 0.7)
points(Infg[A], rep(0, length(A)), pch=16, col='blue')
points(Infg[B], rep(0, length(B)), pch=16, col='red')
points(x[,1], cyto.LDA.A, type='l', col='grey')
points(x[,1], cyto.LDA.B, type='l', col='grey')
#! B is red group. So with those priors we want "much more proof" to say that
#! an obs is red. In fact priors were 95% defaulted to A
### k-nearest neighbor classifier
###-------------------------------
library(class)
# help(knn)
#! Very simple and intuitive to implement, and can be done on any kind of data
#! as long as we can define on them a metric, or well just a similarity function
#! But - everything depends on that choice for the dissimilarity
#!     - and there is the curse of dimensionality
#! (1) A common function: the edit one, how many steps are required to go from
#! one object to another. But is very computationally heavy.
#! (2) So maybe we can think of mapping our objects in R^p, in some way, and then
#! use the natural euclidean distance
Infg #! train
x$Infg #! test
#! We cant choice the distance function however, with this package
cyto.knn = knn(train = Infg, test = x, cl = group, k = 3, prob=T)
#! train data, test data,
#!      number of neighbours for the choice, and posterior probs for classifying
cyto.knn.class = cyto.knn == 'B'
cyto.knn.B = ifelse(cyto.knn.class==1,
attributes(cyto.knn)$prob,
1 - attributes(cyto.knn)$prob)
plot(x[,1], cyto.LDA.B, type='l', col='red', lty=2, xlab='x', ylab='estimated posterior')
points(x[,1], cyto.knn.B, type='l', col='black', lty=1)
abline(h = 0.5)
legend(-10, 0.75, legend=c('LDA','knn'), lty=c(2,1), col=c('red','black'))
#! The steps are because we have 2 neighbours of one type, and 1 of the other
#! (we choose k=3 in the call of knn)
# let's change k
par(mfrow=c(3,4))
for(k in 1:12)
{
cyto.knn = knn(train = Infg, test = x, cl = group, k = k, prob=T)
cyto.knn.class = (cyto.knn == 'B')+0
cyto.knn.B = ifelse(cyto.knn.class==1, attributes(cyto.knn)$prob,
1 - attributes(cyto.knn)$prob)
plot(x[,1], cyto.LDA.B, type='l', col='red', lty=2, xlab='x',
ylab='estimated posterior', main=k)
points(x[,1], cyto.knn.B, type='l', col='black', lty=1, lwd=2)
abline(h = 0.5)
}
#! k=1 will too much fit the noise, while k=6 seems nice, really suits the lda
#! But the actual choice can be done with also the # help of cross validation, doing
#! tests on different values
#! Anyway this plot can be done only in the univariate case
#! WARNING: ok tune parameters ecc, but the test set should be used really at the end
#! the moment before delivering our model. Otherwise we will tune to it, overfit it.
#! CioÃ¨ staremmo aggiustando il modello sul test set, but so we will be adapting to it
#! e non va bene. So tests set should be used only once.
detach(cyto)
#_______________________________________________________________________________
### Example 2 (3 classes, bivariate)
###-------------------------------------------
# help(iris)
### We consider only the first two variables, Sepal.Length and Sepal.Width
### (p=2 features, g=3 groups); we aim to build a classifier based on the
### characteristic of the sepal that identifies the iris species.
attach(iris)
species.name = factor(Species)
g = 3
i1 = which(species.name == 'setosa')
i2 = which(species.name == 'versicolor')
i3 = which(species.name == 'virginica')
n1 = length(i1)
n2 = length(i2)
n3 = length(i3)
n = n1 + n2 + n3
detach(iris)
iris2 = iris[,1:2]
#! unique on a dataset removes duplicate lines
dim(unique(iris2))
dim(iris2)
#! We have many duplicates, and so on plots some points will overlap
#! We can fix this by Jittering, adding a small noise to every data point
#! to recover all data to be different.
#! In general is useful to do jittering on our data, to see if our analysis
#! is robust. Ie if it works also on the new small perturbed data, or it is fragile
#! and as soon as we change data it doesnt work anymore.
#! To understand how much trust the data. As our analysis are always data driven
set.seed(1)
iris2 = iris2 + cbind(rnorm(150, sd=0.025)) # jittering
# plot the data
# cols = hcl.colors(3, palette = hcl.pals()[8])
cols = c("red","green","blue")
plot(iris2, main='Iris Sepal', xlab='Sepal.Length', ylab='Sepal.Width', pch=19)
points(iris2[i1,], col=cols[1], pch=19)
points(iris2[i2,], col=cols[2], pch=19)
points(iris2[i3,], col=cols[3], pch=19)
legend("topright", legend=levels(species.name), fill=c(cols))
#! It seems there is a difference in red and others. But the others two
#! instead seems closer
m =  colMeans(iris2)
m1 = colMeans(iris2[i1,])
m2 = colMeans(iris2[i2,])
m3 = colMeans(iris2[i3,])
S1 = cov(iris2[i1,])
S2 = cov(iris2[i2,])
S3 = cov(iris2[i3,])
Sp  = ((n1-1)*S1+(n2-1)*S2+(n3-1)*S3) / (n-g)
#! manova to see if the distribution of the features is different in the
#! different groups. Otherwise we wont be able to build the classifier
# One-way MANOVA (See LAB 6)
#! We should check the assumptions
fit = manova(as.matrix(iris2) ~ species.name)
summary.manova(fit, test="Wilks")
#! pvalue small -> we reject H0, ie there are differences
# Linear Discriminant Analysis (LDA)
#! We should check the assumptions, but by the scatterplot all seems legit
#! "data approx normal, and covariance similar"
lda.iris = lda(iris2, species.name)
lda.iris
#! "Coefficients of linear discriminants" as separating regions we have lines,
#! so two coefficients
#! "Proportion of trace" refers to eigvals, as LDA is derived from the spectral
#! decomposition of the covariance (?), lambda/sum of lambda
#! Ie the percentage of separation which is captured by the linear discriminant
#! Like the first is high as it should be the direction SE->NO, and from the plot
#! yes it seems to be really capturing differences in the species
#! While the second, NE->SO doesnt seem really discriminant (and in fact second
#! value is low)
# "coefficients of linear discriminants" and "proportion of trace":
# Fisher discriminant analysis.
# In particular:
# - coefficients of linear discriminants: versors of the canonical directions
#   [to be read column-wise]
# - proportion of trace: proportion of variance (between wrt to within)
#   explained by the corresponding canonical direction
Lda.iris = predict(lda.iris, iris2)
names(Lda.iris)
# Estimate of AER (actual error rate):
# 1) APER (apparent error rate)
# 2) estimate of AER by cross-validation
# 1) Compute the APER
Lda.iris$class   # assigned classes
species.name     # true labels
table(class.true=species.name, class.assigned=Lda.iris$class)
#! "confusion matrix", diagonal elements are the right ones
#! All the other outside diagonal are misclassified
errors = (Lda.iris$class != species.name)
sum(errors) #! = 15+15+1
length(species.name)
#! sum(el outside diag) / all elements
(1+15+15)/150
APER   = sum(errors)/length(species.name)
APER
#! The APER estimation is valid only if we used the computed priors from
#! the empirical ones observed in the data, ie:
# REMARK: this is correct only if we estimate the prior with the empirical
#! frequencies! Otherwise we have to attribute a proper wheight to the
#! confusion matrix, so
prior = c(1/3,1/3,1/3)
G = 3
misc = table(class.true=species.name, class.assigned=Lda.iris$class)
APER = 0
for(g in 1:G)
APER = APER + sum(misc[g,-g])/sum(misc[g,]) * prior[g]
APER
# 2) Compute the estimate of the AER by leave-one-out cross-validation
# by hand LOO CV (leave one out cross validation):
errors_CV = 0
for(i in 1:150){
#! we remove the i-th obs
LdaCV.i = lda(iris2[-i,], species.name[-i], prior=c(50,50,50)/150)
#! we compute the error
errors_CV = errors_CV + as.numeric(predict(LdaCV.i,iris2[i,])$class != species.name[i])
}
errors_CV
AERCV   = sum(errors_CV)/length(species.name)
AERCV
#! It usually is higher than APER, as APER is more optimistic
# with R:
LdaCV.iris = lda(iris2, species.name, CV=TRUE)  # specify the argument CV
LdaCV.iris$class
species.name
table(class.true=species.name, class.assignedCV=LdaCV.iris$class)
errorsCV = (LdaCV.iris$class != species.name)
errorsCV
sum(errorsCV)
AERCV   = sum(errorsCV)/length(species.name)
AERCV
# REMARK: correct only if we estimate the priors through the sample frequencies!
#! So the same as before, ie otherwise we have to wheight our computation
# Plot the partition induced by LDA
plot(iris2, main='Iris Sepal', xlab='Sepal.Length', ylab='Sepal.Width', pch=20)
points(iris2[i1,], col='red', pch=20)
points(iris2[i2,], col='green', pch=20)
points(iris2[i3,], col='blue', pch=20)
legend("topright", legend=levels(species.name), fill=c('red','green','blue'), cex=.7)
points(lda.iris$means, pch=4,col=c('red','green','blue'), lwd=2, cex=1.5)
x  = seq(min(iris[,1]), max(iris[,1]), length=200)
y  = seq(min(iris[,2]), max(iris[,2]), length=200)
xy = expand.grid(Sepal.Length=x, Sepal.Width=y)
z  = predict(lda.iris, xy)$post  # these are P_i*f_i(x,y)
#! Because we take the $post (posterior) probabilities
z1 = z[,1] - pmax(z[,2], z[,3])  # P_1*f_1(x,y)-max{P_j*f_j(x,y)}
z2 = z[,2] - pmax(z[,1], z[,3])  # P_2*f_2(x,y)-max{P_j*f_j(x,y)}
z3 = z[,3] - pmax(z[,1], z[,2])  # P_3*f_3(x,y)-max{P_j*f_j(x,y)}
# Plot the contour line of level (levels=0) of z1, z2, z3:
# P_i*f_i(x,y)-max{P_j*f_j(x,y)}=0 ie, boundary between R.i and R.j
# where j realizes the max.
contour(x, y, matrix(z1, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z2, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z3, 200), levels=0, drawlabels=F, add=T)
library(rgl)
library(mvtnorm)
open3d()
points3d(iris2[i1,1], iris2[i1,2], 0, col='red', pch=15)
points3d(iris2[i2,1], iris2[i3,2], 0, col='green', pch=15)
points3d(iris2[i3,1], iris2[i2,2], 0, col='blue', pch=15)
surface3d(x, y, dmvnorm(xy, m1, Sp)/3, alpha=0.4, color='red')
surface3d(x, y, dmvnorm(xy, m2, Sp)/3, alpha=0.4, color='green', add=T)
surface3d(x, y, dmvnorm(xy, m3, Sp)/3, alpha=0.4, color='blue', add=T)
box3d()
### Quadratic Discriminant Analysis (QDA)
###---------------------------------------
# help(qda)
#! If we believe that the boundary between our groups is not linear,
#!    - dont use LDA (move like here to QDA)
#!    - or use LDA but with transformed/added variables
qda.iris = qda(iris2, species.name)
qda.iris
Qda.iris = predict(qda.iris, iris2)
# compute the APER
Qda.iris$class
species.name
table(class.true=species.name, class.assigned=Qda.iris$class)
errorsq = (Qda.iris$class != species.name)
errorsq
APERq   = sum(errorsq)/length(species.name)
APERq
(15+13+1)/150
APER #! of LDA
# REMARK: correct only if we estimate the priors through the sample frequencies!
#! QDA shows a lower APER, does it mean is it better? No, it's just that QDA has more
#! parameters, so a more complicate method, that hence will fit more precisely our
#! train data and produce a lower APER
# Compute the estimate of the AER by leave-one-out cross-validation
QdaCV.iris = qda(iris2, species.name, CV=T)
QdaCV.iris$class
species.name
table(class.true=species.name, class.assignedCV=QdaCV.iris$class)
errorsqCV = (QdaCV.iris$class != species.name)
errorsqCV
AERqCV   = sum(errorsqCV)/length(species.name)
AERqCV
# REMARK: correct only if we estimate the priors through the sample frequencies!
AERCV #! of LDA
#! In fact actually LDA seems to outperform QDA. Focus on the AER for really
#! choosing the "best" model
# Plot the partition induced by QDA
plot(iris2, main='Iris Sepal', xlab='Sepal.Length', ylab='Sepal.Width', pch=20)
points(iris2[i1,], col='red', pch=20)
points(iris2[i2,], col='green', pch=20)
points(iris2[i3,], col='blue', pch=20)
legend("topright", legend=levels(species.name), fill=c('red','green','blue'))
points(qda.iris$means, col=c('red','green','blue'), pch=4, lwd=2, cex=1.5)
x  = seq(min(iris[,1]), max(iris[,1]), length=200)
y  = seq(min(iris[,2]), max(iris[,2]), length=200)
xy = expand.grid(Sepal.Length=x, Sepal.Width=y)
z  = predict(qda.iris, xy)$post
z1 = z[,1] - pmax(z[,2], z[,3])
z2 = z[,2] - pmax(z[,1], z[,3])
z3 = z[,3] - pmax(z[,1], z[,2])
contour(x, y, matrix(z1, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z2, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z3, 200), levels=0, drawlabels=F, add=T)
open3d()
points3d(iris2[i1,1], iris2[i1,2], 0, col='red', pch=15)
points3d(iris2[i2,1], iris2[i3,2], 0, col='green', pch=15)
points3d(iris2[i3,1], iris2[i2,2], 0, col='blue', pch=15)
surface3d(x, y, dmvnorm(xy, m1, S1) / 3, alpha=0.4, color='red')
surface3d(x, y, dmvnorm(xy, m2, S2) / 3, alpha=0.4, color='green', add=T)
surface3d(x, y, dmvnorm(xy, m3, S3) / 3, alpha=0.4, color='blue', add=T)
box3d()
### knn-classifier
###----------------
# Plot the partition induced by knn
for (kk in c(1,7,60)){
plot(iris2, xlab='Sepal.Length', ylab='Sepal.Width', pch=20,
main=paste("Iris.Sepal with k =",kk))
points(iris2[i1,], col=2, pch=20)
points(iris2[i3,], col=4, pch=20)
points(iris2[i2,], col=3, pch=20)
legend("topright", legend=levels(species.name), fill=c(2,3,4))
x  = seq(min(iris[,1]), max(iris[,1]), length=200)
y  = seq(min(iris[,2]), max(iris[,2]), length=200)
xy = expand.grid(Sepal.Length=x, Sepal.Width=y)
iris.knn = knn(train = iris2, test = xy, cl = iris$Species, k = kk)
z  = as.numeric(iris.knn)
contour(x, y, matrix(z, 200), levels=c(1.5, 2.5), drawlabels=F, add=T)
}
#! The problem of KNN is that it may overfit a lot, there are sometimes sub-regions
#! insted of clear ones as with L/QDA. But we can fix it working on k:
#!  - lower k: lots of disconnected regions as we try to capture all the close
#!   data, so there is overfit (perfect APER, bad AER)
#!  - higher k: better, we start to re-get the regions of L/QDA
#_______________________________________________________________________________
##### Problem 2 of 9/09/2009
#####--------------------------
# The vending machines of the Exxon fuel contain optical detectors able
# to measure the size of the banknotes inserted. Knowing that 0.1% of the
# 10$ banknotes in circulation are counterfeit, Exxon would like to implement a
# software to identify false 10$ banknotes, as to minimize the economic losses.
# Assuming that:
#  - both the populations of real and false banknotes follow a normal
#    distribution (with different mean and covariance matrices);
#  - accepting a false banknote leads to an economic loss of 10$;
#  - rejecting a true banknote brings a economic loss quantifiable in 5 cents;
# satisfy the following requests of the Exxon:
# a) build an appropriate classifier, estimating the unknown parameters
#    starting from the two datasets moneytrue.txt and moneyfalse.txt, containing
#    data about 100 true banknotes and 100 counterfeit banknotes (in mm).
#    Qualitatively show the two classification regions in a graph;
# b) calculate the APER of the classifier and, based on the APER, estimate the
#    expected economic damage of the classifier;
# c) what is the estimated probability that the first 10$ banknote inserted in the
#    machine is rejected?
true = read.table('Lab07/moneytrue.txt',header=TRUE)
false = read.table('Lab07/moneyfalse.txt',header=TRUE)
banknotes = rbind(true,false)
head(banknotes)
vf = factor(rep(c('true','false'),each=100), levels=c('true','false'))
plot(banknotes[,1:2], main='Banknotes', xlab='V1', ylab='V2', pch=20)
points(false, col='red', pch=20)
points(true, col='blue', pch=20)
legend('bottomleft', legend=levels(vf), fill=c('blue','red'), cex=.7)
#! Some data points may overlap (we can see the "grid of points")
#! so consider also doing jittering
#! We can see already a possible difference: the true ones are more random,
#! sparse, while the false ones are more regular
#! So we could take the ratio of the two features and make an analysis on it (like
#! distribution, MANOVA, test for the mean, ecc) instead on working on the data
#! Like
true_ratio = true$V1/true$V2
false_ratio = false$V1/false$V2
boxplot(true_ratio,false_ratio, col=c("blue","red"), main="true vs false")
shapiro.test(true_ratio)
shapiro.test(false_ratio)
#! ecc
# question a)
library(MVN)
library(mvtnorm)
library(mvnormtest)
mshapiro.test(t(true))
mshapiro.test(t(false))
#! Multivariate normality was a requirement for L/QDA
# misclassification costs
c.vf = 10
c.fv = 0.05
#prior probabilities
pf = 0.001
pt = 1-0.001
#! When we have just two classes there is the trick to combine misclassification
#! costs into the prior probabilities, as some R functions cant deal with costs
#! new priors = (old priors) adjusted by (miscl costs)
# Prior modified to account for the misclassification costs
prior.c = c(pt*c.fv/(c.vf*pf+c.fv*pt), pf*c.vf/(c.vf*pf+c.fv*pt))
prior.c
#! QDA or LDA? The text cleary say we dont have the assumption of equal covariance
#! matrices (requirement for LDA), so we must do QDA
qda.m = qda(banknotes, vf, prior=prior.c)
qda.m
plot(banknotes[,1:2], main='Banknotes', xlab='V1', ylab='V2', pch=20)
points(false, col='red', pch=20)
points(true, col='blue', pch=20)
legend('bottomleft', legend=levels(vf), fill=c('blue','red'), cex=.7)
points(qda.m$means, pch=4,col=c('red','blue') , lwd=2, cex=1.5)
x  = seq(min(banknotes[,1]), max(banknotes[,1]), length=200)
y  = seq(min(banknotes[,2]), max(banknotes[,2]), length=200)
xy = expand.grid(V1=x, V2=y)
z  = predict(qda.m, xy)$post
z1 = z[,1] - z[,2]
z2 = z[,2] - z[,1]
contour(x, y, matrix(z1, 200), levels=0, drawlabels=F, add=T)
contour(x, y, matrix(z2, 200), levels=0, drawlabels=F, add=T)
m1 = colMeans(true)
m2 = colMeans(false)
S1 = cov(true)
S2 = cov(false)
open3d()
points3d(true[,1], true[,2], 0, col='blue', pch=15)
points3d(false[,1], false[,2], 0, col='red', pch=15)
surface3d(x,y,matrix(dmvnorm(xy, m1, S1)*prior.c[1]/100 , 200), alpha=0.6, color='blue')
surface3d(x,y,matrix(dmvnorm(xy, m2, S2)*prior.c[2]/100 , 200), alpha=0.6, color='red', add=T)
box3d()
# question b)
# APER
Qda.m = predict(qda.m)
table(class.true=vf, class.assigned=Qda.m$class)
#! We had different priors so we adjust it
APER  = (2*pt+80*pf)/100
#! But dont consider costs of miscl for the APER computation!
#! Costs intervene later in the real "loss" sense
APER
#! But again is almost always better to study the AER through CR, cross validation
#! Here
# Expected economic loss:
2/100*pt*c.fv+80/100*pf*c.vf
# question c)
#! We use the total probabilities law
# P[rejected] = P[rejected | true]P[true] + P[rejected | false]P[false]
2/100*pt + 20/100*pf
###-----------------------------------
### FISHER DISCRIMINANT ANALYSIS
###-----------------------------------
### Let's change viewpoint: we look for the directions that highlight
### the discrimination among groups
### -> we look for the canonical directions
#! Often it will be an iterative procedure:
#! transform data -> LDA -> plot scores -> satisfied? / fantastic
#!                                                    \ try different transformations
#! Like polinomials, log, exp, a mix of them, ecc
# REMARK. Assumptions: homogeneity of the covariance structure
# [we relax the normality assumption]
# Let's consider again the iris dataset
attach(iris)
species.name = factor(Species, labels=c('setosa','versicolor','virginica'))
g = 3
i1 = which(species.name=='setosa')
i2 = which(species.name=='versicolor')
i3 = which(species.name=='virginica')
n1 = length(i1)
n2 = length(i2)
n3 = length(i3)
n = n1+n2+n3
detach(iris)
iris2 = iris[,1:2]
head(iris2)
set.seed(1)
iris2 = iris2 + cbind(rnorm(150, sd=0.025))    # jittering
m =  colMeans(iris2)
m1 = colMeans(iris2[i1,])
m2 = colMeans(iris2[i2,])
m3 = colMeans(iris2[i3,])
S1 = cov(iris2[i1,])
S2 = cov(iris2[i2,])
S3 = cov(iris2[i3,])
Sp  = ((n1-1)*S1+(n2-1)*S2+(n3-1)*S3)/(n-g)
# covariance between groups (estimate)
B = 1/g*(cbind(m1 - m) %*% rbind(m1 - m) +
cbind(m2 - m) %*% rbind(m2 - m) +
cbind(m3 - m) %*% rbind(m3 - m))
B
# covariance within groups (estimate)
Sp
# how many coordinates?
g = 3
p = 2
s = min(g-1,p)
s
#! The s represents the best space (here R^2) in which we could see the whole discriminants
# Matrix Sp^(-1/2)
val.Sp = eigen(Sp)$val
vec.Sp = eigen(Sp)$vec
invSp.2 = 1/sqrt(val.Sp[1])*vec.Sp[,1]%*%t(vec.Sp[,1]) +
1/sqrt(val.Sp[2])*vec.Sp[,2]%*%t(vec.Sp[,2])
invSp.2
# spectral decomposition of Sp^(-1/2) B Sp^(-1/2)
spec.dec = eigen(invSp.2 %*% B %*% invSp.2)
# first canonical coordinate
a1 = invSp.2 %*% spec.dec$vec[,1]
a1
# second canonical coordinate
a2 = invSp.2 %*% spec.dec$vec[,2]
a2
# compare with the output of lda():
lda.iris = lda(iris2, species.name)
lda.iris
lda.iris$scaling
t(lda.iris$scaling[1:2])
a1
t(lda.iris$scaling[3:4])
a2
spec.dec$val/sum(spec.dec$val)
pwd()
