# Linear Mixed Models

## Settings

```{r, setup}
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/pisa_scores_final.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
pisa_data <- read.csv(file=dataset_path)
```

```{r}
library(mvtnorm)
library(MASS)
library(car)
library(rgl)
library(leaps)
library(ISLR)
library(glmnet)
library(lme4)
library(nlmeU) ## --> for the dataset
library(nlme)  ## --> for models implementation

library(corrplot)
library(lattice)
library(plot.matrix)

library(insight)
```

```{r}
pisa_data$X <- NULL
pisa_data$SCHLTYPE <- as.factor(pisa_data$SCHLTYPE)
pisa_data$CNT <- as.factor(pisa_data$CNT)

head(pisa_data)
```

# Mixed Models

```{r}
#regressors
regressors <- names(pisa_data)
to_discard <- c("CNT", "Social.well.being", "Psychological.well.being", "SCHLTYPE")
regressors <- regressors[!regressors %in% to_discard]

#formula object
formula_str <- paste("Social.well.being", paste(regressors, collapse = "+"), sep ="~")

#viewing the target distibution
hist(pisa_data$Social.well.being)
```

## Fixed effects

### Variable selection

1.  Exhaustive search

```{r}
# Best Subset Selection: we say when we stop
nmax_covariates = 20
regfit.full = regsubsets(as.formula(formula_str), 
                         data=pisa_data, 
                         nvmax=nmax_covariates,
                         really.big = T)

reg.summary = summary(regfit.full)

# Plots
par(mfrow=c(1,3))
plot(reg.summary$rsq, xlab="Number of Variables", ylab="R-squared", type="b")
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="b")

# We want the model with max r.adj^2 so we extract the coefficients of that model
# Note: ind = how many coefficients has the model
ind = which.max(reg.summary$adjr2)
coef(regfit.full, ind)

# Graphical table of best results
par(mfrow=c(1,2))
plot(regfit.full, scale="r2", main="Exhaustive search")
plot(regfit.full, scale="adjr2", main="Exhaustive search")
```

2.  Forward and backward selection

```{r}
# Forward
regfit.fwd = regsubsets(as.formula(formula_str),
                        data=pisa_data,
                        nvmax=nmax_covariates,
                        method="forward")

# Plot
par(mfrow=c(1,3))
plot(summary(regfit.fwd)$rsq, xlab="Number of Variables", ylab="R-squared", type="b")
plot(summary(regfit.fwd)$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
plot(summary(regfit.fwd)$rss, xlab="Number of Variables", ylab="RSS", type="b")

par(mfrow=c(1,2))
plot(regfit.fwd,scale="r2",main="Forward Stepwise Selection")
plot(regfit.fwd,scale="adjr2",main="Forward Stepwise Selection")

# Backward
regfit.bwd = regsubsets(as.formula(formula_str),
                        data=pisa_data,
                        nvmax=nmax_covariates,
                        method="backward")

# Plot
par(mfrow=c(1,3))
plot(summary(regfit.bwd)$rsq, xlab="Number of Variables", ylab="R-squared", type="b")
plot(summary(regfit.bwd)$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
plot(summary(regfit.bwd)$rss, xlab="Number of Variables", ylab="RSS", type="b")

par(mfrow=c(1,2))
plot(regfit.bwd,scale="r2",main="Backward Stepwise Selection")
plot(regfit.bwd,scale="adjr2",main="Backward Stepwise Selection")
```

Comparing

```{r}
max_vars <- 15

coef(regfit.full,max_vars) # Exhaustive search
coef(regfit.fwd,max_vars) # Forward Stepwise Selection
coef(regfit.bwd,max_vars) # Backward Stepwise Selection
```

### Fitting the best model

```{r}
max_vars <- 15
vars <- names(coef(regfit.bwd,max_vars))[2:max_vars]
final_formula_str <- paste("Social.well.being",paste(vars,collapse="+"),sep="~")

fit_social_final = lm(as.formula(final_formula_str),
                      data = pisa_data)
summary(fit_social_final)
```

## Random effects

### Fitting

```{r}
ME_formula_str <- paste(final_formula_str,"(1|CNT)",sep="+")

fit_social_lmm <- lmer(ME_formula_str,
                       data=pisa_data)
```

### PVRE

```{r}
#computing PVRE
sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm))

PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
PVRE

#computing pseudo r-squared
sigma2_f <- as.numeric(get_variance_fixed(fit_social_lmm))

Rsq <- (sigma2_f+sigma2_b)/(sigma_f+sigma2_eps+sigma2_b)
Rsq

library(MuMIn)
r.squaredGLMM(fit_social_lmm)
```

### Dotplot and random effects

```{r}
rand_eff<-ranef(fit_social_lmm)
dotplot(rand_eff)
```

## Assumptions and diagnostic

```{r}
# normality of residuals
lmm = fit_social_lmm
qqnorm(resid(lmm))
qqline(resid(lmm), col='red', lwd=2)
shapiro.test(resid(lmm))

# normality of random effects
# case only interecept
qqnorm(unlist(ranef(lmm)$CNT),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(lmm)$CNT), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(lmm)~pisa_data$CNT,las=2)

res = residuals(lmm)
hist(res)
```

# ALTRO

### **4.** K-fold-cross-validation (exhaustive search)

```{r, warning=FALSE, message=FALSE}
k = 10
folds = sample(1:k,nrow(data),replace=TRUE)
table(folds)

# Function that performs the prediction for regsubsets
predict.regsubsets = function(object,newdata,id){
  form  = as.formula(object$call[[2]])
  mat   = model.matrix(form,newdata)
  coefi = coef(object,id=id)
  xvars = names(coefi)
  mat[,xvars]%*%coefi
}
p = number_of_covariates
cv.errors = matrix(NA,k,p, dimnames=list(NULL, paste(1:number_of_covariates)))

for(j in 1:k){
  best.fit = regsubsets(formula_reg,data=data[folds!=j,],nvmax=number_of_covariates,really.big=T)
  for(i in 1:p){
    pred = predict(best.fit,data[folds==j,],id=i)
    cv.errors[j,i] = mean( (data$Y[folds==j]-pred)^2 )
  }
}

cv.errors

root.mean.cv.errors = sqrt(apply(cv.errors,2,mean)) # average over the columns
root.mean.cv.errors

# Plot
plot(root.mean.cv.errors,type='b')
points(which.min(root.mean.cv.errors),
       root.mean.cv.errors[which.min(root.mean.cv.errors)], col='red',pch=19)
which.min(root.mean.cv.errors)

# Estimation on the full dataset
reg.best = regsubsets(Y~.,data=data, nvmax=number_of_covariates)
coef(reg.best,10)

```

## Inference on Betas

```{r, warning=FALSE, message=FALSE}
## Assuption: Eps ~ N(0, sigma^2)
## Test (Fisher):
##    H0: (beta1, beta2) == (0, 0)
##    H1: (beta1, beta2) != (0, 0)
r = linear_model_classic$rank - 1  # number of regressors

# needs to be specified
kk=9
linearHypothesis(linear_model, cbind(rep(0,kk),diag(kk)), rep(0,kk))
linear_model
# Comment
#   Pr(>F) = final p-value in summary(fm)
```

### Bonferroni Intervals

```{r, warning=FALSE, message=FALSE}
alpha = 0.05
n = dim(data)[1]
qT = qt(1-alpha/(2*p), n-(r+1))

vals = 1:20
C = diag(20)
Bf = c()
for(j in vals){
  Bf = rbind(Bf,
         c( (C %*%coefficients(linear_model_classic))[j]-sqrt((C %*%vcov(linear_model_classic) %*% t(C))[j,j])*qT,
            (C %*%coefficients(linear_model_classic))[j]+sqrt((C %*%vcov(linear_model_classic) %*% t(C))[j,j])*qT))
}

Bf = data.frame(Bf)
all(Bf[,1]*Bf[,2]>0)

# Generic beta_j (p, r, n, alpha generici)
# beta_j = c(coefficients(fm)[j]-sqrt(vcov(fm)[j,j])*qt(1-alpha/(2*p), n-(r+1)),
#            coefficients(fm)[j]+sqrt(vcov(fm)[j,j])*qt(1-alpha/(2*p), n-(r+1)))

# Alternatively: Bonferroni's correction
confint(linear_model_classic, level= 1-alpha/p)[vals,]
```

### Inferenza sui beta classic model

```{r}
## Assuption: Eps ~ N(0, sigma^2)
## Test (Fisher):
##    H0: (beta1, beta2) == (0, 0)
##    H1: (beta1, beta2) != (0, 0)
r = linear_model_classic$rank - 1  # number of regressors

# needs to be specified
kk=19
linearHypothesis(linear_model_classic, cbind(rep(0,kk),diag(kk)), rep(0,kk))
linear_model_classic

```
