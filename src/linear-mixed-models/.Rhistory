fattori=fattori/100
data_new=data[,linear_model_vars]
fattori=c(tech=10,
teach=10,
school=20,
student=10,
family=30
)
fattori=fattori/100
names_fattori = names(categories_variables_filtered)
values_for_predict=list()
for(i in 1:5){
temp=NULL
for(j in 1:length(categories_variables_filtered[[i]])){
covariate = data[,categories_variables_filtered[[i]][j]]
temp=c(temp,quantile(covariate,fattori[i]))
}
values_for_predict[[names_fattori[i]]]=temp
}
values_for_predict
names_fattori
data_new=data[,linear_model_vars]
fattori=c(tech=10,
teach=10,
school=20,
student=10,
family=30
)
fattori=fattori/100
names_fattori = names(categories_variables_filtered)
values_for_predict=list()
for(i in 1:5){
temp=NULL
for(j in 1:length(categories_variables_filtered[[i]])){
covariate = data[,categories_variables_filtered[[i]][j]]
temp=c(temp,quantile(covariate,fattori[i]))
}
values_for_predict[[names_fattori[i]]]=temp
}
e
values_for_predict
quantile(data$PV1MATH,0.1)
new_data = data.frame(unlist(values_for_predict))
new_data
new_data = data.frame(t(unlist(values_for_predict)))
new_data
new_data = data.frame(t(unlist(values_for_predict)),colnames=linear_model_vars)
new_data
linear_model_vars
new_data = data.frame(t(unlist(values_for_predict)),colnames=linear_model_vars)
new_data
new_data = data.frame(t(unlist(values_for_predict)),colnames=linear_model_vars,nrow=1)
new_data
linear_model_vars
new_data = data.frame(t(unlist(values_for_predict)),colnames=linear_model_vars,nrow=1)
new_data
help("data.frame")
new_data = data.frame(t(unlist(values_for_predict)),col.name=linear_model_vars)
new_data
new_data = data.frame(t(unlist(values_for_predict)),col.names=linear_model_vars)
new_data
new_data = data.frame(t(unlist(values_for_predict)),col.names=linear_model_vars)
new_data
new_data = data.frame(t(unlist(values_for_predict)))
new_data
colnames(new_data)=linear_model_vars
colnames(new_data)=linear_model_vars
new_data
levels(data$CNT)
new_data$IM_PUBLIC = is_public
new_data$IM_PUBLIC = is_public
new_data
new_data$CNT=country_chosen
fattori=fattori/100
country_chosen="CZE"
is_public = 0
new_data$IM_PUBLIC = is_public
new_data
new_data = data.frame(t(unlist(values_for_predict)))
colnames(new_data)=linear_model_vars
new_data$CNT=country_chosen
new_data$IM_PUBLIC = is_public
new_data
vars = c(bwd[2:max_vars],"CNT","IM_PUBLIC")
formula_social <- paste(paste("Social.well.being","~"), paste(vars, collapse = "+"))
formula_social
fit_social_final = lm(formula_social,data=data)
summary(fit_social_final)
par(mfrow=c(2,2))
plot(fit_social_final)
shapiro.test(fit_social_final$residuals)
vars = c(bwd[2:max_vars],"(1|NEW_VAR)")
FORMULA_LMM <- paste(paste("Social.well.being","~"), paste(vars, collapse = "+"))
FORMULA_LMM
fit_social_lmm = lmer(FORMULA_LMM,data=data)
fit_social_lmm
sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm))
PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))
## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_social_lmm, condVar=T))$NEW_VAR
predict(fit_social_lmm,new_data)
new_data = data.frame(t(unlist(values_for_predict)))
colnames(new_data)=linear_model_vars
new_data$CNT=country_chosen
new_data$IM_PUBLIC = is_public
new_data
alpha = 0.05
Pred = predict(fit_social_lmm, new_data, interval='prediction', level=1-alpha)
Pred
fit_social_lmm
levels(data$NEW_VAR)
new_data = data.frame(t(unlist(values_for_predict)))
colnames(new_data)=linear_model_vars
#new_data$CNT=country_chosen
#new_data$IM_PUBLIC = is_public
new_data$NEW_VAR = "CZE-0"
new_data
alpha = 0.05
Pred = predict(fit_social_lmm, new_data, interval='prediction', level=1-alpha)
Pred
fit_social_lmm
colnames(new_data)
names(coefficients((fit_social_lmm)))
colnames(new_data)
(coefficients((fit_social_lmm)))
colnames(coefficients((fit_social_lmm)))
colnames(new_data)
colnames(new_data)
(coefficients((fit_social_lmm)))
new_data = data.frame(t(unlist(values_for_predict)))
colnames(new_data)=linear_model_vars
#new_data$CNT=country_chosen
#new_data$IM_PUBLIC = is_public
new_data$NEW_VAR = "CZE-0"
new_data
alpha = 0.05
Pred = predict(fit_social_lmm, new_data, interval='prediction', level=1-alpha)
Pred
Pred = predict(fit_social_lmm, new_data, interval='prediction', level=1-alpha)
Pred
Pred = predict(fit_social_lmm, new_data, interval='prediction', level=1-alpha)
countries <- unique(data$CNT)
countries
cnt_comb <- combn(unique(countries),2)
cnt_comb
cnt_comb[1]
cnt_comb[1,]
cnt_comb
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/pisa_scores_final.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
data <- read.csv(file=dataset_path)
head(data)
library(mvtnorm)
library(MASS)
library(car)
library(rgl)
library(leaps)
library(ISLR)
library(glmnet)
library(lme4)
library(nlmeU) ## --> for the dataset
library(nlme)  ## --> for models implementation
library(corrplot)
library(lattice)
library(plot.matrix)
library(insight)
data$X <- NULL
data$SCHLTYPE <- as.factor(data$SCHLTYPE)
data$CNT <- as.factor(data$CNT)
IM_PUBLIC = rep(0,dim(data)[1])
IM_PUBLIC [which(data$SCHLTYPE=="Public")] = 1
data$IM_PUBLIC = as.factor(IM_PUBLIC)
data$SCHLTYPE <- NULL
# Example categorical variables
index_cnt = grep("CNT",colnames(data))
colnames(data)
levels(data[,index_cnt])=levels(data$CNT)
cat_var1 <- data[,index_cnt]
index_public= grep("IM_PUBLIC",colnames(data))
levels(data[,index_public])=levels(data$IM_PUBLIC)
cat_var2 <- data[,index_public]
# Combine categorical variables
new_var <- interaction(cat_var1, cat_var2, sep = "-")
levels(new_var)
length(levels(new_var))
data$NEW_VAR = as.factor(new_var)
colnames(data)
FORMULA_COMPLETE_SOCIAL = formula(Social.well.being~.-Psychological.well.being-CNT-IM_PUBLIC-NEW_VAR)
fit_social = lm(FORMULA_COMPLETE_SOCIAL,data=data)
summary(fit_social)
b=coefficients(fit_social)
nv_max = length(b)-1
names(b)
regfit.full = regsubsets(FORMULA_COMPLETE_SOCIAL, data=data, nvmax=nv_max)
reg.summary = summary(regfit.full)
# Which one we choose:
#reg.summary$which
# Plots
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
# We want the model with max r.adj^2 so we extract the coefficients of that model
# Note: ind = how many coefficients has the model
ind = which.max(reg.summary$adjr2)
ind
coef(regfit.full, ind)
# Forward
regfit.fwd = regsubsets(FORMULA_COMPLETE_SOCIAL,data=data,nvmax=nv_max,method="forward")
# Plot
plot(summary(regfit.fwd)$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
ind_fwd = which.max(summary(regfit.fwd)$adjr2)
ind_fwd
# Backward
regfit.bwd = regsubsets(FORMULA_COMPLETE_SOCIAL,data=data,nvmax=nv_max,method="backward")
# Plot
plot(summary(regfit.bwd)$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
ind_bwd = which.max(summary(regfit.bwd)$adjr2)
ind_bwd
max_vars = 18
full = names(coef(regfit.full,max_vars)) # Exhaustive search
fwd = names(coef(regfit.fwd,max_vars)) # Forward Stepwise Selection
bwd = names(coef(regfit.bwd,max_vars)) # Backward Stepwise Selection
all(full==bwd)
all(fwd==bwd)
bwd
vars_chosen_social <- bwd[2:max_vars]
# Salvataggio dei nomi nel file "nomi.txt"
writeLines(vars_chosen_social,"../../data/lm_social_vars.txt")
vars = c(bwd[2:max_vars],"CNT","IM_PUBLIC")
formula_social <- paste(paste("Social.well.being","~"), paste(vars, collapse = "+"))
formula_social
fit_social_final = lm(formula_social,data=data)
summary(fit_social_final)
par(mfrow=c(2,2))
plot(fit_social_final)
shapiro.test(fit_social_final$residuals)
res = fit_social_final$residuals
for (i in 1:(dim(data)[2]-1)){
plot(res,data[,i],main=colnames(data)[i])
}
vars = c(bwd[2:max_vars],"(1|NEW_VAR)")
FORMULA_LMM <- paste(paste("Social.well.being","~"), paste(vars, collapse = "+"))
FORMULA_LMM
fit_social_lmm = lmer(FORMULA_LMM,data=data)
fit_social_lmm
sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm))
PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))
## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_social_lmm, condVar=T))$NEW_VAR
# normality of residuals
fm = fit_social_lmm
qqnorm(resid(fm))
qqline(resid(fm), col='red', lwd=2)
shapiro.test(resid(fm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fm)$NEW_VAR),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fm)$NEW_VAR), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fm)~data$NEW_VAR,las=2)
res = residuals(fm)
hist(res)
shapiro.test(log(res+abs(min(res))+1))
## raw residuals
plot(fm, resid(., type = "response") ~ fitted(.)) # Raw vs. fitted
# We observe an asymmetric pattern, with large positive (negative) residuals present mainly for small (large) fitted values.
# but it can be a consequence of the fact that raw residuals are intrinsically heteroscedastic and correlated.
plot(fm, resid(., type = "response") ~ as.numeric(NEW_VAR)) # Raw vs. NEW_VAR (not shown)
bwplot(resid(fm) ~ NEW_VAR, pch = "|", data = data)
# The boxand-whiskers plots clearly show an increasing variance of the residuals.
## Pearson residuals
## Pearson residuals are obtained from the raw residuals by dividing the latter by an
## estimate of the appropriate residual standard deviation, so they should be more homoscedastic
plot(fm, resid(., type = "pearson" ) ~ fitted(.)) # Pearson vs. fitted
plot(fm,resid(., type = "pearson") ~ as.numeric(NEW_VAR))
bwplot( resid(fm, type = "pearson") ~ NEW_VAR, # Pearson vs. time.f
pch = "|", data = data)
## this plot illustrate the effect of scaling: the variance of the residuals is virtually constant.
data
colnames(data)
numerical_variables = c(1:23,25,26)
colnames(data)[numerical_variables]
data_scaled = data
data_scaled[,c(numerical_variables)] = data.frame(scale(data_scaled[,c(numerical_variables)]))
boxplot(data[,numerical_variables], main="old data")
boxplot(data_scaled[,numerical_variables],main="scaled data")
fit_social_lmm_scaled = lmer(FORMULA_LMM,data=data_scaled)
fit_social_lmm_scaled
sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm_scaled))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm_scaled))
PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))
## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_social_lmm_scaled, condVar=T))$NEW_VAR
# normality of residuals
fm = fit_social_lmm_scaled
qqnorm(resid(fm))
qqline(resid(fm), col='red', lwd=2)
shapiro.test(resid(fm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fm)$NEW_VAR),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fm)$NEW_VAR), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fm)~data$NEW_VAR,las=2)
res = residuals(fm)
hist(res)
## raw residuals
plot(fm, resid(., type = "response") ~ fitted(.)) # Raw vs. fitted
# We observe an asymmetric pattern, with large positive (negative) residuals present mainly for small (large) fitted values.
# but it can be a consequence of the fact that raw residuals are intrinsically heteroscedastic and correlated.
plot(fm, resid(., type = "response") ~ as.numeric(NEW_VAR)) # Raw vs. NEW_VAR (not shown)
bwplot(resid(fm) ~ NEW_VAR, pch = "|", data = data)
# The boxand-whiskers plots clearly show an increasing variance of the residuals.
## Pearson residuals
## Pearson residuals are obtained from the raw residuals by dividing the latter by an
## estimate of the appropriate residual standard deviation, so they should be more homoscedastic
plot(fm, resid(., type = "pearson" ) ~ fitted(.)) # Pearson vs. fitted
plot(fm,resid(., type = "pearson") ~ as.numeric(NEW_VAR))
bwplot( resid(fm, type = "pearson") ~ NEW_VAR, # Pearson vs. time.f
pch = "|", data = data)
## this plot illustrate the effect of scaling: the variance of the residuals is virtually constant.
data_woo = data # woo = [W]ith[O]ut [O]utliers
for(i in 1:(length(vars)-1) )
boxplot(data_woo[,i],main=paste("Overall,",vars[i]))
for(i in 1:(length(vars)-1) )
boxplot(data_woo[,i]~data_woo$CNT,main=paste("By state,",vars[i]),
col=colora(length(unique(data_woo$CNT))),las=2)
# la funzione colora dovrebbe essere dentro Utilities
M = colMeans(data[,numerical_variables])
S = cov(data[,numerical_variables])
d2 = matrix(mahalanobis(data[,numerical_variables], M, S))
hist(d2,breaks = 300,xlim = c(0,300))
abline(v=25,col="red")
data_woo = data[which(d2 <= 25), ]
print(paste("From",dim(data)[1],"obs we moved to",dim(data_woo)[1]))
print(paste("Percentuale di dati sopravvissuti:",dim(data_woo)[1]/dim(data)[1]*100,"%"))
# magari ci sta fare meno filtraggio overall al punto prima, cioÃ¨ lasciare la soglia
# piÃ¹ alta nella selezione sopra
# e poi fare un "raffinamento" qui, togliendo outlier relativi non al comportamento
# generale ma a quello che esce dal confronto tra gli stati
for(i in 1:(length(vars)-1) )
boxplot(data[,i],data_woo[,i],main=paste("Overall,",vars[i],"before vs after"))
for(i in 1:(length(vars)-1) )
boxplot(data_woo[,i]~data_woo$CNT,main=paste("By state,",vars[i]),
col=colora(length(unique(data_woo$CNT))),las=2)
# la funzione colora dovrebbe essere dentro Utilities
# cols = heat.colors(100)
cols = gray.colors(100)
# cols=colora(100)
image(cov(data_woo[,numerical_variables]),
breaks = quantile(cov(data_woo[,numerical_variables]),(0:100)/100),
col=cols)
image(cov(data[,numerical_variables]),
breaks = quantile(cov(data[,numerical_variables]),(0:100)/100),
col=cols)
# sembrano uguali quindi non c'Ã¨ stata molta perdita di informazioni
fit_social_lmm_woo = lmer(FORMULA_LMM,data=data_woo)
fit_social_lmm_woo
sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm_woo))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm_woo))
PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))
## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_social_lmm_woo, condVar=T))$NEW_VAR
# normality of residuals
fm = fit_social_lmm_woo
qqnorm(resid(fm))
qqline(resid(fm), col='red', lwd=2)
shapiro.test(resid(fm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fm)$NEW_VAR),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fm)$NEW_VAR), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fm)~data_woo$NEW_VAR,las=2)
res = residuals(fm)
hist(res)
minimum = min # funzione in julia, che qui si chiama solo min
# pvalue 0.0527
x1 = (data_woo$Approach.to.ICT)^2
x2 = (data_woo$Use.of.ICT)
x3 = (data_woo$Teachers..degree)^2
x4 = (data_woo$Teacher.skill)^2
x5 = (data_woo$ESCS)
x6 = (data_woo$RATCMP1)^2
x7 = (data_woo$ICTSCH)^2
x8 = (data_woo$HEDRES)^2
x9 = (data_woo$STUBEHA)^2
x10 = (data_woo$ATTLNACT)^2
x11 = (data_woo$JOYREAD)^2
x12 = (data_woo$PROAT6)
x13 = (data_woo$CLSIZE)
x14 = (data_woo$EDUSHORT)^2
x15 = (data_woo$STAFFSHORT)^2
x16 = (data_woo$PV1MATH)^2
x17 = (data_woo$PV1READ)
y = (data_woo$Psychological.well.being)
# pvalue 0.04373
x1 = (data_woo$Approach.to.ICT)^2
x2 = (data_woo$Use.of.ICT)^2
x3 = (data_woo$Teachers..degree)^2
x4 = (data_woo$Teacher.skill)^2
x5 = (data_woo$ESCS)
x6 = log((data_woo$RATCMP1)+abs(minimum((data_woo$RATCMP1)))+1)
x7 = (data_woo$ICTSCH)^2
x8 = log((data_woo$HEDRES)+abs(minimum((data_woo$HEDRES)))+1)
x9 = log((data_woo$STUBEHA)+abs(minimum((data_woo$STUBEHA)))+1)
x10 = (data_woo$ATTLNACT)^2
x11 = log((data_woo$JOYREAD)+abs(minimum((data_woo$JOYREAD)))+1)
x12 = log((data_woo$PROAT6)+abs(minimum((data_woo$PROAT6)))+1)
x13 = (data_woo$CLSIZE)
x14 = (data_woo$EDUSHORT)
x15 = (data_woo$STAFFSHORT)^2
x16 = (data_woo$PV1MATH)
x17 = (data_woo$PV1READ)^2
y = (data_woo$Psychological.well.being)
# pvalue 0.05855
x1 = (data_woo$Approach.to.ICT)^2
x2 = (data_woo$Use.of.ICT)^2
x3 = (data_woo$Teachers..degree)^2
x4 = (data_woo$Teacher.skill)^2
x5 = log((data_woo$ESCS)+abs(minimum((data_woo$ESCS)))+1)
x6 = (data_woo$RATCMP1)
x7 = (data_woo$ICTSCH)^2
x8 = (data_woo$HEDRES)^2
x9 = (data_woo$STUBEHA)^2
x10 = (data_woo$ATTLNACT)^2
x11 = (data_woo$JOYREAD)
x12 = log((data_woo$PROAT6)+abs(minimum((data_woo$PROAT6)))+1)
x13 = (data_woo$CLSIZE)
x14 = log((data_woo$EDUSHORT)+abs(minimum((data_woo$EDUSHORT)))+1)
x15 = log((data_woo$STAFFSHORT)+abs(minimum((data_woo$STAFFSHORT)))+1)
x16 = (data_woo$PV1MATH)^2
x17 = (data_woo$PV1READ)^2
y = (data_woo$Psychological.well.being)
fit_social_lmm_woo = lmer(
y ~ x1 +x2 +x3 +x4 +x5 +x6 +x7 +x8 +x9 +x10 +x11 +x12 +x13 +x14 +x15 +x16 +x17 +
+(1|data_woo$NEW_VAR)
)
fit_social_lmm_woo
sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm_woo))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm_woo))
PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))
## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_social_lmm_woo, condVar=T))
# normality of residuals
fm = fit_social_lmm_woo
qqnorm(resid(fm))
qqline(resid(fm), col='red', lwd=2)
shapiro.test(resid(fm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fm)),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fm)), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fm)~data_woo$NEW_VAR,las=2)
res = residuals(fm)
hist(res)
M = colMeans(data[,numerical_variables])
S = cov(data[,numerical_variables])
d2 = matrix(mahalanobis(data[,numerical_variables], M, S))
hist(d2,breaks = 300,xlim = c(0,300))
abline(v=50,col="red")
data_woo = data[which(d2 <= 28), ]
print(paste("From",dim(data)[1],"obs we moved to",dim(data_woo)[1]))
print(paste("Percentuale di dati sopravvissuti:",dim(data_woo)[1]/dim(data)[1]*100,"%"))
data_woo_scaled = data_woo
data_woo_scaled[,c(numerical_variables)] =
data.frame(scale(data_woo_scaled[,c(numerical_variables)]))
boxplot(data[,numerical_variables], main="old data",las=2)
boxplot(data_woo[,numerical_variables], main="woo data",las=2)
boxplot(data_woo_scaled[,numerical_variables],main="scaled and woo data",las=2)
# magari dare qualche spuntatina manuale a proat6 e ratcmp1 che sembrano le peggiori
boxplot(data_woo_scaled[,numerical_variables],main="scaled and woo data",las=2)
old_dim = dim(data_woo_scaled)[1]
data_woo_scaled = data_woo_scaled[which(data_woo_scaled$PROAT6<4),]
data_woo_scaled = data_woo_scaled[which(data_woo_scaled$RATCMP1<4),]
data_woo_scaled = data_woo_scaled[which(data_woo_scaled$LM_MINS<4.1),]
new_dim = dim(data_woo_scaled)[1]
boxplot(data_woo_scaled[,numerical_variables],main="scaled and woo data, after spuntatina",las=2)
print(paste("From",old_dim,"obs we moved to",new_dim))
print(paste("Percentuale di dati sopravvissuti:",new_dim/old_dim*100,"%"))
write.csv(data_woo_scaled,"../../data/pisa_score_woo.csv")
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/pisa_score_woo.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
data <- read.csv(file=dataset_path)
head(data)
data$SCHLTYPE <- as.factor(data$SCHLTYPE)
data$X <- NULL
data$CNT <- as.factor(data$CNT)
data
