---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---
```{r}
library(mvtnorm)
library(MASS)
library(car)
library(rgl)
library(leaps)
library(ISLR)
library(glmnet)
library(lme4)
library(nlmeU) ## --> for the dataset
library(nlme)  ## --> for models implementation

library(corrplot)
library(lattice)
library(plot.matrix)

library(insight)

library(MuMIn)
```

```{r}
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/data_social_woo.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
include_path_2 = paste(root_proj_dir,"data/data_psych_woo.csv",sep="")

#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
data_socio <- read.csv(dataset_path)
data_psycho <- read.csv(file=include_path_2) 
```


# [0] DATA PREP
```{r}
data=data_socio
head(data)
```


```{r}
data$X <- NULL
data$CNT <- as.factor(data$CNT)
data$IM_PUBLIC <- as.factor(data$IM_PUBLIC)
data$NEW_VAR <- as.factor(data$NEW_VAR)
```

# SELECTED COVARIATES
```{r}
linear_model_vars <- readLines("src/linear-mixed-models/lm_social_vars.txt")
```

# GROUPS OF VARIABLES- ignorabile
```{r}
filter <- function(category,var_lm) {
  risultato <- c()
  
  for (elemento in category) {
    if (elemento %in% var_lm) {
      risultato <- c(risultato, elemento)
    }
  }
  
  return(risultato)
}


categories_variables_filtered=list()

for(cat in cat_var_names ){
  categories_variables_filtered[[cat]]=filter(categories_variables[[cat]],linear_model_vars)
}

categories_variables_filtered
```

# Modello base-lineare semplice
```{r}
vars = c(linear_model_vars,"CNT","IM_PUBLIC")
formula_social <- paste(paste("Social.well.being","~"), paste(vars, collapse = "+"))
formula_social

fit_social_final = lm(formula_social,data=data)
summary(fit_social_final)
```


#  LMM solo intercetta
```{r}
#vars = c(bwd[2:max_vars],"(1|NEW_VAR)")
vars=c(vars,"(1|NEW_VAR)")
FORMULA_LMM <- paste(paste("Social.well.being","~"), paste(vars, collapse = "+"))
FORMULA_LMM

fit_social_lmm = lmer(FORMULA_LMM,data=data)
fit_social_lmm

sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm))

PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))

## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_social_lmm, condVar=T))$NEW_VAR
```

Ciclo for per aggiungere le varie slopes
```{r}
#ciclare sulle variabili per vedere quale miglior PVRE come slope rispetto a sola intercetta NEW_VAR
#confronto tra modello con solo intercetta e modello con una slope
#codice abbastanza flessibile per aggiungere ulteriori slopes

n=length(vars)
PVRE_slo=rep(0,n-3)

#dà warning e richiede di scalare variabili
scaled_data=data
ind=sapply(data, is.numeric)
scaled_data[ind]=lapply(data[ind], scale)

#affinchè anche il modello di partenza sia calcolato sul medesimo dataset scalato
fit_social_lmm_scaled = lmer(FORMULA_LMM,data=scaled_data)


for(i in 1:(n-3)){    #-3 per togliere CNT,IM_PUBLIC,NEW_VAR (già in NEW_VAR)
  slopevar=vars[i]
  varsfor=c(vars,paste("(",paste(slopevar),"|NEW_VAR)",collapse=""))
  FORMULA_LMM_slo <- paste(paste("Social.well.being","~"), paste(varsfor, collapse = "+"))
  FORMULA_LMM_slo

  fit_social_lmm_slo = lmer(FORMULA_LMM_slo,data=scaled_data)
  fit_social_lmm_slo
  
  #lasciare pvre a 0 se singular (random non ha effetto significativo su modello, non vale la pena considerarlo)
  if(!isSingular(fit_social_lmm_slo, tol = 1e-4)){
    sigma2_eps_slo <- as.numeric(get_variance_residual(fit_social_lmm_slo))
    sigma2_b_slo <- as.numeric(get_variance_random(fit_social_lmm_slo))

    PVRE_slo[i] <- sigma2_b_slo/(sigma2_b_slo+sigma2_eps_slo)
  }
  
  print(paste("Modello con slope su ",slopevar))
  
  #per pvalues di anova tra modello con solo intercetta e modello con slope
  comp=anova(fit_social_lmm_scaled, fit_social_lmm_slo)$'Pr(>Chisq)'
  
  print(paste("Rsquared",r.squaredGLMM(fit_social_lmm_slo)))
  #prints marginal and conditional Rsquared
  
  print(paste("PVRE =",PVRE_slo[i]))
  print(paste("P-valueAnova =",comp))

}


#pvre -> anche quelli non a 0 molto molto bassi 
#random slope non migliora situazione varianza spiegata
#alcuni modelli sembrano addirittura non convergere

#anova invece sembra segnalare miglioramento per alcuni modelli
#(pvalue che conta è il secondo)
#ATTLNACT, JOYREAD e PV1READ sembrano promettenti

#molti warning per singolarità e refitting per fare anova
#su internet dice potrebbe dipendere da complessità modello (?)

```








### assumptions check
```{r}
# normality of residuals
fm = fit_social_lmm
qqnorm(resid(fm))
qqline(resid(fm), col='red', lwd=2)
shapiro.test(resid(fm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fm)$NEW_VAR),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fm)$NEW_VAR), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fm)~data$NEW_VAR,las=2)


## raw residuals
plot(fm, resid(., type = "response") ~ fitted(.)) # Raw vs. fitted
# We observe an asymmetric pattern, with large positive (negative) residuals present mainly for small (large) fitted values.
# but it can be a consequence of the fact that raw residuals are intrinsically heteroscedastic and correlated.

plot(fm, resid(., type = "response") ~ as.numeric(NEW_VAR)) # Raw vs. NEW_VAR (not shown)
bwplot(resid(fm) ~ NEW_VAR, pch = "|", data = data)
# The boxand-whiskers plots clearly show an increasing variance of the residuals.

## Pearson residuals
## Pearson residuals are obtained from the raw residuals by dividing the latter by an
## estimate of the appropriate residual standard deviation, so they should be more homoscedastic


plot(fm, resid(., type = "pearson" ) ~ fitted(.)) # Pearson vs. fitted
plot(fm,resid(., type = "pearson") ~ as.numeric(NEW_VAR))
bwplot( resid(fm, type = "pearson") ~ NEW_VAR, # Pearson vs. time.f
        pch = "|", data = data)
## this plot illustrate the effect of scaling: the variance of the residuals is virtually constant.
```

