---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

```{r, setup}
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/pisa_scores_final.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
data <- read.csv(file=dataset_path)
```

```{r}
library(mvtnorm)
library(MASS)
library(car)
library(rgl)
library(leaps)
library(ISLR)
library(glmnet)
library(lme4)
library(nlmeU) ## --> for the dataset
library(nlme)  ## --> for models implementation

library(corrplot)
library(lattice)
library(plot.matrix)

library(insight)
```

```{r}
data=data
head(data)
colnames(data)
data$X <- NULL
data$SCHLTYPE <- as.factor(data$SCHLTYPE)
data$CNT <- as.factor(data$CNT)
```


# Modello 1
```{r}
fit_social = lm(Social.well.being ~ Approach.to.ICT+Use.of.ICT+Teachers..degree+Teacher.skill+ESCS+RATCMP1+ICTSCH+ICTRES+ENTUSE+LM_MINS+HEDRES+STUBEHA+ATTLNACT+
JOYREAD+PROAT6+TEACHBEHA+STRATIO+CLSIZE+EDUSHORT+STAFFSHORT+PV1MATH+
PV1READ+CREACTIV+SCHLTYPE,data=data)

summary(fit_social)
```

## Cose noiose
```{r}
# test su
# ICTRES
# ENTUSE
# LM_MINS

fit = fit_social
b=coefficients(fit)
check_beta <- c(
	grep("ICTRES", names(b)),
	grep("ENTUSE", names(b)),
	grep("LM_MINS", names(b))
)

C <- c()
for( i in 1: length(check_beta)){
  vect <- rep(0,fit$rank)
  vect[check_beta[i]] <- 1
  C <- rbind(C,vect)
}
linearHypothesis(fit, C, rep(0,length(check_beta)))
# loro si tolgono

fit_social_2 = lm(Social.well.being ~ Approach.to.ICT+Use.of.ICT+Teachers..degree+Teacher.skill+ESCS+RATCMP1+ICTSCH+HEDRES+STUBEHA+ATTLNACT+
JOYREAD+PROAT6+TEACHBEHA+STRATIO+CLSIZE+EDUSHORT+STAFFSHORT+PV1MATH+
PV1READ+CREACTIV+SCHLTYPE,data=data)

summary(fit_social_2)
IM_PUBLIC = rep(0,dim(data)[1])
IM_PUBLIC [which(data$SCHLTYPE=="Public")] = 1

fit_social_3 = lm(Social.well.being ~ Approach.to.ICT+Use.of.ICT+Teachers..degree+Teacher.skill+ESCS+RATCMP1+ICTSCH+HEDRES+STUBEHA+ATTLNACT+
JOYREAD+PROAT6+TEACHBEHA+STRATIO+CLSIZE+EDUSHORT+STAFFSHORT+PV1MATH+
PV1READ+CREACTIV+IM_PUBLIC,data=data)

summary(fit_social_3)

fit = fit_social
b=coefficients(fit)
check_beta <- c(
	grep("ICTRES", names(b)),
	grep("ENTUSE", names(b)),
	grep("LM_MINS", names(b)),
	grep("TEACHBEHA", names(b)),
	grep("EDUSHORT", names(b))
)

C <- c()
for( i in 1: length(check_beta)){
  vect <- rep(0,fit$rank)
  vect[check_beta[i]] <- 1
  C <- rbind(C,vect)
}
linearHypothesis(fit, C, rep(0,length(check_beta)))
# loro si tolgono


fit_social_4 = lm(Social.well.being ~ Approach.to.ICT+Use.of.ICT+Teachers..degree+Teacher.skill+ESCS+RATCMP1+ICTSCH+HEDRES+STUBEHA+ATTLNACT+
JOYREAD+PROAT6+STRATIO+CLSIZE+STAFFSHORT+PV1MATH+
PV1READ+CREACTIV+IM_PUBLIC,data=data)

summary(fit_social_4)


fit = fit_social
b=coefficients(fit)
check_beta <- c(
	grep("ICTRES", names(b)),
	grep("ENTUSE", names(b)),
	grep("LM_MINS", names(b)),
	grep("TEACHBEHA", names(b)),
	grep("EDUSHORT", names(b)),
	grep("STUBEHA", names(b)),
	grep("JOYREAD", names(b)),
	grep("STRATIO", names(b))
)

C <- c()
for( i in 1: length(check_beta)){
  vect <- rep(0,fit$rank)
  vect[check_beta[i]] <- 1
  C <- rbind(C,vect)
}
linearHypothesis(fit, C, rep(0,length(check_beta)))
# loro si tolgono



fit_social_5 = lm(Social.well.being ~ Approach.to.ICT+Use.of.ICT+Teachers..degree+Teacher.skill+ESCS+RATCMP1+ICTSCH+HEDRES+ATTLNACT+PROAT6+CLSIZE+STAFFSHORT+PV1MATH+
PV1READ+CREACTIV+IM_PUBLIC,data=data)
summary(fit_social_5)

fit = fit_social
b=coefficients(fit)
check_beta <- c(
	grep("ICTRES", names(b)),
	grep("ENTUSE", names(b)),
	grep("LM_MINS", names(b)),
	grep("TEACHBEHA", names(b)),
	grep("EDUSHORT", names(b)),
	grep("STUBEHA", names(b)),
	grep("JOYREAD", names(b)),
	grep("STRATIO", names(b)),
	grep("ESCS", names(b)),
	grep("PROAT6", names(b)),
	grep("RATCMP1", names(b))
)

C <- c()
for( i in 1: length(check_beta)){
  vect <- rep(0,fit$rank)
  vect[check_beta[i]] <- 1
  C <- rbind(C,vect)
}
linearHypothesis(fit, C, rep(0,length(check_beta)))
# loro si tolgono


fit_social_5 = lm(Social.well.being ~ Approach.to.ICT+Use.of.ICT+Teachers..degree+Teacher.skill+RATCMP1+ICTSCH+HEDRES+ATTLNACT+CLSIZE+STAFFSHORT+PV1MATH+
PV1READ+CREACTIV+IM_PUBLIC,data=data)
summary(fit_social_5)


```

## SALTA QUI :)
```{r}
FORMULA = formula(Social.well.being ~ Approach.to.ICT+Use.of.ICT+Teachers..degree+Teacher.skill+RATCMP1+ICTSCH+HEDRES+ATTLNACT+CLSIZE+STAFFSHORT+PV1MATH+
PV1READ+CREACTIV+IM_PUBLIC)
fit_social = lm(FORMULA,data=data)
summary(fit_social)
```


```{r}
par(mfrow=c(2,2))
plot(fit_social)
shapiro.test(fit_social$residuals)
```


```{r}
res = fit_social$residuals
for (i in 1:(dim(data)[2]-1)){
	plot(res,data[,i],main=colnames(data)[i])
}
```
```{r}
x11()
hist(data$Teachers..degree)
```


## Lasso
```{r}
LAMBDA_GRID = 10^seq(-10,1,by=0.1)

x = model.matrix(FORMULA,data=data)[,-1]
# Build the vector of response/target
y = data$Social.well.being
lambda.grid = LAMBDA_GRID
fit.lasso = glmnet(x,y, lambda = lambda.grid, alpha=1)

cols = colora(dim(x)[2],1)
plot(fit.lasso, xvar='lambda', label=TRUE, col = cols)
legend('topright', dimnames(x)[[2]], col = cols, lty=1, cex=0.4)

### Select lambda with CV
cv.lasso = cv.glmnet(x,y,lambda=lambda.grid, nfolds = 100)
bestlam.lasso = cv.lasso$lambda.min
bestlam.lasso
plot(cv.lasso)
abline(v=log(bestlam.lasso), lty=1)

coef.lasso = predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')
coef.lasso
```

## Move to LMM
### creazione unico factor combinato
```{r}
# Example categorical variables
grep("CNT",colnames(data))
which(colnames(data)=="CNT")
colnames(data)

levels(data[,24])=levels(data$CNT)
cat_var1 <- data[,24]

data$IM_PUBLIC = as.factor(IM_PUBLIC)
colnames(data)

grep("IM_PUBLIC",colnames(data))
which(colnames(data)=="IM_PUBLIC")
levels(data[,28])=levels(data$IM_PUBLIC)
cat_var2 <- data[,28]

# Combine categorical variables
new_var <- interaction(cat_var1, cat_var2, sep = "-")
levels(new_var)
length(levels(new_var))

data$NEW_VAR = as.factor(new_var)
colnames(data)
```



### fit modello
```{r}
FORMULA_LMM = formula(Social.well.being ~ Approach.to.ICT+Use.of.ICT+Teachers..degree+
					  	Teacher.skill+RATCMP1+
					  	ICTSCH+HEDRES+ATTLNACT+CLSIZE+STAFFSHORT+PV1MATH+
PV1READ+CREACTIV++(1|NEW_VAR))
fit_social_lmm = lmer(FORMULA_LMM,data=data)
fit_social_lmm

fm = fit_social_lmm
sigma2_eps <- as.numeric(get_variance_residual(fm))
sigma2_b <- as.numeric(get_variance_random(fm))

PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
PVRE # 51% is very high!

## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fm, condVar=T))$NEW_VAR
```

### assumptions check
```{r}
# normality of residuals
qqnorm(resid(fit_social_lmm))
qqline(resid(fit_social_lmm), col='red', lwd=2)
shapiro.test(resid(fit_social_lmm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fit_social_lmm)$NEW_VAR),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fit_social_lmm)$NEW_VAR), col='red', lwd=2)
# case interecept and slope
```




# Modello 2
```{r}
IM_PUBLIC = rep(0,dim(data)[1])
IM_PUBLIC [which(data$SCHLTYPE=="Public")] = 1

fit_psych = lm(Psychological.well.being ~ Approach.to.ICT+Use.of.ICT+Teachers..degree+Teacher.skill+
ESCS+RATCMP1+ICTSCH+ICTRES+ENTUSE+LM_MINS+HEDRES+STUBEHA+ATTLNACT+
JOYREAD+PROAT6+TEACHBEHA+STRATIO+CLSIZE+EDUSHORT+STAFFSHORT+PV1MATH+
PV1READ+CREACTIV+IM_PUBLIC,data=data)
summary(fit_psych)
```




## Cose noiose
```{r}
fit = fit_psych
b=coefficients(fit)
check_beta <- c(
	grep("Teachers..degree ", names(b)),
	grep("STUBEHA", names(b)),
	grep("JOYREAD", names(b)),
	grep("PROAT6", names(b)),
	grep("TEACHBEHA", names(b)),
	grep("STRATIO", names(b)),
	grep("STAFFSHORT", names(b)),
	grep("EDUSHORT", names(b))
)

C <- c()
for( i in 1: length(check_beta)){
  vect <- rep(0,fit$rank)
  vect[check_beta[i]] <- 1
  C <- rbind(C,vect)
}
linearHypothesis(fit, C, rep(0,length(check_beta)))
# loro si tolgono
```

## SALTA QUI :)
## Move to LMM
```{r}
FORMULA_LMM_PSYCH = formula(
	Psychological.well.being ~
			Approach.to.ICT+Use.of.ICT+Teacher.skill+
ESCS+RATCMP1+ICTSCH+ICTRES+ENTUSE+LM_MINS+HEDRES+ATTLNACT+CLSIZE+PV1MATH+
PV1READ+CREACTIV+(1|NEW_VAR))

fit_lmm_psych = lmer(FORMULA_LMM_PSYCH,data=data)
fit_lmm_psych


fm = fit_lmm_psych
sigma2_eps <- as.numeric(get_variance_residual(fm))
sigma2_b <- as.numeric(get_variance_random(fm))

PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
PVRE # 51% is very high!

## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fm, condVar=T))$NEW_VAR
```





### assumptions check
```{r}
# normality of residuals
fm = fit_lmm_psych
qqnorm(resid(fm))
qqline(resid(fm), col='red', lwd=2)
shapiro.test(resid(fm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fm)$NEW_VAR),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fm)$NEW_VAR), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fm)~data$NEW_VAR,las=2)

res = residuals(fm)
hist(res)
shapiro.test(log(res+abs(min(res))+1))
```

```{r}
fm_var = fm
## raw residuals 
plot(fm_var, resid(., type = "response") ~ fitted(.)) # Raw vs. fitted
# We observe an asymmetric pattern, with large positive (negative) residuals present mainly for small (large) fitted values.
# but it can be a consequence of the fact that raw residuals are intrinsically heteroscedastic and correlated.

plot(fm_var, resid(., type = "response") ~ as.numeric(NEW_VAR)) # Raw vs. NEW_VAR (not shown)
bwplot(resid(fm_var) ~ NEW_VAR, pch = "|", data = data)
# The boxand-whiskers plots clearly show an increasing variance of the residuals.

## Pearson residuals 
## Pearson residuals are obtained from the raw residuals by dividing the latter by an
## estimate of the appropriate residual standard deviation, so they should be more homoscedastic


plot(fm_var, resid(., type = "pearson" ) ~ fitted(.)) # Pearson vs. fitted
plot(fm_var,resid(., type = "pearson") ~ as.numeric(NEW_VAR)) 
bwplot( resid(fm_var, type = "pearson") ~ NEW_VAR, # Pearson vs. time.f
        pch = "|", data = data)
## this plot illustrate the effect of scaling: the variance of the residuals is virtually constant.
```

## possibili soluzioni ai residui
```{r}
library(robustlmm)
robustlmm::rlmer()


```







