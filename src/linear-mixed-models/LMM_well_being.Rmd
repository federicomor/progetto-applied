---
title: "R Notebook"
output: html_notebook
editor_options:
  chunk_output_type: inline
---

(ho messo le quadre davanti ai numeri, quindi [1] anziché **1** perché mi sembrava più leggibile)

```{r}
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/pisa_scores_final.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
data <- read.csv(file=dataset_path)
head(data)
```

```{r}
library(mvtnorm)
library(MASS)
library(car)
library(rgl)
library(leaps)
library(ISLR)
library(glmnet)
library(lme4)
library(nlmeU) ## --> for the dataset
library(nlme)  ## --> for models implementation

library(corrplot)
library(lattice)
library(plot.matrix)

library(insight)
```
# [0] DATA PREP
```{r}
data$X <- NULL
data$SCHLTYPE <- as.factor(data$SCHLTYPE)
data$CNT <- as.factor(data$CNT)

IM_PUBLIC = rep(0,dim(data)[1])
IM_PUBLIC [which(data$SCHLTYPE=="Public")] = 1
data$IM_PUBLIC = as.factor(IM_PUBLIC)
data$SCHLTYPE <- NULL

```
### creazione unico factor combinato
```{r}
# Example categorical variables
index_cnt = grep("CNT",colnames(data))

colnames(data)

levels(data[,index_cnt])=levels(data$CNT)
cat_var1 <- data[,index_cnt]

index_public= grep("IM_PUBLIC",colnames(data))
levels(data[,index_public])=levels(data$IM_PUBLIC)
cat_var2 <- data[,index_public]

# Combine categorical variables
new_var <- interaction(cat_var1, cat_var2, sep = "-")
levels(new_var)
length(levels(new_var))

data$NEW_VAR = as.factor(new_var)
colnames(data)
```


# [1] Modello 1-Social.well.being
```{r}
FORMULA_COMPLETE_SOCIAL = formula(Social.well.being~.-Psychological.well.being-CNT-IM_PUBLIC-NEW_VAR)
fit_social = lm(FORMULA_COMPLETE_SOCIAL,data=data)
summary(fit_social)
b=coefficients(fit_social)
nv_max = length(b)-1
names(b)
```

# [2] Variable selection
## Best Subset Selection (exhaustive search)
```{r, warning=FALSE, message=FALSE}
regfit.full = regsubsets(FORMULA_COMPLETE_SOCIAL, data=data, nvmax=nv_max)
reg.summary = summary(regfit.full)

# Which one we choose:
#reg.summary$which
# Plots
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")


# We want the model with max r.adj^2 so we extract the coefficients of that model
# Note: ind = how many coefficients has the model
ind = which.max(reg.summary$adjr2)
ind
coef(regfit.full, ind)
```

##  Forward and Backward Stepwise Selection
```{r, warning=FALSE, message=FALSE}
# Forward
regfit.fwd = regsubsets(FORMULA_COMPLETE_SOCIAL,data=data,nvmax=nv_max,method="forward")


# Plot

plot(summary(regfit.fwd)$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")

ind_fwd = which.max(summary(regfit.fwd)$adjr2)
ind_fwd
# Backward
regfit.bwd = regsubsets(FORMULA_COMPLETE_SOCIAL,data=data,nvmax=nv_max,method="backward")


# Plot


plot(summary(regfit.bwd)$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
ind_bwd = which.max(summary(regfit.bwd)$adjr2)
ind_bwd

```

## Comparison- they are all the same
```{r, warning=FALSE, message=FALSE}
max_vars = 18
full = names(coef(regfit.full,max_vars)) # Exhaustive search

fwd = names(coef(regfit.fwd,max_vars)) # Forward Stepwise Selection

bwd = names(coef(regfit.bwd,max_vars)) # Backward Stepwise Selection

all(full==bwd)
all(fwd==bwd)
bwd

vars_chosen_social <- bwd[2:max_vars]

# Salvataggio dei nomi nel file "nomi.txt"
writeLines(vars_chosen_social,"../../data/lm_social_vars.txt")
```


# [3] Fit final model
```{r}
vars = c(bwd[2:max_vars],"CNT","IM_PUBLIC")
formula_social <- paste(paste("Social.well.being","~"), paste(vars, collapse = "+"))
formula_social

fit_social_final = lm(formula_social,data=data)
summary(fit_social_final)
```



# [4] Analysis of residuals
```{r}
par(mfrow=c(2,2))
plot(fit_social_final)
shapiro.test(fit_social_final$residuals)
```

### For every covariate
```{r}
res = fit_social_final$residuals
for (i in 1:(dim(data)[2]-1)){
	plot(res,data[,i],main=colnames(data)[i])
}
```


# [5] Move to LMM
```{r}
vars = c(bwd[2:max_vars],"(1|NEW_VAR)")
FORMULA_LMM <- paste(paste("Social.well.being","~"), paste(vars, collapse = "+"))
FORMULA_LMM

fit_social_lmm = lmer(FORMULA_LMM,data=data)
fit_social_lmm

sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm))

PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))

## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_social_lmm, condVar=T))$NEW_VAR
```

### assumptions check
```{r}
# normality of residuals
fm = fit_social_lmm
qqnorm(resid(fm))
qqline(resid(fm), col='red', lwd=2)
shapiro.test(resid(fm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fm)$NEW_VAR),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fm)$NEW_VAR), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fm)~data$NEW_VAR,las=2)

res = residuals(fm)
hist(res)
shapiro.test(log(res+abs(min(res))+1))
```

```{r}
## raw residuals
plot(fm, resid(., type = "response") ~ fitted(.)) # Raw vs. fitted
# We observe an asymmetric pattern, with large positive (negative) residuals present mainly for small (large) fitted values.
# but it can be a consequence of the fact that raw residuals are intrinsically heteroscedastic and correlated.

plot(fm, resid(., type = "response") ~ as.numeric(NEW_VAR)) # Raw vs. NEW_VAR (not shown)
bwplot(resid(fm) ~ NEW_VAR, pch = "|", data = data)
# The boxand-whiskers plots clearly show an increasing variance of the residuals.

## Pearson residuals
## Pearson residuals are obtained from the raw residuals by dividing the latter by an
## estimate of the appropriate residual standard deviation, so they should be more homoscedastic


plot(fm, resid(., type = "pearson" ) ~ fitted(.)) # Pearson vs. fitted
plot(fm,resid(., type = "pearson") ~ as.numeric(NEW_VAR))
bwplot( resid(fm, type = "pearson") ~ NEW_VAR, # Pearson vs. time.f
        pch = "|", data = data)
## this plot illustrate the effect of scaling: the variance of the residuals is virtually constant.
```


# [6] Tentativi di recupero assumptions

## [6.1] Scalando le covariate
Nel LMM qui sopra usciva il warnign "some predictor variables are on very different scales: consider rescaling". So we try rescaling.

```{r}
data
colnames(data)
numerical_variables = c(1:23,25,26)
colnames(data)[numerical_variables]

data_scaled = data
data_scaled[,c(numerical_variables)] = data.frame(scale(data_scaled[,c(numerical_variables)]))
boxplot(data[,numerical_variables], main="old data")
boxplot(data_scaled[,numerical_variables],main="scaled data")
```

### Fit sul nuovo dataset
```{r}
fit_social_lmm_scaled = lmer(FORMULA_LMM,data=data_scaled)
fit_social_lmm_scaled

sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm_scaled))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm_scaled))

PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))

## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_social_lmm_scaled, condVar=T))$NEW_VAR
```

### Assumptions check
```{r}
# normality of residuals
fm = fit_social_lmm_scaled
qqnorm(resid(fm))
qqline(resid(fm), col='red', lwd=2)
shapiro.test(resid(fm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fm)$NEW_VAR),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fm)$NEW_VAR), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fm)~data$NEW_VAR,las=2)

res = residuals(fm)
hist(res)
```

```{r}
## raw residuals
plot(fm, resid(., type = "response") ~ fitted(.)) # Raw vs. fitted
# We observe an asymmetric pattern, with large positive (negative) residuals present mainly for small (large) fitted values.
# but it can be a consequence of the fact that raw residuals are intrinsically heteroscedastic and correlated.

plot(fm, resid(., type = "response") ~ as.numeric(NEW_VAR)) # Raw vs. NEW_VAR (not shown)
bwplot(resid(fm) ~ NEW_VAR, pch = "|", data = data)
# The boxand-whiskers plots clearly show an increasing variance of the residuals.

## Pearson residuals
## Pearson residuals are obtained from the raw residuals by dividing the latter by an
## estimate of the appropriate residual standard deviation, so they should be more homoscedastic


plot(fm, resid(., type = "pearson" ) ~ fitted(.)) # Pearson vs. fitted
plot(fm,resid(., type = "pearson") ~ as.numeric(NEW_VAR))
bwplot( resid(fm, type = "pearson") ~ NEW_VAR, # Pearson vs. time.f
        pch = "|", data = data)
## this plot illustrate the effect of scaling: the variance of the residuals is virtually constant.
```


Niente, sembra che il problema siano sempre le code dei residui, quindi magari è questione di outliers?

## [6.2] Rimuovendo outliers
### Visual check
```{r}
data_woo = data # woo = [W]ith[O]ut [O]utliers

for(i in 1:(length(vars)-1) )
  boxplot(data_woo[,i],main=paste("Overall,",vars[i]))
for(i in 1:(length(vars)-1) )
  boxplot(data_woo[,i]~data_woo$CNT,main=paste("By state,",vars[i]),
          col=colora(length(unique(data_woo$CNT))),las=2)
# la funzione colora dovrebbe essere dentro Utilities
```

### Rimozione overall
```{r}
M = colMeans(data[,numerical_variables])
S = cov(data[,numerical_variables])
d2 = matrix(mahalanobis(data[,numerical_variables], M, S))
hist(d2,breaks = 300,xlim = c(0,300))
abline(v=25,col="red")

data_woo = data[which(d2 <= 25), ]
print(paste("From",dim(data)[1],"obs we moved to",dim(data_woo)[1]))
print(paste("Percentuale di dati sopravvissuti:",dim(data_woo)[1]/dim(data)[1]*100,"%"))

```

### Rimozione by state
```{r}
# magari ci sta fare meno filtraggio overall al punto prima, cioè lasciare la soglia
# più alta nella selezione sopra
# e poi fare un "raffinamento" qui, togliendo outlier relativi non al comportamento
# generale ma a quello che esce dal confronto tra gli stati
```


### Visual check after removing outliers
```{r}
for(i in 1:(length(vars)-1) )
  boxplot(data[,i],data_woo[,i],main=paste("Overall,",vars[i],"before vs after"))
for(i in 1:(length(vars)-1) )
  boxplot(data_woo[,i]~data_woo$CNT,main=paste("By state,",vars[i]),
          col=colora(length(unique(data_woo$CNT))),las=2)
# la funzione colora dovrebbe essere dentro Utilities
```

### Maybe more precise check about the filtering
```{r}
# cols = heat.colors(100)
cols = gray.colors(100)
# cols=colora(100)

image(cov(data_woo[,numerical_variables]),
      breaks = quantile(cov(data_woo[,numerical_variables]),(0:100)/100),
      col=cols)
image(cov(data[,numerical_variables]),
      breaks = quantile(cov(data[,numerical_variables]),(0:100)/100),
      col=cols)

# sembrano uguali quindi non c'è stata molta perdita di informazioni
```


### Fit sul nuovo dataset
```{r}
fit_social_lmm_woo = lmer(FORMULA_LMM,data=data_woo)
fit_social_lmm_woo

sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm_woo))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm_woo))

PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))

## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_social_lmm_woo, condVar=T))$NEW_VAR
```

### Assumptions check
```{r}
# normality of residuals
fm = fit_social_lmm_woo
qqnorm(resid(fm))
qqline(resid(fm), col='red', lwd=2)
shapiro.test(resid(fm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fm)$NEW_VAR),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fm)$NEW_VAR), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fm)~data_woo$NEW_VAR,las=2)

res = residuals(fm)
hist(res)

```

### Modifiche suggerite da brute_force.jl

```{r}
x1 = (data_woo$Approach.to.ICT)^2
x2 = (data_woo$Use.of.ICT)
x3 = log((data_woo$Teachers..degree)+abs(min((data_woo$Teachers..degree)))+1)
x4 = (data_woo$Teacher.skill)^2
x5 = (data_woo$ESCS)
x6 = sqrt(abs((data_woo$RATCMP1)))
x7 = (data_woo$ICTSCH)^2
x8 = (data_woo$HEDRES)^2
x9 = (data_woo$STUBEHA)
x10 = sqrt(abs((data_woo$ATTLNACT)))
x11 = (data_woo$JOYREAD)^2
x12 = log((data_woo$PROAT6)+abs(min((data_woo$PROAT6)))+1)
x13 = (data_woo$CLSIZE)
x14 = (data_woo$EDUSHORT)
x15 = (data_woo$STAFFSHORT)^2
x16 = (data_woo$PV1MATH)^2
x17 = (data_woo$PV1READ)
y = (data_woo$Psychological.well.being)


fit_social_lmm_woo = lmer(
  y ~ x1 +x2 +x3 +x4 +x5 +x6 +x7 +x8 +x9 +x10 +x11 +x12 +x13 +x14 +x15 +x16 +x17+
    (1|data_woo$NEW_VAR)
  )
fit_social_lmm_woo

sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm_woo))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm_woo))

PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))

## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_social_lmm_woo, condVar=T))
```
### Assumptions check
```{r}
# normality of residuals
fm = fit_social_lmm_woo
qqnorm(resid(fm))
qqline(resid(fm), col='red', lwd=2)
shapiro.test(resid(fm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fm)),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fm)), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fm)~data_woo$NEW_VAR,las=2)

res = residuals(fm)
hist(res)

```




## [6.3] Combinando le due idee
Scaliamo e togliamo gli outliers.

### Togliamo outliers
```{r}
M = colMeans(data[,numerical_variables])
S = cov(data[,numerical_variables])
d2 = matrix(mahalanobis(data[,numerical_variables], M, S))
hist(d2,breaks = 300,xlim = c(0,300))
abline(v=50,col="red")

data_woo = data[which(d2 <= 28), ]
print(paste("From",dim(data)[1],"obs we moved to",dim(data_woo)[1]))
print(paste("Percentuale di dati sopravvissuti:",dim(data_woo)[1]/dim(data)[1]*100,"%"))
```


### Scaliamo
```{r}
data_woo_scaled = data_woo
data_woo_scaled[,c(numerical_variables)] =
  data.frame(scale(data_woo_scaled[,c(numerical_variables)]))

boxplot(data[,numerical_variables], main="old data",las=2)
boxplot(data_woo[,numerical_variables], main="woo data",las=2)
boxplot(data_woo_scaled[,numerical_variables],main="scaled and woo data",las=2)
# magari dare qualche spuntatina manuale a proat6 e ratcmp1 che sembrano le peggiori
```

### Spuntatina manuale
```{r}
boxplot(data_woo_scaled[,numerical_variables],main="scaled and woo data",las=2)

old_dim = dim(data_woo_scaled)[1]
data_woo_scaled = data_woo_scaled[which(data_woo_scaled$PROAT6<4),]
data_woo_scaled = data_woo_scaled[which(data_woo_scaled$RATCMP1<4),]
data_woo_scaled = data_woo_scaled[which(data_woo_scaled$LM_MINS<4.1),]
new_dim = dim(data_woo_scaled)[1]
boxplot(data_woo_scaled[,numerical_variables],main="scaled and woo data, after spuntatina",las=2)

print(paste("From",old_dim,"obs we moved to",new_dim))
print(paste("Percentuale di dati sopravvissuti:",new_dim/old_dim*100,"%"))
```


### Fit sul nuovo dataset
```{r}
fit_social_lmm_woo_scaled = lmer(FORMULA_LMM,data=data_woo_scaled)
fit_social_lmm_woo_scaled

sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm_woo_scaled))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm_woo_scaled))

PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))

## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_social_lmm_woo_scaled, condVar=T))$NEW_VAR
```

### Assumptions check
```{r}
# normality of residuals
fm = fit_social_lmm_woo_scaled
qqnorm(resid(fm))
qqline(resid(fm), col='red', lwd=2)
shapiro.test(resid(fm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fm)$NEW_VAR),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fm)$NEW_VAR), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fm)~data_woo_scaled$NEW_VAR,las=2)

res = residuals(fm)
hist(res)
```



# ????????????????????????????????????????????

#[1] Modello 2-Psychological.well.being
```{r}
FORMULA_COMPLETE_PSICO = formula(Psychological.well.being~.-Social.well.being-CNT-IM_PUBLIC-NEW_VAR)
fit_psico = lm(FORMULA_COMPLETE_PSICO,data=data)
summary(fit_psico)
b=coefficients(fit_psico)
nv_max = length(b)-1
names(b)
```

# [2] Variable selection
## Best Subset Selection (exhaustive search)
```{r, warning=FALSE, message=FALSE}
regfit.full = regsubsets(FORMULA_COMPLETE_PSICO, data=data, nvmax=nv_max)
reg.summary = summary(regfit.full)


reg.summary = summary(regfit.full)

# Which one we choose:
#reg.summary$which
# Plots
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")

# We want the model with max r.adj^2 so we extract the coefficients of that model
# Note: ind = how many coefficients has the model
ind = which.max(reg.summary$adjr2)
ind

```

##  Forward and Backward Stepwise Selection

```{r, warning=FALSE, message=FALSE}
# Forward
regfit.fwd = regsubsets(FORMULA_COMPLETE_PSICO,data=data,nvmax=nv_max,method="forward")


# Plot

plot(summary(regfit.fwd)$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")

ind_fwd = which.max(summary(regfit.fwd)$adjr2)
ind_fwd
# Backward
regfit.bwd = regsubsets(FORMULA_COMPLETE_PSICO,data=data,nvmax=nv_max,method="backward")


# Plot


plot(summary(regfit.bwd)$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
ind_bwd = which.max(summary(regfit.bwd)$adjr2)
ind_bwd

```

## Comparison- they are all the same
```{r, warning=FALSE, message=FALSE}
max_vars = 17
full = names(coef(regfit.full,max_vars)) # Exhaustive search

fwd = names(coef(regfit.fwd,max_vars)) # Forward Stepwise Selection

bwd = names(coef(regfit.bwd,max_vars)) # Backward Stepwise Selection

all(full==bwd)
all(fwd==bwd)
bwd

vars_chosen_psyc <- bwd[2:max_vars]

# Salvataggio dei nomi nel file "nomi.txt"
writeLines(vars_chosen_psyc, "../../data/lm_psico_vars.txt")
```
# [3] fit final model
```{r}
vars = c(bwd[2:max_vars],"CNT","IM_PUBLIC")
formula_psyco <- paste(paste("Psychological.well.being","~"), paste(vars, collapse = "+"))
formula_psyco

formula_psyco_final = lm(formula_psyco,data=data)
summary(formula_psyco_final)
```









# [4] Analysis of residuals
```{r}
par(mfrow=c(2,2))
plot(formula_psyco_final)
shapiro.test(formula_psyco_final$residuals)
```

### For every covariate
```{r}
res = formula_psyco_final$residuals
for (i in 1:(dim(data)[2]-1)){
	plot(res,data[,i],main=colnames(data)[i])
}
```


# [5] Move to LMM
```{r}
vars = c(bwd[2:max_vars],"(1|NEW_VAR)")
FORMULA_LMM <- paste(paste("Psychological.well.being","~"), paste(vars, collapse = "+"))
FORMULA_LMM

fit_social_lmm = lmer(FORMULA_LMM,data=data)
fit_social_lmm

sigma2_eps <- as.numeric(get_variance_residual(fit_social_lmm))
sigma2_b <- as.numeric(get_variance_random(fit_social_lmm))

PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))

## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_social_lmm, condVar=T))$NEW_VAR
```





### assumptions check
```{r}
# normality of residuals
qqnorm(resid(fit_social_lmm))
qqline(resid(fit_social_lmm), col='red', lwd=2)
shapiro.test(resid(fit_social_lmm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fit_social_lmm)$NEW_VAR),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fit_social_lmm)$NEW_VAR), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fit_social_lmm)~data$NEW_VAR,las=2)

res = residuals(fit_social_lmm)
hist(res)
shapiro.test(log(res+abs(min(res))+1))
```

```{r}
## raw residuals
plot(fit_social_lmm, resid(., type = "response") ~ fitted(.)) # Raw vs. fitted
# We observe an asymmetric pattern, with large positive (negative) residuals present mainly for small (large) fitted values.
# but it can be a consequence of the fact that raw residuals are intrinsically heteroscedastic and correlated.

plot(fit_social_lmm, resid(., type = "response") ~ as.numeric(NEW_VAR)) # Raw vs. NEW_VAR (not shown)
bwplot(resid(fit_social_lmm) ~ NEW_VAR, pch = "|", data = data)
# The boxand-whiskers plots clearly show an increasing variance of the residuals.

## Pearson residuals
## Pearson residuals are obtained from the raw residuals by dividing the latter by an
## estimate of the appropriate residual standard deviation, so they should be more homoscedastic


plot(fit_social_lmm, resid(., type = "pearson" ) ~ fitted(.)) # Pearson vs. fitted
plot(fit_social_lmm,resid(., type = "pearson") ~ as.numeric(NEW_VAR))
bwplot( resid(fit_social_lmm, type = "pearson") ~ NEW_VAR, # Pearson vs. time.f
        pch = "|", data = data)
## this plot illustrate the effect of scaling: the variance of the residuals is virtually constant.
```


# [6] Tentativi di recupero assumptions
## [6.2] Rimuovendo outliers
### Visual check
```{r}
data_woo = data # woo = [W]ith[O]ut [O]utliers

# Codice in teoria generico: qui vars lo prende dalle vars per la psych well being
# prima invece era il vars definito per la social well being
# Così come con FORMULA e altro

for(i in 1:(length(vars)-1) )
  boxplot(data_woo[,i],main=paste("Overall,",vars[i]))
for(i in 1:(length(vars)-1) )
  boxplot(data_woo[,i]~data_woo$CNT,main=paste("By state,",vars[i]),
          col=colora(length(unique(data_woo$CNT))),las=2)
# la funzione colora dovrebbe essere dentro Utilities
```

### UPDATE: Rimozione pensata
Rimuoviamo gli outleirs solo in base alle covariate interessanti per il modello.

Sennò magari outliers su covariate che poi non ci interessano potrebbero eliminare osservazioni invece interessanti forse.
```{r}
vars
numerical_relevant_variables = NULL
for (i in 1:(length(vars)-1) ) # -1 perché il rand effect non è in data
  numerical_relevant_variables = c(numerical_relevant_variables,
                                   which(colnames(data)==vars[i]))
numerical_relevant_variables
length(numerical_relevant_variables) == length(vars)-1 # check

M = colMeans(data[,numerical_relevant_variables])
S = cov(data[,numerical_relevant_variables])
d2 = matrix(mahalanobis(data[,numerical_relevant_variables], M, S))
hist(d2,breaks = 300,xlim = c(0,300))
#========== tune this ===========================#
SOGLIA = 10
#========== tune this ===========================#
abline(v=SOGLIA,col="red")

data_woo = data[which(d2 <= SOGLIA), ]
print(paste("From",dim(data)[1],"obs we moved to",dim(data_woo)[1]))
print(paste("Percentuale di dati sopravvissuti:",dim(data_woo)[1]/dim(data)[1]*100,"%"))
```




### Rimozione overall
```{r}
M = colMeans(data[,numerical_variables])
S = cov(data[,numerical_variables])
d2 = matrix(mahalanobis(data[,numerical_variables], M, S))
hist(d2,breaks = 300,xlim = c(0,300))
abline(v=25,col="red")

data_woo = data[which(d2 <= 30), ]
print(paste("From",dim(data)[1],"obs we moved to",dim(data_woo)[1]))
print(paste("Percentuale di dati sopravvissuti:",dim(data_woo)[1]/dim(data)[1]*100,"%"))

```

### Rimozione by state
```{r}
# magari ci sta fare meno filtraggio overall al punto prima, cioè lasciare la soglia
# più alta nella selezione sopra
# e poi fare un "raffinamento" qui, togliendo outlier relativi non al comportamento
# generale ma a quello che esce dal confronto tra gli stati
```


### Visual check after removing outliers
```{r}
for(i in 1:(length(vars)-1) )
  boxplot(data[,i],data_woo[,i],main=paste("Overall,",vars[i],"before vs after"))
for(i in 1:(length(vars)-1) )
  boxplot(data_woo[,i]~data_woo$CNT,main=paste("By state,",vars[i]),
          col=colora(length(unique(data_woo$CNT))),las=2)
# la funzione colora dovrebbe essere dentro Utilities
```

## Maybe more precise check about the filtering
```{r}
# cols = heat.colors(100)
cols = gray.colors(100)
# cols = colora(100)
numerical_variables = numerical_relevant_variables

image(cov(data_woo[,numerical_variables]),
      breaks = quantile(cov(data_woo[,numerical_variables]),(0:100)/100),
      col=cols)
image(cov(data[,numerical_variables]),
      breaks = quantile(cov(data[,numerical_variables]),(0:100)/100),
      col=cols)

# sembrano uguali quindi non c'è stata molta perdita di informazioni
```


### Fit sul nuovo dataset
```{r}
fit_psych_lmm_woo = lmer(FORMULA_LMM,data=data_woo)
fit_psych_lmm_woo

sigma2_eps <- as.numeric(get_variance_residual(fit_psych_lmm_woo))
sigma2_b <- as.numeric(get_variance_random(fit_psych_lmm_woo))

PVRE <- sigma2_b/(sigma2_b+sigma2_eps)
print(paste("PVRE =",PVRE))

## visualization of the random intercepts with their 95% confidence intervals
# Random effects: b_0i for i=1,...,234
dotplot(ranef(fit_psych_lmm_woo, condVar=T))$NEW_VAR
```

### Assumptions check
```{r}
# normality of residuals
fm = fit_psych_lmm_woo
qqnorm(resid(fm))
qqline(resid(fm), col='red', lwd=2)
shapiro.test(resid(fm))
# normality of random effects
# case only interecept
qqnorm(unlist(ranef(fm)$NEW_VAR),
main='Normal Q-Q Plot - Random Effects on Intercept')
qqline(unlist(ranef(fm)$NEW_VAR), col='red', lwd=2)
# case interecept and slope
boxplot(residuals(fm)~data_woo$NEW_VAR,las=2)

res = residuals(fm)
hist(res)
shapiro.test(res)
```
