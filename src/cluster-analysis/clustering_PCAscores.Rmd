---
title: 'Unsupervised learning: Hierarchical, DB-scan, K-means clustering'
output: html_document
editor_options:
  chunk_output_type: inline
---

# SETTINGS

```{r setup}
rm( list = ls() )

list.files()
graphics.off() # chiude tutti i device grafici
cat("\014") #pulisci console
```

```{r setup}
library(mvtnorm)
library(MVN)
library(rgl)
library(car)
library(dbscan)
library(cluster)
library(fields)
library(ggplot2)
library(GGally)
```

```{r}
#directories
dataset_dir = "../../data/"
include_dir = "../include/Utilities.R"
#including utilities
source(include_dir)
#importing the dataset
pisa_data <- read.csv(file=paste(dataset_dir,"pisa_school_final_wo_Outl_PCA_SCORES.csv",sep=""))
```

```{r}
categories_variables = list(tec,psi,clt,fam,tch,sch)
cat_var_names = c("tec","psi","clt","fam","tch","sch")

data_grezzi <- pisa_data
data_clustered <- pisa_data
# Dimensions
n = dim(data_grezzi)[1]
p = dim(data_grezzi)[2]
#removing the CNT column
meas_labels<-c(2:p)
data_grezzi = data_grezzi[,meas_labels]
head(data_grezzi)
## note: rescale the variables if there's a high order of magnitude
```

# 0. MAYBE SOME SCATTERPLOTS TO SEE IF DATA ARE CLUSTERED FOR SOME VARIABLES

```{r}
ggpairs(pisa_data[,categories_variables[[1]]]) #tec
ggpairs(pisa_data[,categories_variables[[2]]]) #psi
ggpairs(pisa_data[,categories_variables[[3]]]) #clt
ggpairs(pisa_data[,categories_variables[[4]]]) #fam
ggpairs(pisa_data[,categories_variables[[5]]]) #tch
ggpairs(pisa_data[,categories_variables[[6]]]) #sch
```

# **1.** HIERARCHICAL CLUSTERING

## 1.1 Exploration: DISTANCES AND LINKAGES

(I put the commented code in a chunk to test it. I set eval=FALSE and include=FALSE not toI run it when I use the "run all" command)

```{r echo=FALSE}
#looping over the categories of variables
for(k in 1:length(categories_variables)){ 
  #selecting the group of variables
  data_grezzi = pisa_data[,c("CNT",categories_variables[[k]])] 
  #discarding CNT variable in measures
  meas_labels <-c(2: dim(data_grezzi)[2]) 
  measures = data_grezzi[,meas_labels]
  # EXPERIMENTING DIFFERENT DISTANCES
  #computing dissimiliraties measures for each group of variables
  data.e = dist(measures, method="euclidean") 
  data.m = dist(measures, method="manhattan") 
  data.c = dist(measures, method="canberra")
  #storing the dissimilarity matrices in a list
  distances = list(data.e,data.m,data.c) 
  names = c("euclidean","manhattan","canberra")

  #EXPERIMENTING DIFFERENT LINKAGES
  i <- 1 
  for(dist_chosen in distances){ 
    data_grezzi = pisa_data[,c("CNT",categories_variables[[k]])] 
    name_dist_chosen <- names[i] 
    i <- i+1
    #hierarchical clustering for each type of linkage
    # linkages: "single", "average", "complete", "ward","ward.D2"
    data.ds = hclust(dist_chosen, method='single') 
    data.da = hclust(dist_chosen, method='average') 
    data.dc = hclust(dist_chosen, method='complete') 
    data.dw = hclust(dist_chosen, method="ward.D") 
    data.dw2 = hclust(dist_chosen, method="ward.D2")
    #plotting the resulting dendograms
    par(mfrow=c(2,2)) 
    plot(data.dc, main=paste(name_dist_chosen,'complete',cat_var_names[k]),
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='')
    plot(data.da, main=paste(name_dist_chosen,'average',cat_var_names[k]), 
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='') 
    plot(data.dw, main=paste(name_dist_chosen,'ward.D',cat_var_names[k]), 
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='') 
    plot(data.dw2, main=paste(name_dist_chosen,'wardD2',cat_var_names[k]), 
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='')
    
    
    #Evaluting the results of our clustering with the cophenitic correlation       coeffincient
    # Cophenetic Matrices
    coph.dc <- cophenetic(data.dc) 
    coph.da <- cophenetic(data.da) 
    coph.dw <- cophenetic(data.dw) 
    coph.dw2 <- cophenetic(data.dw2)
  
    # Cophenetic Coefficients
    dc = cor(data.e, coph.dc) 
    da = cor(data.e, coph.da) 
    dw = cor(data.e, coph.dw) 
    dw2 = cor(data.e, coph.dw2)
    
    print(cat_var_names[k])
    print(name_dist_chosen)
    print(c(dc,da,dw,dw2))
    # euclidian average : 0.6, but ward seems so much better from the dendogram 
    # manhatta average 0.55 but ward seems so much better from the dendogram 
    # terrible canberra
  }
}
```

## 1.2 APPLIYING THE SELECTED DISTANCE AND LINKAGE

```{r, warning=FALSE, message=FALSE}
data_grezzi <- pisa_data
counter_names = 1
for (k in categories_variables){
  measures = data_grezzi[,k]
  head(measures)
  
  # methods: "euclidean", "manhattan", "canberra"
  name_dist_chosen <- "manhattan"
  # linkages: "single", "average", "complete", "ward", "ward.D2"
  name_linkage_chosen <- 'ward.D2'
  #computing the dissimilarity matrix
  data.dist = dist(measures, method=name_dist_chosen)
  #applying hierarchical clustering with the computed dissimilarity matrix and the selected      linkage
  data.hclust = hclust(data.dist, method=name_linkage_chosen)

  #Inside data.hclust
  #   $merge -> order of aggregation
  #   $height -> distance at which we have aggregation
  #   $order -> ordering that allows to avoid intersection in the dendrogram

  # Plotting the dendogram
  k_chosen = 3
  plot(data.hclust, main=paste(name_dist_chosen,name_linkage_chosen), hang=-0.1, xlab='',        labels=F, cex=0.6, sub='')
  rect.hclust(data.hclust, k=k_chosen)
  
  # How to cut a dendrogram?
  # We generate vectors of labels through the command cutree()
  cluster.cutree = cutree(data.hclust, k=k_chosen)
  table(cluster.cutree)

  ## How good is the clustering?
  # Cophenetic Matrices
  coph.mat <- cophenetic(data.hclust)
  # Cophenetic Coefficients
  coph.coeff = cor(data.dist, coph.mat)
  coph.coeff


  #plot(measures[,1],measures[,3],col=cluster.cutree+1)
  #plot3d(measures, size=3, col=cluster.cutree+1, aspect = F)
  
  ## **2.** Saving the clustering as factors
  
  data_clustered[,cat_var_names[counter_names]] = as.factor(cluster.cutree)
  counter_names = counter_names+ 1
}

head(data_clustered)
```

## **1.3** Exploring the relationship between countries and clusters

```{r}
country_names <- unique(data_clustered$CNT)
g=16
indeces = list()
for (jj in 1:g){
    indeces[jj] <- list(which(data_clustered$CNT == country_names[jj]))
}
n_list = list()
for(j in 1:g) {
  n_list[j] = list(length(indeces[[j]]))
}

####
# per ogni cluster voglio vedere quale percentuale di scuole della stessa nazione ricadono nello stesso
percent_list = list()
counter = 0


for(k in cat_var_names){ # categories
  counter = counter +1  
  percent_list_mid = list()
  counter2=0
  for(i in 1:k_chosen){ # clusters
    counter2 = counter2 +1
  percent = c()
    for(j in 1:g){ # countrys 
      temp = length(data_clustered[which(data_clustered[,k]==i & data_clustered$CNT==country_names[j]),1])
      temp = temp/n_list[[j]]
      percent = c(percent,temp)
    }
    percent_list_mid[counter2]=list(percent)

  }
  percent_list[counter] = list(percent_list_mid)
}


```

```{r}
for(i in 1:3){
  x11()
  plot(measures[,1],measures[,2],col=cluster.cutree+1)
}
```

```{r}
#pdf("cluster_country_proportions.pdf")

for(kk in 1:6){
# Create sample data
group_percentages <- percent_list[[kk]]

groups_names = c()
for (i in 1:k_chosen){
  groups_names = c(groups_names,paste("Group",i))
}
# Convert percentages to data frame

df <- data.frame(
  country = rep(country_names, each = k_chosen),
  group = rep(groups_names, times = length(country_names)),
  percentage = NA
)


for(i in country_names){
  for(j in groups_names){
    df[which((df$group==j) & (df$country==i)),3] = group_percentages[[which(groups_names==j)]][which(country_names==i)]*100
  }
}
  
x11()
# Create stacked bar chart
ggplot(df, aes(x = country, y = percentage, fill = group)) +
  geom_bar(stat = "identity") +
  labs(title = "Proportion of Schools in Each Group by Country",
       x = "Country",
       y = "Proportion") +
  scale_fill_manual(values = c("#E69F00", "#56B4E9", "#009E73"),
                    labels = groups_names) +
  geom_text(aes(label = paste0(round(percentage,2), "%")), 
            position = position_stack(vjust = 0.5))

}
#dev.off()
```

# **2.** DBSCAN

### **2.1** choice of hyperparameters

```{r}
data_to_cluster = data_grezzi[,tec]
# Rule of thumb, minPts = dimensionality + 1 = 3 here
# How to choose eps from minPts?
# Plot of the distances to the minPts nearest neighbor
k_chosen_2=dim(data_to_cluster)[2]+1
kNNdistplot(as.matrix(data_to_cluster), k = k_chosen_2)
eps_chosen = 0.5
abline(h = eps_chosen, col = "red", lty = 2)

# Run the dbscan
dbs <- dbscan(data_to_cluster, eps = eps_chosen, minPts = p+1)
dbs
plot(data_to_cluster, col = dbs$cluster + 1L, pch=19)
```

### **2.2** Silhouette

```{r}
# Let's compute the silhouette score on the clustering performed before
# WARNING (specific to DBSCAN): We need to remove the noise points as they do
# not belong to a cluster, before computing the silhouette score
clustered_index <- which(dbs$cluster != 0) # Index of non noise points
clustered_points <- data_grezzi[clustered_index,] # only clustered points
clustered_labels <- dbs$cluster[clustered_index] # corresponding labels

sil <- silhouette(clustered_labels, dist(clustered_points))
summary(sil)

sil_score <- function(labels, dist) {
  # Compute the average of the silhouette widths
  sil <- silhouette(labels, dist)
  sil_widths <- sil[,"sil_width"]
  mean(sil_widths)
}

sil_score(clustered_labels, dist(clustered_points))
```

# 3. K-MEANS
