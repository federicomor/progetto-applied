---
title: 'Unsupervised learning: Hierarchical, DB-scan, K-means clustering'
output: html_document
editor_options:
  chunk_output_type: inline
---

# SETTINGS

```{r}
rm( list = ls() )

list.files()
graphics.off() # chiude tutti i device grafici
cat("\014") #pulisci console

library(mvtnorm)
library(MVN)
library(rgl)
library(car)
library(dbscan)
library(cluster)
library(fields)
library(ggplot2)
library(GGally)
```
```{r}
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/pisa_scores_final.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
data <- read.csv(file=dataset_path)
head(data)
```


# **0** DATA PREP
```{r}
data$X <- NULL
data$SCHLTYPE <- as.factor(data$SCHLTYPE)
data$CNT <- as.factor(data$CNT)

IM_PUBLIC = rep(0,dim(data)[1])
IM_PUBLIC [which(data$SCHLTYPE=="Public")] = 1
data$IM_PUBLIC = as.factor(IM_PUBLIC)
data$SCHLTYPE <- NULL
head(data)

```
# Define grouped variables, dimension too big to do complete
```{r}
linear_model_vars <- readLines("../../data/lm_social_vars.txt")

filter <- function(category,var_lm) {
  risultato <- c()
  
  for (elemento in category) {
    if (elemento %in% var_lm) {
      risultato <- c(risultato, elemento)
    }
  }
  
  return(risultato)
}

categories_variables=list()

categories_variables[["TECH"]] = c("Approach.to.ICT","ICTSCH","ICTRES" ,"Use.of.ICT","RATCMP1","ENTUSE" )

categories_variables[["TEACH"]] = c("Teacher.skill","TEACHBEHA","Teachers..degree","PROAT6")

categories_variables[["WELL-BEING"]] = c("Social.well.being","Psychological.well.being")

categories_variables[["SCHOOL"]] = c("EDUSHORT","STAFFSHORT","STRATIO","CLSIZE","CREACTIV")

categories_variables[["STUDENT"]] = c("LM_MINS","ATTLNACT","PV1READ", "PV1MATH" ,"STUBEHA","JOYREAD"  )

categories_variables[["FAMILY"]] = c("ESCS", "HEDRES")
cat_var_names = names(categories_variables)

categories_variables_filtered=list()

for(cat in cat_var_names ){
  categories_variables_filtered[[cat]]=filter(categories_variables[[cat]],linear_model_vars)
}
categories_variables_filtered[["WELL-BEING"]] = c("Social.well.being","Psychological.well.being")
categories_variables_filtered
#categories_variables=categories_variables_filtered
```
```{r}
k=1
for(cat in categories_variables_filtered){
  if (length(cat)==2){
    plot(data[,cat],main=cat_var_names[k],col=data$CNT)
  }
  if (length(cat)==3){
    plot3d(data[,cat],main=cat_var_names[k],col=as.numeric(data$CNT))
  }
  k=k+1
}
```



```{r}
p =dim(data)[2]
lab_index =c(24,27) # CNT and IM_PUBLIC

cols <- seq(1,p)
feats <- cols[!cols %in% lab_index]


data_grezzi <- data
data_clustered <- data
# Dimensions
n = dim(data_grezzi)[1]
p = dim(data_grezzi)[2]
#removing the CNT column
data_grezzi = data_grezzi[,feats]
head(data_grezzi)
## note: rescale the variables if there's a high order of magnitude
```

# **1.** HIERARCHICAL CLUSTERING

## 1.1 Exploration: DISTANCES AND LINKAGES

(I put the commented code in a chunk to test it. I set eval=FALSE and include=FALSE not toI run it when I use the "run all" command)

```{r echo=FALSE}
#looping over the categories of variables
for(k in 1:length(categories_variables)){ 
  #selecting the group of variables
  data_grezzi = data[,c("CNT",categories_variables[[k]])] 
  #discarding CNT variable in measures
  meas_labels <-c(2: dim(data_grezzi)[2]) 
  measures = data_grezzi[,meas_labels]
  # EXPERIMENTING DIFFERENT DISTANCES
  #computing dissimiliraties measures for each group of variables
  data.e = dist(measures, method="euclidean") 
  data.m = dist(measures, method="manhattan") 
  data.c = dist(measures, method="canberra")
  #storing the dissimilarity matrices in a list
  distances = list(data.e,data.m,data.c) 
  names = c("euclidean","manhattan","canberra")

  #EXPERIMENTING DIFFERENT LINKAGES
  i <- 1 
  for(dist_chosen in distances){ 
    data_grezzi = data[,c("CNT",categories_variables[[k]])] 
    name_dist_chosen <- names[i] 
    i <- i+1
    #hierarchical clustering for each type of linkage
    # linkages: "single", "average", "complete", "ward","ward.D2"
    data.ds = hclust(dist_chosen, method='single') 
    data.da = hclust(dist_chosen, method='average') 
    data.dc = hclust(dist_chosen, method='complete') 
    data.dw = hclust(dist_chosen, method="ward.D") 
    data.dw2 = hclust(dist_chosen, method="ward.D2")
    #plotting the resulting dendograms
    par(mfrow=c(2,2)) 
    plot(data.dc, main=paste(name_dist_chosen,'complete',cat_var_names[k]),
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='')
    plot(data.da, main=paste(name_dist_chosen,'average',cat_var_names[k]), 
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='') 
    plot(data.dw, main=paste(name_dist_chosen,'ward.D',cat_var_names[k]), 
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='') 
    plot(data.dw2, main=paste(name_dist_chosen,'wardD2',cat_var_names[k]), 
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='')
    
    
    #Evaluting the results of our clustering with the cophenitic correlation       coeffincient
    # Cophenetic Matrices
    coph.dc <- cophenetic(data.dc) 
    coph.da <- cophenetic(data.da) 
    coph.dw <- cophenetic(data.dw) 
    coph.dw2 <- cophenetic(data.dw2)
  
    # Cophenetic Coefficients
    dc = cor(data.e, coph.dc) 
    da = cor(data.e, coph.da) 
    dw = cor(data.e, coph.dw) 
    dw2 = cor(data.e, coph.dw2)
    coph= list(dc,da,dw,dw2)
    
    categories_variables[[k]]$coph = list(name_dist_chosen,coph)
    
    print(cat_var_names[k])
    print(name_dist_chosen)
    print(c(dc,da,dw,dw2))
    # euclidian average : 0.6, but ward seems so much better from the dendogram 
    # manhatta average 0.55 but ward seems so much better from the dendogram 
    # terrible canberra
  }
}
```

## 1.2 APPLIYING THE SELECTED DISTANCE AND LINKAGE

```{r, warning=FALSE, message=FALSE}
data_grezzi <- data
counter_names = 1
for (k in categories_variables){
  measures = data_grezzi[,k]
  head(measures)
  
  # methods: "euclidean", "manhattan", "canberra"
  name_dist_chosen <- "manhattan"
  # linkages: "single", "average", "complete", "ward", "ward.D2"
  name_linkage_chosen <- 'ward.D2'
  #computing the dissimilarity matrix
  data.dist = dist(measures, method=name_dist_chosen)
  #applying hierarchical clustering with the computed dissimilarity matrix and the selected      linkage
  data.hclust = hclust(data.dist, method=name_linkage_chosen)

  #Inside data.hclust
  #   $merge -> order of aggregation
  #   $height -> distance at which we have aggregation
  #   $order -> ordering that allows to avoid intersection in the dendrogram

  # Plotting the dendogram
  k_chosen = 3
  plot(data.hclust, main=paste(name_dist_chosen,name_linkage_chosen), hang=-0.1, xlab='',        labels=F, cex=0.6, sub='')
  rect.hclust(data.hclust, k=k_chosen)
  
  # How to cut a dendrogram?
  # We generate vectors of labels through the command cutree()
  cluster.cutree = cutree(data.hclust, k=k_chosen)
  table(cluster.cutree)

  ## How good is the clustering?
  # Cophenetic Matrices
  coph.mat <- cophenetic(data.hclust)
  # Cophenetic Coefficients
  coph.coeff = cor(data.dist, coph.mat)
  coph.coeff


  #plot(measures[,1],measures[,3],col=cluster.cutree+1)
  #plot3d(measures, size=3, col=cluster.cutree+1, aspect = F)
  
  ## **2.** Saving the clustering as factors
  
  data_clustered[,cat_var_names[counter_names]] = as.factor(cluster.cutree)
  counter_names = counter_names+ 1
}

head(data_clustered)
```

## **1.3** Exploring the relationship between countries and clusters

```{r}
country_names <- unique(data_clustered$CNT)
g=14
indeces = list()
for (jj in 1:g){
    indeces[jj] <- list(which(data_clustered$CNT == country_names[jj]))
}
n_list = list()
for(j in 1:g) {
  n_list[j] = list(length(indeces[[j]]))
}

####
# per ogni cluster voglio vedere quale percentuale di scuole della stessa nazione ricadono nello stesso
percent_list = list()
counter = 0


for(k in cat_var_names){ # categories
  counter = counter +1  
  percent_list_mid = list()
  counter2=0
  for(i in 1:k_chosen){ # clusters
    counter2 = counter2 +1
  percent = c()
    for(j in 1:g){ # countrys 
      temp = length(data_clustered[which(data_clustered[,k]==i & data_clustered$CNT==country_names[j]),1])
      temp = temp/n_list[[j]]
      percent = c(percent,temp)
    }
    percent_list_mid[counter2]=list(percent)

  }
  percent_list[counter] = list(percent_list_mid)
}


```

```{r}
for(i in 1:3){
  x11()
  plot(measures[,1],measures[,2],col=cluster.cutree+1)
}
```

```{r}
#pdf("cluster_country_proportions.pdf")

for(kk in 1:6){
# Create sample data
group_percentages <- percent_list[[kk]]

groups_names = c()
for (i in 1:k_chosen){
  groups_names = c(groups_names,paste("Group",i))
}
# Convert percentages to data frame

df <- data.frame(
  country = rep(country_names, each = k_chosen),
  group = rep(groups_names, times = length(country_names)),
  percentage = NA
)


for(i in country_names){
  for(j in groups_names){
    df[which((df$group==j) & (df$country==i)),3] = group_percentages[[which(groups_names==j)]][which(country_names==i)]*100
  }
}
  
x11()
# Create stacked bar chart
ggplot(df, aes(x = country, y = percentage, fill = group)) +
  geom_bar(stat = "identity") +
  labs(title = "Proportion of Schools in Each Group by Country",
       x = "Country",
       y = "Proportion") +
  scale_fill_manual(values = c("#E69F00", "#56B4E9", "#009E73"),
                    labels = groups_names) +
  geom_text(aes(label = paste0(round(percentage,2), "%")), 
            position = position_stack(vjust = 0.5))

}
#dev.off()
```



## **3.** DBSCAN
### **3.1** choice of hyperparameters 
```{r}
measures = data_grezzi
colnames(measures)
# Rule of thumb, minPts = dimensionality + 1 = 3 here
# How to choose eps from minPts?
# Plot of the distances to the minPts nearest neighbor
k_chosen_2=2
kNNdistplot(measures, k = k_chosen_2) 
eps_chosen = 50
abline(h = eps_chosen, col = "red", lty = 2)

# Run the dbscan
minpts = 2
dbs <- dbscan(measures, eps = eps_chosen, minPts = minpts)
dbs

```
### **3.2** Silhouette
```{r}
# Let's compute the silhouette score on the clustering performed before
# WARNING (specific to DBSCAN): We need to remove the noise points as they do
# not belong to a cluster, before computing the silhouette score
clustered_index <- which(dbs$cluster != 0) # Index of non noise points
clustered_points <- measures[clustered_index,] # only clustered points
clustered_labels <- dbs$cluster[clustered_index] # corresponding labels

sil <- silhouette(clustered_labels, dist(clustered_points))
summary(sil)

sil_score <- function(labels, dist) {
  # Compute the average of the silhouette widths
  sil <- silhouette(labels, dist)
  sil_widths <- sil[,"sil_width"]
  mean(sil_widths)
}

sil_score(clustered_labels, dist(clustered_points))
```
### **3.3** Grid Search
```{r}
n = dim(measures)[1]
# Grid Search
minPts_grid <- 1:20
eps_grid <- seq(from = 50, length.out=length(minPts_grid), by = 10)

max_share_noise <- 0.2

dbscan_perf <- function(minPts, eps) {
  # Compute the silhouette score resulting from dbscan clustering
  dbs <- dbscan(measures, eps, minPts) # Run dbscan
  
  clustered_index <- which(dbs$cluster != 0) # Index of non noise points
  clustered_points <- measures[clustered_index,] # only clustered points
  clustered_labels <- dbs$cluster[clustered_index] # corresponding labels
  nb_clusters <- length(unique(clustered_labels))
  
  if ((nb_clusters > 1 & nb_clusters < n) & (length(which(dbs$cluster == 0))/n < max_share_noise)) { 
    # Silhouette score is defined only if 2 <= nb_clusters <= n-1
    sil_score(clustered_labels, dist(clustered_points))
  }
  
  else {
    # otherwise we return 0 which would be the approx. value of the silhouette
    # score if the clusters were completely overlapping
    0
  }
}
# We compute the silhouette score for all combinations of minPts and eps
perf_grid <- outer(minPts_grid, eps_grid, FUN = Vectorize(dbscan_perf))
dimnames(perf_grid) <- list(minPts_grid, eps_grid)

# Histogram of the Silhouette scores

hist(perf_grid, breaks = 20, xlab = "Silhouette score", xlim = c(-1, 1), main = NULL)

max_score <- max(perf_grid)
min_score <- min(perf_grid)
max_abs <- max(abs(max_score), abs(min_score))

image.plot(x = eps_grid, y = minPts_grid, z = perf_grid, xlab = "eps", ylab = "minPts",
      main = 'Silhouette score', col = hcl.colors(64, palette = 'Blue-Red'),
      breaks = c(seq(-max_abs, 0, length=33)[-33], seq(0, max_abs, length=33)))

# Retrieve best parameter values
max_score <- max(perf_grid)
argmax_score <- which(perf_grid == max_score, arr.ind = TRUE)
best_eps <- eps_grid[argmax_score[2]]
best_minPts <- minPts_grid[argmax_score[1]]
best_eps
best_minPts
max_score
```
### **3.4** re-run dbscan
```{r}
eps_chosen <- best_eps
minPts_chosen <- best_minPts

dbs <- dbscan(measures, eps = eps_chosen, minPts = minPts_chosen)
dbs


plot(measures, col = dbs$cluster + 1L, pch=19)
```
