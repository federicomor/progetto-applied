---
title: 'Hierarchical clustering'
editor_options: 
  chunk_output_type: inline
---

# Settings

```{r setup}
library(mvtnorm)
library(MVN)
library(rgl)
library(car)
library(dbscan)
library(cluster)
library(fields)
library(ggplot2)
library(GGally)
```

```{r}
#directories
dataset_dir = "../../data/"
include_dir = "../include/Utilities.R"
#including utilities
source(include_dir)
#importing the dataset
pisa_data <- read.csv(file=paste(dataset_dir,"pisa_scores_final.csv",sep=""))
```

```{r}
categories_variables[["TECH"]] = c("Approach.to.ICT","ICTSCH","ICTRES" ,"Use.of.ICT","RATCMP1","ENTUSE" )

categories_variables[["TEACH"]] = c("Teacher.skill","TEACHBEHA","Teachers..degree","PROAT6")

categories_variables[["WELL-BEING"]] = c("Social.well.being","Psychological.well.being")

categories_variables[["SCHOOL"]] = c("EDUSHORT","STAFFSHORT","STRATIO","CLSIZE","CREACTIV")

categories_variables[["STUDENT"]] = c("LM_MINS","ATTLNACT","PV1READ", "PV1MATH" ,"STUBEHA","JOYREAD"  )

categories_variables[["FAMILY"]] = c("ESCS", "HEDRES")
#variabili finite nel dataset
grouped_variables <-categories_variables

group_list <- names(grouped_variables)
```

# Scatterplots

Firstly, we check graphically for possible clusters induced by pairs of variable.

By the graphs we don't see any significative clustering effects

```{r echo=FALSE}
ggpairs(pisa_data[, grouped_variables[[1]]], progress = NULL) #tec
ggpairs(pisa_data[, grouped_variables[[2]]]) #psi
ggpairs(pisa_data[, grouped_variables[[3]]]) #clt
ggpairs(pisa_data[, grouped_variables[[4]]]) #fam
ggpairs(pisa_data[, grouped_variables[[5]]]) #tch
ggpairs(pisa_data[, grouped_variables[[6]]]) #sch
```

# Hierarchical clustering

## Exploration: DISTANCES and LINKAGES

```{r echo=FALSE}
#looping over the categories of variables
for(k in 1:length(grouped_variables)){ 
  #selecting the group of variables
  data_grezzi = pisa_data[,c("CNT",grouped_variables[[k]])] 
  #discarding CNT variable in measures
  meas_labels <-c(2: dim(data_grezzi)[2]) 
  measures = data_grezzi[,meas_labels]
  # EXPERIMENTING DIFFERENT DISTANCES
  #computing dissimiliraties measures for each group of variables
  diss_mat.e = dist(measures, method="euclidean") 
  diss_mat.m = dist(measures, method="manhattan") 
  #storing the dissimilarity matrices in a list
  distances = list(diss_mat.e,diss_mat.m) 
  names = c("euclidean","manhattan")

  #EXPERIMENTING DIFFERENT LINKAGES
  i <- 1 
  for(dist_chosen in distances){ 
    data_grezzi = pisa_data[,c("CNT",grouped_variables[[k]])] 
    name_dist_chosen <- names[i] 
    i <- i+1
    #hierarchical clustering for each type of linkage
    # linkages: "single", "average", "complete", "ward","ward.D2"
    hclust.ds = hclust(dist_chosen, method='single') 
    hclust.da = hclust(dist_chosen, method='average') 
    hclust.dc = hclust(dist_chosen, method='complete') 
    hclust.dw = hclust(dist_chosen, method="ward.D") 
    hclust.dw2 = hclust(dist_chosen, method="ward.D2")
    
    #plotting the resulting dendograms
    par(mfrow=c(2,3))
    plot(hclust.ds, main=paste(name_dist_chosen,'single',group_list[k]),
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='')
    plot(hclust.dc, main=paste(name_dist_chosen,'complete',group_list[k]),
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='')
    plot(hclust.da, main=paste(name_dist_chosen,'average',group_list[k]), 
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='') 
    plot(hclust.dw, main=paste(name_dist_chosen,'ward.D',group_list[k]), 
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='') 
    plot(hclust.dw2, main=paste(name_dist_chosen,'wardD2',group_list[k]), 
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='')
    
    #Evaluting the results of our clustering with the cophenitic correlation coeffincient
    # Cophenetic Matrices
    coph.ds <- cophenetic(hclust.ds)
    coph.dc <- cophenetic(hclust.dc) 
    coph.da <- cophenetic(hclust.da) 
    coph.dw <- cophenetic(hclust.dw) 
    coph.dw2 <- cophenetic(hclust.dw2)
  
    #Cophenetic Coefficients
    ds = cor(diss_mat.e, coph.ds)
    dc = cor(diss_mat.e, coph.dc) 
    da = cor(diss_mat.e, coph.da) 
    dw = cor(diss_mat.e, coph.dw) 
    dw2 = cor(diss_mat.e, coph.dw2)
    
    #output
    print(group_list[k])
    print(name_dist_chosen)
    print(c(ds,dc,da,dw,dw2))
  }
}
```

The output contains the cophenetic correlation coefficients for the following linkages: single, complete, average, ward and wardD2

Note that the best results are always given by euclidean distance and average linkage, but by the dendrograms the clusters induced by these pairs do no seem to be significant and seem to only outliers

These plots and CPCCs coefficients make us think that hierarchical clustering is not a good choice for clustering in our task

## Poor clustering

Example of high CPCC (0.84) and poor clustering results

```{r}
data_grezzi = pisa_data[,c("CNT",grouped_variables[["ICT available at school"]])] 
#discarding CNT variable in measures
meas_labels <-c(2: dim(data_grezzi)[2]) 
measures = data_grezzi[,meas_labels]

#computing dissimiliraties measures
diss_mat.e = dist(measures, method="manhattan") 

#applying hierarchical clustering
hclust.m = hclust(diss_mat.e, method='average') 

#visualizing to select the number of clusters
plot(hclust.m, main=paste("manhattan",'average',"tec"), 
         hang =-0.1, xlab='', labels=F, cex=0.6, sub='') 
```

```{r}
#increasing k does not change the situation
clusters.m <- cutree(hclust.m, k=5) # euclidean-average
table(clusters.m)
```

## Exploring the relationship between countries and clusters

```{r}
country_names <- unique(data_clustered$CNT)
g=16
indeces = list()
for (jj in 1:g){
    indeces[jj] <- list(which(data_clustered$CNT == country_names[jj]))
}
n_list = list()
for(j in 1:g) {
  n_list[j] = list(length(indeces[[j]]))
}

####
# per ogni cluster voglio vedere quale percentuale di scuole della stessa nazione ricadono nello stesso
percent_list = list()
counter = 0


for(k in group_list){ # categories
  counter = counter +1  
  percent_list_mid = list()
  counter2=0
  for(i in 1:k_chosen){ # clusters
    counter2 = counter2 +1
  percent = c()
    for(j in 1:g){ # countrys 
      temp = length(data_clustered[which(data_clustered[,k]==i & data_clustered$CNT==country_names[j]),1])
      temp = temp/n_list[[j]]
      percent = c(percent,temp)
    }
    percent_list_mid[counter2]=list(percent)

  }
  percent_list[counter] = list(percent_list_mid)
}


```

```{r}
for(i in 1:3){
  x11()
  plot(measures[,1],measures[,2],col=cluster.cutree+1)
}
```

```{r}
pdf("cluster_country_proportions.pdf")

for(kk in 1:6){
# Create sample data
group_percentages <- percent_list[[kk]]

groups_names = c()
for (i in 1:k_chosen){
  groups_names = c(groups_names,paste("Group",i))
}
# Convert percentages to data frame

df <- data.frame(
  country = rep(country_names, each = k_chosen),
  group = rep(groups_names, times = length(country_names)),
  percentage = NA
)


for(i in country_names){
  for(j in groups_names){
    df[which((df$group==j) & (df$country==i)),3] = group_percentages[[which(groups_names==j)]][which(country_names==i)]*100
  }
}
  

# Create stacked bar chart
print(ggplot(df, aes(x = country, y = percentage, fill = group)) +
  geom_bar(stat = "identity") +
  labs(title = "Proportion of Schools in Each Group by Country",
       x = "Country",
       y = "Proportion") +
  scale_fill_manual(values = c("#E69F00", "#56B4E9", "#009E73"),
                    labels = groups_names) +
  geom_text(aes(label = paste0(round(percentage,2), "%")), 
            position = position_stack(vjust = 0.5)))

}
dev.off()
```
