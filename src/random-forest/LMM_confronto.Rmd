---
title: "R Notebook"
# output: html_notebook
editor_options:
  chunk_output_type: inline
---

```{r}
library(mvtnorm)
library(MASS)
library(car)
library(rgl)
library(leaps)
library(ISLR)
library(glmnet)
library(lme4)
library(nlmeU) ## --> for the dataset
library(nlme)  ## --> for models implementation

library(corrplot)
library(lattice)
library(plot.matrix)

library(insight)
```

```{r}
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/pisa_scores_final.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
data <- read.csv(file=dataset_path)
head(data)
```

# [0] DATA PREP

```{r}
data$X <- NULL
data$SCHLTYPE <- as.factor(data$SCHLTYPE)
data$CNT <- as.factor(data$CNT)

IM_PUBLIC = rep(0,dim(data)[1])
IM_PUBLIC [which(data$SCHLTYPE=="Public")] = 1
data$IM_PUBLIC = as.factor(IM_PUBLIC)
data$SCHLTYPE <- NULL
```

### creazione unico factor combinato

```{r}
# Example categorical variables
index_cnt = grep("CNT",colnames(data))

colnames(data)

levels(data[,index_cnt])=levels(data$CNT)
cat_var1 <- data[,index_cnt]

index_public= grep("IM_PUBLIC",colnames(data))
levels(data[,index_public])=levels(data$IM_PUBLIC)
cat_var2 <- data[,index_public]

# Combine categorical variables
new_var <- interaction(cat_var1, cat_var2, sep = "-")
levels(new_var)
length(levels(new_var))

data$NEW_VAR = as.factor(new_var)
colnames(data)
```

# [1] Definizione formule

Please tenete questa sezione alla fine delle vostre analisi, mi serve per avere le formule definitive da usare nei modelli per calcolare il punteggio del gioco nel bot. Grazie! :)

```{r}
linear_model_vars <- readLines("../../data/non csv/lm_social_vars.txt")

vars = c(linear_model_vars)
FORMULA_SOCIAL <- paste(paste("Social.well.being","~"), paste(vars, collapse = "+"))

vars = c(linear_model_vars,"(1|CNT)")
FORMULA_SOCIAL_LMM <- paste(paste("Social.well.being","~"), paste(vars, collapse = "+"))
```

```{r}
linear_model_vars <- readLines("../../data/non csv/lm_psico_vars.txt")

vars = c(linear_model_vars)
FORMULA_PSYCH <- paste(paste("Psychological.well.being","~"), paste(vars, collapse = "+"))

vars = c(linear_model_vars,"(1|CNT)")
FORMULA_PSYCH_LMM <- paste(paste("Psychological.well.being","~"), paste(vars, collapse = "+"))
```

# [2] Gestione outliers

## Social

### Leverages

```{r}
fit = lm(FORMULA_SOCIAL,data=data)
summary(fit)

#Punti leva
lev=hatvalues(fit)
sum(lev)

p=fit$rank
n=dim(data)[1]
fs=summary(fit)

#soglia per leverages è 2*p/n

threshold_lev <- 0.025 #rule of thumb 2*p/n = 0.01102435 (seems to low frmo the plot)

watchout_points_lev = lev[ which( lev > threshold_lev) ]
watchout_ids_lev = seq_along( lev )[ which( lev > threshold_lev) ]

plot(fit$fitted.values,lev,pch=16, ylab='lev',xlab='Fitted values',main='Leverages')
abline(h=threshold_lev,col='red')
points( fit$fitted.values[ watchout_ids_lev ], watchout_points_lev, col = 'red', pch = 16 )
#molti valori sopra soglia

sum(lev[lev>threshold_lev])
#somma è 1.65, punti leva pesano 33%
fit2=lm(FORMULA_SOCIAL,data=data,subset=(lev<threshold_lev))
summary(fit2)
#R^2 adj

abs((fit$coefficients-fit2$coefficients)/fit$coefficients)
#Impatto delle leve sui beta
```

### Standardized Residuals

```{r}
#Residui standardizzati

res_std=fit$res/fs$sigma

plot(fit$fitted.values,res_std, pch=16, main='Standardized Residuals',ylab='res',xlab='Fitted values')

threshold_std_res <- 5

abline(h=c(-threshold_std_res,threshold_std_res),col='red')
#punto influente se res standardizzato >2
watchout_ids_rstd = which( abs( res_std ) > threshold_std_res )
watchout_rstd = res_std[ watchout_ids_rstd ]

points( fit$fitted.values[watchout_ids_rstd],
        res_std[watchout_ids_rstd], col = 'red', pch = 16 )
points( fit$fitted.values[watchout_ids_lev],
        res_std[watchout_ids_lev], col = 'orange', pch = 16 )
#in rosso residui alti, in arancione leve
```

### Stundentized residuals

```{r}
#Residui studentizzati
stud = rstandard( fit )

threshold_stud_res <- 5

watchout_ids_stud = which( abs( stud ) > threshold_stud_res )
watchout_stud = stud[ watchout_ids_stud ]

plot( fit$fitted.values, stud, ylab = 'res',xlab='Fitted values', main = "Studentized Residuals", pch = 16 )
points( fit$fitted.values[watchout_ids_stud],
        stud[watchout_ids_stud], col = 'pink', pch = 16 )
points( fit$fitted.values[watchout_ids_lev],
        stud[watchout_ids_lev], col = 'orange', pch = 16 )
abline( h = c(-threshold_stud_res,threshold_stud_res), col = 'red' )
#Residui e residui studentizzati sembrano rilevare stessi punti
```

### Cook's distance

```{r}
#Distanza di Cook
Cdist = cooks.distance( fit )

threshold_cook_dist <- 0.017#rule of thumb 4/(n-p)=0.0009237875 seems too low

watchout_ids_Cdist = which( Cdist >threshold_cook_dist )
watchout_Cdist = Cdist[ watchout_ids_Cdist ]

plot( fit$fitted.values, Cdist, pch = 16, xlab = 'Fitted values',
      ylab = 'distance', main = 'Cooks Distance' )
abline(h=threshold_cook_dist, col='red')
points( fit$fitted.values[ watchout_ids_Cdist ], Cdist[ watchout_ids_Cdist ],
        col = 'green', pch = 16 )
points( fit$fitted.values[watchout_ids_stud],
        Cdist[watchout_ids_stud], col = 'pink', pch = 16 )
points( fit$fitted.values[watchout_ids_lev] ,
        Cdist[watchout_ids_lev], col = 'orange', pch = 16 )
```

### Removing all influential points

```{r}
#punti da tenere non notevoli
id_to_keep1 = !(1:n %in% watchout_ids_Cdist)
id_to_keep2 = !(1:n %in% watchout_ids_stud)
id_to_keep3 = !(1:n %in% watchout_ids_rstd)
id_to_keep4 = !(1:n %in% watchout_ids_lev)

id_to_keep=id_to_keep1 & id_to_keep2 &id_to_keep3 & id_to_keep4

#tolgo tutte le righe notevoli
dati_soc_woo=subset(data,id_to_keep)
ID_TO_KEEP_SOCIAL = id_to_keep
dim(dati_soc_woo)
#si scende a circa 3964 (circa 91% dei dati)
print(paste("From",dim(data)[1],"obs we moved to",dim(dati_soc_woo)[1]))
print(paste("Percentuale di dati sopravvissuti:",dim(dati_soc_woo)[1]/dim(data)[1]*100,"%"))

fit_fin=lm(FORMULA_SOCIAL, dati_soc_woo)
summary(fit_fin)
#R^2 adj è 0.6861 -> alto
#R^2 adj è 0.6898 -> alto (con la modifica di Ettore alla Formula)

```

## Psych

```{r}
fit = lm(FORMULA_PSYCH,data=data)
summary(fit)

#Punti leva
lev=hatvalues(fit)
sum(lev)

p=fit$rank
n=dim(data)[1]
fs=summary(fit)

#soglia per leverages è 2*p/n
watchout_points_lev = lev[ which( lev > 2 * p/n ) ]
watchout_ids_lev = seq_along( lev )[ which( lev > 2 * p/n ) ]

plot(fit$fitted.values,lev,pch=16, ylab='lev',xlab='Fitted values',main='Leverages')
abline(h=2*p/n,col='red')
points( fit$fitted.values[ watchout_ids_lev ], watchout_points_lev, col = 'red', pch = 16 )
#molti valori sopra soglia

sum(lev[lev>2*p/n])
#somma è 1.65, punti leva pesano 33%
fit2=lm(FORMULA_PSYCH,data=data,subset=(lev<2*p/n))
summary(fit2)
#R^2 adj

abs((fit$coefficients-fit2$coefficients)/fit$coefficients)
#Impatto delle leve sui beta



#Residui standardizzati

res_std=fit$res/fs$sigma

plot(fit$fitted.values,res_std, pch=16, main='Standardized Residuals',ylab='res',xlab='Fitted values')
abline(h=c(-2,2),col='red')

#punto influente se res standardizzato >2
watchout_ids_rstd = which( abs( res_std ) > 2 )
watchout_rstd = res_std[ watchout_ids_rstd ]

points( fit$fitted.values[watchout_ids_rstd],
        res_std[watchout_ids_rstd], col = 'red', pch = 16 )
points( fit$fitted.values[watchout_ids_lev],
        res_std[watchout_ids_lev], col = 'orange', pch = 16 )
#in rosso residui alti, in arancione leve



#Residui studentizzati
stud = rstandard( fit )

watchout_ids_stud = which( abs( stud ) > 2 )
watchout_stud = stud[ watchout_ids_stud ]

plot( fit$fitted.values, stud, ylab = 'res',xlab='Fitted values', main = "Studentized Residuals", pch = 16 )
points( fit$fitted.values[watchout_ids_stud],
        stud[watchout_ids_stud], col = 'pink', pch = 16 )
points( fit$fitted.values[watchout_ids_lev],
        stud[watchout_ids_lev], col = 'orange', pch = 16 )
abline( h = c(-2,2), col = 'red' )
#Residui e residui studentizzati sembrano rilevare stessi punti



#Distanza di Cook
Cdist = cooks.distance( fit )

watchout_ids_Cdist = which( Cdist > 4/(n-p) )
watchout_Cdist = Cdist[ watchout_ids_Cdist ]

plot( fit$fitted.values, Cdist, pch = 16, xlab = 'Fitted values',
      ylab = 'distance', main = 'Cooks Distance' )
abline(h=4/(n-p), col='red')
points( fit$fitted.values[ watchout_ids_Cdist ], Cdist[ watchout_ids_Cdist ],
        col = 'green', pch = 16 )



#punti da tenere non notevoli
id_to_keep1 = !(1:n %in% watchout_ids_Cdist)
id_to_keep2 = !(1:n %in% watchout_ids_stud)
id_to_keep3 = !(1:n %in% watchout_ids_rstd)
id_to_keep4 = !(1:n %in% watchout_ids_lev)

id_to_keep=id_to_keep1 & id_to_keep2 &id_to_keep3 & id_to_keep4
```

```{r}
#tolgo tutte le righe notevoli
dati_psi_woo=subset(data,id_to_keep)
ID_TO_KEEP_PSYCH = id_to_keep
dim(dati_psi_woo)
#si scende a circa 3978 (circa 91% dei dati)
print(paste("From",dim(data)[1],"obs we moved to",dim(dati_psi_woo)[1]))
print(paste("Percentuale di dati sopravvissuti:",dim(dati_psi_woo)[1]/dim(data)[1]*100,"%"))

fit_fin=lm(FORMULA_PSYCH, dati_psi_woo)
summary(fit_fin)
#R^2 adj è 0.6861 -> alto
#R^2 adj è 0.5068 -> alto ma meno alto (con la modifica di Ettore alla Formula)

```

# Confronto assumptions

Dati originali, dati versione-1, dati versione-2.

```{r}
suppressWarnings({fit_data = lmer(FORMULA_SOCIAL_LMM,data=data)})
res = residuals(fit_data)
qqnorm(res,main="data")
qqline(res,col="red")

suppressWarnings({fit_data_soc_woo = lmer(FORMULA_SOCIAL_LMM,data=dati_soc_woo)})
res = residuals(fit_data_soc_woo)
qqnorm(res,main="data_soc_woo (version-2)")
qqline(res,col="red")

```

# [3] Creazione dei fit

```{r}
fit_social_lm = lm(   FORMULA_SOCIAL,    data=data_social_woo_scaled)
fit_social_lmm = lmer(FORMULA_SOCIAL_LMM,data=data_social_woo_scaled)

fit_psych_lm = lm(   FORMULA_PSYCH,    data=data_psych_woo)
fit_psych_lmm = lmer(FORMULA_PSYCH_LMM,data=data_psych_woo_scaled)
```

```{r}
sigma2_b <- get_variance_random(fit_social_lmm)
sigma2_eps <- get_variance_residual(fit_social_lmm)
sigma2_f <- get_variance_fixed(fit_social_lmm)

sigma2_b/(sigma2_b+sigma2_eps)
(sigma2_b+sigma2_f)/(sigma2_b+sigma2_eps+sigma2_f)
```

# [4] Assumptions LM

```{r}
#############################
fit = fit_social_lm
df = data_social_woo
# fit = fit_psych_lm
# df = data_psych_woo
#############################
res = fit$residuals
alpha = 0.05

shapiro.test(res)$p
print(paste("Normality of residuals?",shapiro.test(res)$p>alpha))
qqnorm(res)
qqline(res,col="red")

# homoschedasticity
boxplot(res ~ df$CNT,las=2)
```

# [5] Assumptions LMM

```{r}
#############################
fit = fit_social_lmm
df = data_social_woo
# fit = fit_psych_lmm
# df = data_psych_woo
#############################
res = residuals(fit)
raneff = unlist(ranef(fit))
alpha = 0.05

shapiro.test(res)$p
shapiro.test(raneff)$p
print(paste("Normality of residuals?",shapiro.test(res)$p>alpha))
print(paste("Normality of rand effects?",shapiro.test(raneff)$p>alpha))
qqnorm(res)
qqline(res,col="red")
qqnorm(raneff)
qqline(raneff,col="red")

# homoschedasticity
boxplot(res ~ df$CNT,las=2)
```

# [6] Dotplots

## Social

```{r}
#############################
fit = fit_social_lmm
# fit = fit_psych_lmm
#############################

dotplot(ranef(fit, condVar=T))$CNT
ranef(fit)
```

## Psych

```{r}
#############################
# fit = fit_social_lmm
fit = fit_psych_lmm
#############################

dotplot(ranef(fit, condVar=T))$CNT
ranef(fit)
```

# [7] Analisi outliers

```{r}
data_out_social = subset(data,!ID_TO_KEEP_SOCIAL)
data_out_psych = subset(data,!ID_TO_KEEP_PSYCH)
dim(data_out_social)
dim(data_out_psych) # ok sì sono diversi
```

## Social or Psych

```{r}
###########################
df_out = data_out_social
target = "Social.well.being"
#df_out = data_out_psych
#target = "Psychological.well.being"
###########################

for (i in 1:length(STATES)) { # STATES sta in Utilities
	print(paste(STATES[i],":",sum(df_out$CNT==STATES[i]),"outliers // out of",
		  sum(data$CNT==STATES[i]),"obs originally // %lost =",
					sum(df_out$CNT==STATES[i])/sum(data$CNT==STATES[i])*100))
}
```

*Social*: CZE, FIN, LTU, POL, SVK =\> sono peggiori della medie dei dati "normali" ESP, LUX =\> sono migliori della medie dei dati "normali"

Forse ci sta, già ESP era lo stato migliore secondo il LMM, e LUX è uno stato molto ricco. Quindi magari togliendo gli outliers sono state rimosse scuole fin troppo belle nella già bella SPA, e lo stesso vale per LUX.

*Psych*: Sembra molto più sottile qui la questione. Ma comunque ancora ESP e LUX mostrano un comportamento superiore di poco alla media. Insieme ora anche a Croazia (HRV) e DNK.

```{r}
mean_target_woo = mean(df_out[,target])
boxplot(df_out[,target] ~ df_out[,"CNT"],col = colora(14),las=2,main=target)
abline(h=mean_target_woo)
```
