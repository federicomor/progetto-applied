plot(X1, Y, xlab='X1', ylab='Y')
plot(X2, Y, xlab='X2', ylab='Y')
pairs(data)
#plot3d(X1, X2, Y, size=4, asp = T)
# Y = beta_0 + beta_1 * X1 + beta_2 * X2 + Eps
## Assumptions:
# 1) Parameter estimation: E(Eps) = 0 and Var(Eps) = sigma^2
# 2) Inference : Eps ~ N(0, sigma^2)
# Estimate the parameters
fm = lm(Y ~ X1 + X2)
summary(fm)
vif(fm)
# Comment
# * Residual standard error = estimate of sigma
# * Degrees of freedom = n-(r+1)
# * F-statistic - p.value = H0: beta_1 = .. = beta_r = 0
#   (low p-value -> it makes sense to go on)
# * Check for R^2 and R.adj^2
# * Residuals have to be symmetric
# * Pr(>|t|) = p-value of the one-at-time beta test
#   !ATTENTION! Delete one-at-the-time
show = 0
if(show==1){
# Y hat
fitted(fm)
# Eps hat
residuals(fm)
plot(residuals(fm))
# Beta hat
coefficients(fm)
# Cov(beta hat) = sigma^2 * (Z^T * Z)^(-1)
vcov(fm)
# Order of the model (r+1)
fm$rank
# Degrees of freedom for the residuals
fm$df
# Leverages h_ii
hatvalues(fm)
# They quantify:
# 1) How far is the i-th observation from the other ones in the features space
# 2) The influence of the i-th observation on the fit (can be seen as the
# derivative dyhat_i / dy_i)
# Standardized residuals
rstandard(fm)
plot(rstandard(fm))
# Estimate of sigma^2
sum(residuals(fm)^2)/fm$df
}
# (specific case of a regression of the form: Y = b_0 + b_1 * X + b_2 * X^2)
x = seq(0,max(X1)+5,by=0.01)
b = coef(fm)
plot(X1, Y, xlab='X1', ylab='Y')
lines(x, b[1]+b[2]*x+b[3]*x^2)
# If p=2: 3D plot
#par3d(windowRect=c(680,40,1350,720))
#points3d(x=X1, y=X2, z=Y, size=4, aspect = T)
#box3d()
#axes3d()
#points3d(x=X1, y=X2, z=fitted(fm), size=4, col = 'blue')
#surface3d(range(data$X1), range(data$X2),
#          matrix(predict(fm, expand.grid(X1=range(X1), X2=range(X2))),2,2),alpha = 0.5)
## Assuption: Eps ~ N(0, sigma^2)
## Test (Fisher):
##    H0: (beta1, beta2) == (0, 0)
##    H1: (beta1, beta2) != (0, 0)
r = fm$rank - 1  # number of regressors
kk=2
linearHypothesis(fm, cbind(rep(0,kk),diag(kk)), rep(0,kk))
# With more betas (e.g. 3):
# linearHypothesis(fm, rbind(c(0,1,0,0), c(0,0,1,0), c(0,0,0,1)), c(0,0,0))
# Comment
#   Pr(>F) = final p-value in summary(fm)
alpha = 0.05
if(p==2){
# Center
c(coefficients(fm)[2], coefficients(fm)[3])
# Direction of the axes
eigen(vcov(fm)[2:3, 2:3])$vectors
plot(coefficients(fm)[2], coefficients(fm)[3], xlim = c(-6,6), ylim = c(-6,6),
asp=1, xlab='beta1', ylab='beta2')
ellipse(coefficients(fm)[2:3], vcov(fm)[2:3,2:3], sqrt(p*qf(1-alpha,p,n-(r+1))))
abline(v=0)
abline(h=0)
# Comment
#   If it is 'squished' then (probably) they're collinear.
}
qT = qt(1-alpha/(2*p), n-(r+1))
Bf = rbind(
beta1=c(coefficients(fm)[2]-sqrt(vcov(fm)[2,2])*qT,
coefficients(fm)[2]+sqrt(vcov(fm)[2,2])*qT),
beta2=c(coefficients(fm)[3]-sqrt(vcov(fm)[3,3])*qT,
coefficients(fm)[3]+sqrt(vcov(fm)[3,3])*qT))
Bf
# Generic beta_j (p, r, n, alpha generici)
# beta_j = c(coefficients(fm)[j]-sqrt(vcov(fm)[j,j])*qt(1-alpha/(2*p), n-(r+1)),
# coefficients(fm)[j]+sqrt(vcov(fm)[j,j])*qt(1-alpha/(2*p), n-(r+1)))
# Alternatively: Bonferroni's correction
confint(fm, level= 1-alpha/p)[2:3,]
# Test:
#   H0: (beta0+beta2, beta1) == (0,0)
#   H1: (beta0+beta2, beta1) != (0,0)
C = rbind(c(1,0,1), c(0,1,0))
linearHypothesis(fm, C, c(0,0))
# Simultaneous CI (beta_j combinations):
# Fisher 1-alpha quantile
qf.fish = qf(1-alpha, r+1, n-(r+1))
# Customized combination (here we just consider beta_1)
a = c(0,1,0)
# Confidence Interval
sim_IC = c(t(a)%*%coefficients(fm) - sqrt(t(a)%*%vcov(fm)%*%a) * sqrt((r+1)*qf.fish),
t(a)%*%coefficients(fm) + sqrt(t(a)%*%vcov(fm)%*%a) * sqrt((r+1)*qf.fish))
sim_IC
# New sample
Z0.new = data.frame(X1=10, X2=10^2)
# CI(E[Y|X])
Conf = predict(fm, Z0.new, interval='confidence', level=1-alpha)
Conf
# PI(Y)
Pred = predict(fm, Z0.new, interval='prediction', level=1-alpha)
Pred
# Particularity for this model case
plot(X1, Y, xlab='X1', ylab='Y', las=1)
x = seq(0,max(X1)+5,by=0.1)
b = coef(fm)
lines(x, b[1]+b[2]*x+b[3]*x^2)
points(Z0.new$X1,Conf[1], pch=19)
segments(Z0.new$X1,Pred[2],Z0.new$X1,Pred[3],col='gold', lwd=2)
segments(Z0.new$X1,Conf[2],Z0.new$X1,Conf[3],col='red', lwd=2)
points(Z0.new$X1,Conf[2], pch='-', col='red', lwd=2)
points(Z0.new$X1,Conf[3], pch='-', col='red', lwd=2)
points(Z0.new$X1,Pred[2], pch='-', col='gold', lwd=2)
points(Z0.new$X1,Pred[3], pch='-', col='gold', lwd=2)
min = 0
max = 30
Z0   <- data.frame(cbind(X1=seq(min, max, length=100),
X2=seq(min, max, length=100)^2))
Conf = predict(fm, Z0, interval='confidence')
Pred = predict(fm, Z0, interval='prediction')
plot(X1, Y, xlab='X1', ylab='Y', las=1)
lines(Z0[,1], Conf[,'fit'])
lines(Z0[,1], Conf[,'lwr'], lty=2, col='red', lwd=2)
lines(Z0[,1], Conf[,'upr'], lty=2, col='red', lwd=2)
lines(Z0[,1], Pred[,'lwr'], lty=3, col='gold', lwd=2)
lines(Z0[,1], Pred[,'upr'], lty=3, col='gold', lwd=2)
legend('topleft', legend=c('regression line','confidence intervals','prediction intervals'),
col=c('black','red','gold'), lwd=2, cex=0.85)
# Comment
#   These are NOT confidence/prediction BANDS, they're done one-at-the-time.
# Linear regression
logX1 = log(X1)
logY  = log(Y)
fm    = lm(logY~logX1)
# Plot
plot(logX1, logY)
abline(coef(fm)[1], coef(fm)[2])
# Plot IC vs IP: transformed datas' grid
logZ0 = data.frame(logX1=seq(min(logX1), max(logX1), length=100))
Conf  = predict(fm, logZ0, interval='confidence')
Pred  = predict(fm, logZ0, interval='prediction')
plot(logX1, logY, xlab='logX1', ylab='logY', las=1)
lines(logZ0[,1], Conf[,'fit'])
lines(logZ0[,1], Conf[,'lwr'], lty=2, col='red', lwd=2)
lines(logZ0[,1], Conf[,'upr'], lty=2, col='red', lwd=2)
lines(logZ0[,1], Pred[,'lwr'], lty=3, col='gold', lwd=2)
lines(logZ0[,1], Pred[,'upr'], lty=3, col='gold', lwd=2)
legend('topleft', legend=c('regression line','confidence intervals','prediction intervals'),
col=c('black','red','gold'), lwd=2, cex=0.85)
# Comment
#   These are NOT confidence/prediction BANDS, they're done one-at-the-time.
# Plot IC vs IP: transformed datas' grid
plot(X1, Y, xlab='X1', ylab='Y', las=1)
CI = exp(Conf)
PI = exp(Pred)
Z0 = exp(logZ0)
lines(Z0[,1], CI[,'fit'])
lines(Z0[,1], CI[,'lwr'], lty=2, col='red', lwd=2)
lines(Z0[,1], CI[,'upr'], lty=2, col='red', lwd=2)
lines(Z0[,1], PI[,'lwr'], lty=3, col='gold', lwd=2)
lines(Z0[,1], PI[,'upr'], lty=3, col='gold', lwd=2)
legend('topleft', legend=c('regression line','confidence intervals','prediction intervals'),
col=c('black','red','gold'), lwd=2, cex=0.85)
# Comment
#   These are NOT confidence/prediction BANDS, they're done one-at-the-time.
detach(data)
# If the model is terrible, we can try to perform a hierarchical clustering and
# add some dummy vairbales (given by the cluster)
data['X2'] = data['X1']^2
head(data)
# Cluster method: ward
clusterw = cutree(hclust(dist(data), method='ward.D2'),2)
dummy = clusterw - 1
# Model
# Y = b_0 + b_1*X1 + b_2*X2 + b_3*dummy + b_4*dummy*X1 + b_5*dummy*X2
fm2 = lm(Y ~ X1 + X2 + dummy + X1:dummy + X2:dummy, data=data)
summary(fm2)
# Plot
par(mfrow=c(2,2))
plot(fm2)
clusterw
# If the model is terrible, we can try to perform a hierarchical clustering and
# add some dummy vairbales (given by the cluster)
data['X2'] = data['X1']^2
head(data)
# Cluster method: ward
clusterw = cutree(hclust(dist(data), method='ward.D2'),2)
dummy = clusterw - 1
# Model
# Y = b_0 + b_1*X1 + b_2*X2 + b_3*dummy + b_4*dummy*X1 + b_5*dummy*X2
fm2 = lm(Y ~ X1 + X2 + dummy + X1:dummy + X2:dummy, data=data)
summary(fm2)
# Plot
par(mfrow=c(2,2))
plot(fm2)
matplot(data, type='l',lwd=2, xlab='Year', ylab='Vehicles')
legend("topleft",c("Group1", "Group2"),lty=1:2,col=1:2)
# If the model is terrible, we can try to perform a hierarchical clustering and
# add some dummy vairbales (given by the cluster)
data['X2'] = data['X1']^2
head(data)
# Cluster method: ward
clusterw = cutree(hclust(dist(data), method='ward.D2'),2)
dummy = clusterw - 1
matplot(data, type='l',lwd=2, xlab='X', ylab='Y')
legend("topleft",c("Group1", "Group2"),lty=1:2,col=1:2)
# Model
# Y = b_0 + b_1*X1 + b_2*X2 + b_3*dummy + b_4*dummy*X1 + b_5*dummy*X2
fm2 = lm(Y ~ X1 + X2 + dummy + X1:dummy + X2:dummy, data=data)
summary(fm2)
# Plot
par(mfrow=c(2,2))
plot(fm2)
help(matplot)
help(linearHypothesis)
help(rep)
help(I)
#DIRECTORIES
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/pisa_school_final_wo_Outl_PCA_SCORES.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
pisa_data <- read.csv(file=dataset_path)
head(pisa_data)
library(rgl)
library(tsne)
library(Rtsne)
#DIRECTORIES
rm(list=ls())
graphics.off()
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/pisa_wPV_grouped_bysch.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
data <- read.csv(file=dataset_path)
head(pisa_data)
#DIRECTORIES
rm(list=ls())
graphics.off()
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/pisa_wPV_grouped_bysch.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
data <- read.csv(file=dataset_path)
head(data)
library(mvtnorm)
library(MASS)
library(car)
library(rgl)
library(leaps)
library(ISLR)
library(glmnet)
data$X <- NULL
data$schID <- NULL
data$CNT <- as.factor(data$CNT)
data$CNTSCHID <- as.factor(data$CNTSCHID)
data$CNTSTUID <- as.factor(data$CNTSTUID)
data <- data[,23:74] #excluding target variables
head(data)
total_model = c("ATTLNACT","EMOSUPS","COMPETE","GFOFAIL","EUDMO","RESILIENCE","BELONG","BEINGBULLIED",
"PERFEED","CREACTIV","STRATIO","SCHSIZE","CLSIZE","EDUSHORT","STAFFSHORT","STUBEHA","TMINS",
"JOYREAD","CULTPOSS","HEDRES","SCREADCOMP","COMPICT","ICTCLASS","ICTHOME","ICTOUTSIDE","ICTRES",
"ENTUSE","HOMESCH","USESCH","INTICT","AUTICT")
response_variable_total = "SWBP"
########
tech <- c("ICTCLASS","ICTHOME","ICTOUTSIDE","ICTRES","ENTUSE","HOMESCH","USESCH","INTICT","AUTICT")
tech2 <- c("AUTICT","INTICT")
response_variable_tech = "COMPICT"
########
culture = c("JOYREAD","CULTPOSS","HEDRES","SCREADCOMP")
TMINS=data$MMINS+data$LMINS
response_variable_culture = "TMINS"
########
psychology = c("ATTLNACT","EMOSUPS","COMPETE","EUDMO","GFOFAIL","RESILIENCE","BELONG","BEINGBULLIED","PERFEED")
response_variable_psy = "SWBP"
########
school_prof = c("STRATIO","SCHSIZE","CLSIZE","EDUSHORT","STAFFSHORT","STUBEHA")
response_variable_school = "CREACTIV"
########
school_prof_2 = c("SCHSIZE","CLSIZE","STAFFSHORT")
response_variable_school_2 = "STRATIO"
########################
vars = total_model
response_variable = response_variable_total
number_of_covariates = length(vars)
number_of_covariates
vars = tech2
response_variable = response_variable_tech
number_of_covariates = length(vars)
number_of_covariates
formula <- paste(paste(response_variable,"~"), paste(vars, collapse = "+"))
formula
linear_model <- lm(formula,data)
summary(linear_model)
vif(linear_model)
vars = psychology
response_variable = response_variable_psy
number_of_covariates = length(vars)
number_of_covariates
formula <- paste(paste(response_variable,"~"), paste(vars, collapse = "+"))
formula
linear_model <- lm(formula,data)
summary(linear_model)
vif(linear_model)
# * Gaussianity
# * Homoschedasticity
plot(linear_model)
# Comment
#   1. We want to see no pattern: a cloud around the zero
#   2. We want to see a good fit on the line
#   3. Again, we want to see no pattern
#   4. We have the iso-lines of the Cook distance: we can identify the outliers
shapiro.test(residuals(linear_model))
# Best Subset Selection
formula_reg = formula(paste(response_variable,"~."))
regfit.full = regsubsets(formula_reg, data=data,really.big = T)
summary(regfit.full)
# Best Subset Selection: we say when we stop
nv_max = number_of_covariates
regfit.full = regsubsets(Y~., data=data, nvmax=nv_max)
regfit.full = regsubsets(formula_reg, data=data, nvmax=nv_max)
regfit.full = regsubsets(formula_reg, data=data, nvmax=nv_max,really.big = T)
reg.summary = summary(regfit.full)
# Which one we choose:
reg.summary$which
# R-squared
reg.summary$rsq
# R.adj^2
reg.summary$adjr2
# SSres (residual sum of squares)
reg.summary$rss
par(mfrow=c(1,3))
plot(reg.summary$rsq, xlab="Number of Variables", ylab="R-squared", type="b")
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="b")
# We want
# We want the model with max r.adj^2 so we extract the coefficients of that model
# Note: ind = how many coefficients has the model
ind = which.max(reg.summary$adjr2)
coef(regfit.full, ind)
# Graphical table of best results
par(mfrow=c(1,2))
plot(regfit.full, scale="r2", main="Exhaustive search")
plot(regfit.full, scale="adjr2", main="Exhaustive search")
# Which one we choose:
reg.summary$which
k = 10
folds = sample(1:k,nrow(data),replace=TRUE)
table(folds)
# Function that performs the prediction for regsubsets
predict.regsubsets = function(object,newdata,id){
form  = as.formula(object$call[[2]])
mat   = model.matrix(form,newdata)
coefi = coef(object,id=id)
xvars = names(coefi)
mat[,xvars]%*%coefi
}
cv.errors = matrix(NA,k,p, dimnames=list(NULL, paste(1:number_of_covariates)))
p = number_of_covariates+1
cv.errors = matrix(NA,k,p, dimnames=list(NULL, paste(1:number_of_covariates)))
p = number_of_covariates
cv.errors = matrix(NA,k,p, dimnames=list(NULL, paste(1:number_of_covariates)))
for(j in 1:k){
best.fit = regsubsets(Y~.,data=data[folds!=j,],nvmax=number_of_covariates)
for(i in 1:p){
pred = predict(best.fit,data[folds==j,],id=i)
cv.errors[j,i] = mean( (data$Y[folds==j]-pred)^2 )
}
}
for(j in 1:k){
best.fit = regsubsets(formula_reg,data=data[folds!=j,],nvmax=number_of_covariates)
for(i in 1:p){
pred = predict(best.fit,data[folds==j,],id=i)
cv.errors[j,i] = mean( (data$Y[folds==j]-pred)^2 )
}
}
#loaded librarires
library(dplyr)
library(psych) #for KMO test and principal()
library(car) #to apply transformations
library(MVN) #to perform multivariate gaussianity check
library(GGally) #for ggcorr
library(ggplot2)
library(randomForest) #bagging
library(gbm) #boosting
#DIRECTORIES
root_proj_dir = "../../"
dataset_dir = paste(root_proj_dir,"/data/pisa_wPV_grouped_bysch.csv",sep="")
include_dir = paste(root_proj_dir,"/src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_dir)
#DIRECTORIES
root_proj_dir = "../../"
dataset_dir = paste(root_proj_dir,"/data/pisa_wPV_grouped_bysch.csv",sep="")
include_dir = paste(root_proj_dir,"/src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_dir)
#DIRECTORIES
root_proj_dir = "../../"
dataset_dir = paste(root_proj_dir,"/data/pisa_wPV_grouped_bysch.csv",sep="")
include_dir = paste(root_proj_dir,"/src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_dir)
#DIRECTORIES
root_proj_dir = "../../"
dataset_dir = paste(root_proj_dir,"data/pisa_wPV_grouped_bysch.csv",sep="")
include_dir = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_dir)
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/pisa_wPV_grouped_bysch.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
data <- read.csv(file=dataset_path)
setwd("C:/Users/modin/Desktop/Ettore/UNIVERSITA/PISA_PROJECT/progetto-applied/src/random-forest")
#DIRECTORIES
root_proj_dir = "../../"
dataset_dir = paste(root_proj_dir,"/data/pisa_wPV_grouped_bysch.csv",sep="")
include_dir = paste(root_proj_dir,"/src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_dir)
#IMPORTING THE DATASET
pisa_data <- read.csv(file=dataset_dir)
#some adjustments on the data
pisa_data$X <- NULL
pisa_data$schID <- NULL
pisa_data$CNT <- as.factor(pisa_data$CNT)
pisa_data$CNTSCHID <- as.factor(pisa_data$CNTSCHID)
pisa_data$CNTSTUID <- as.factor(pisa_data$CNTSTUID)
#standardizing
transformed_data <- as.data.frame(scale(select_if(pisa_data,is.numeric)))
transformed_data$CNT <- pisa_data$CNT #adding CNT column
pisa_data <- transformed_data
rm(transformed_data)
#variabili finite nel dataset
group_list <- c("tec","psi","clt","fam","tch","sch")
grouped_variables <-list()
#list of grouped variables
grouped_variables[["tec"]] <- c("ICTCLASS","ICTHOME","ICTOUTSIDE","ICTRES","AUTICT","COMPICT","INTICT","ENTUSE","HOMESCH","USESCH", "ICTSCH","RATCMP1")
grouped_variables[["psi"]] <- c("ATTLNACT","EMOSUPS","COMPETE","EUDMO","GFOFAIL","SWBP","RESILIENCE","BELONG","BEINGBULLIED","PERFEED")
grouped_variables[["clt"]] <- c("JOYREAD","CULTPOSS","HEDRES","SCREADCOMP","LMINS","MMINS","STUBEHA")
grouped_variables[["fam"]] <- c("WEALTH","ESCS","HOMEPOS","BFMJ2","BMMJ1","HISCED","HISEI","CULTPOSS","HEDRES","ICTHOME")
grouped_variables[["tch"]] <- c("TEACHINT","TEACHSUP","STIMREAD","PROAT5AB","PROAT5AM","PROAT6","TEACHBEHA")
grouped_variables[["sch"]] <- c("PERCOMP","PERCOOP","ICTSCH","RATCMP1","STRATIO","SCHSIZE","CLSIZE","CREACTIV","EDUSHORT","STAFFSHORT")
#cross-validation
train <- sample(1:nrow(pisa_data), nrow(pisa_data)/2)
#selecting the regressors
included_regressors <- c("ESCS", "ICTCLASS", "COMPICT", "HOMESCH", "RATCMP1", "ICTRES", "ICTHOME", "INTICT", "MMINS")
#print explaination for the included regressors
for(regr in included_regressors){
cat(regr, "\n")
cat(spiega(regr), "\n")
}
#compute the formula
formula_str <- paste("PV1MATH", paste(included_regressors, collapse = "+"), sep ="~")
#performing CV to select the parameter mtry
OOB_error <- double(length(included_regressors))
test_error <- double(length(included_regressors))
for(mtry in 1:length(included_regressors)){
#fitting the model
forest_fit <- randomForest(as.formula(formula_str),
ntree = 500,
data = pisa_data,
subset = train,
mtry = mtry,
importance = TRUE)
OOB_error[mtry] <- forest_fit$mse[400]
#predict the test set value for PV1MATH
pred <- predict(forest_fit, pisa_data[-train,])
# save the test prediction mean square error
test_error[mtry] <- with(pisa_data[-train,],mean((PV1MATH-pred)^2))
}
#plotting the graph of OOB error and test error over mtry
matplot(1:length(included_regressors), cbind(test_error,OOB_error),pch=19,col=c('red','blue'),type='b',ylab="Mean Squared Error")
legend('topright',legend=c("Test","OOB"),pch=19,col=c('red','blue'))
best_fit <- randomForest(as.formula(formula_str),
ntree = 500,
data = pisa_data,
subset = train,
mtry = 3,
importance = TRUE)
# prediction and mse
mean((pisa_data[-train,'PV1MATH']-predict(best_fit, pisa_data[-train,]))^2)
#contribution of the regressors
importance(best_fit)
varImpPlot(best_fit)
