# Linear Mixed Effects Model

## Settings

```{r}
library(mvtnorm)
library(MASS)
library(car)
library(rgl)
library(leaps)
library(ISLR)
library(glmnet)
library(lme4)
library(nlmeU) ## --> for the dataset
library(nlme)  ## --> for models implementation

library(corrplot)
library(lattice)
library(plot.matrix)

library(insight)
```

Directories

```{r}
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/pisa_scores_final.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET

#using the new dataset in this very folder
data <- read.csv(file="std_scores_data.csv")
head(data)
```

Utility

```{r}
MSE <- function(fit, target){
  y_hat <- fitted(fit)
  return(mean((target-y_hat)^2))
}
```

## Data preprocessing

```{r}
data$X <- NULL
data$SCHLTYPE <- as.factor(data$SCHLTYPE)
data$CNT <- as.factor(data$CNT)

IM_PUBLIC = rep(0,dim(data)[1])
IM_PUBLIC [which(data$SCHLTYPE=="Public")] = 1
data$IM_PUBLIC = as.factor(IM_PUBLIC)
data$SCHLTYPE <- NULL
```

# [1] Definizione formule

Social well-being

```{r}
regressors <- names(data)
to_discard <- c("CNT", "Social.well.being", "Psychological.well.being", "SCHLTYPE")
regressors <- regressors[!regressors %in% to_discard]
ME_regressors = c(regressors,"(1|CNT)")

#formula object

FORMULA_SOCIAL_COMPLETE_LMM <- paste(paste("Social.well.being","~"), paste(ME_regressors, collapse = "+"))
```

# [2] Variable selection

Too long

```{r eval=FALSE, include=FALSE}
# fm<- lmer(as.formula(FORMULA_SOCIAL_COMPLETE_LMM),
#            data = data)
# 
# Terms <- terms(fit_LMM)
# 
# todrop <-  1:length(attr(Terms,'term.labels'))
# subs <- unlist(sapply(todrop,
#                       function(p){
#                         combn(todrop,p,simplify=F)
#                         }
#                       ),
#                recursive =F)
# 
# fm.subList <- lapply(subs[-length(subs)],
#                      function(s,...){
#                        newf<- formula(drop.terms(terms(fm),s,keep.response = TRUE))
#                        update(fm,newf)
#                        }
#                      )
# 
# names(fm.subList) <- sapply(fm.subList,
#                             function(x){
#                               paste('fm',attr(terms(x),'term.labels'),sep='.')
#                               }
#                             )
# sort(sapply(fm.subList,BIC))
```

## Backward selection algorithm

```{r}
FORMULA_LMM <- FORMULA_SOCIAL_COMPLETE_LMM
selected_regressors <- regressors

var_todrop <- "not none"

while(var_todrop != "<none>"){
  
  FORMULA_LMM <- paste(paste("Social.well.being","~"), 
                       paste(c(selected_regressors,"(1|CNT)"), collapse = "+"))
  
  fit_LMM<- lmer(as.formula(FORMULA_LMM),
            data = data)
  
  todrop <- drop1(fit_LMM)
  index_todrop <- which.min(todrop$AIC)
  var_todrop <- rownames(todrop[index_todrop,])
  
  cat(var_todrop, "was dropped\n")
  
  selected_regressors <- selected_regressors[selected_regressors != var_todrop]
}
```

```{r}
FORMULA_LMM <- paste(paste("Social.well.being","~"), 
                       paste(c(selected_regressors,"(1|CNT)"), collapse = "+"))
  
fit_LMM<- lmer(as.formula(FORMULA_LMM),
              data = data)

summary(fit_LMM)

MSE(fit_LMM, data$Social.well.being)
```

## Forward selection algorithm

```{r}
add1 <- function(fit, regressors, regs_toadd){
  AIC_improvements <- data.frame(added_regressor = c("<none>", regs_toadd),
                                 AIC = rep(0,length(regs_toadd)+1))
  
  AIC_improvements$AIC[1] <- AIC(fit)
  
  for(i in 2:(length(regs_toadd)+1)){
    new_formula <- paste(paste("Social.well.being","~"), 
                       paste(c(regressors,regs_toadd[i-1],"(1|CNT)"), collapse = "+"))
    
    AIC_improvements$AIC[i] <- AIC(lmer(as.formula(new_formula),
                                               data = data))
  } 
  return(AIC_improvements)
} 
```

```{r}
candidate_regressors <- regressors
selected_regressors <- NULL

var_toadd <- "not null"

FORMULA_LMM <- "Social.well.being ~ (1|CNT)"
fit_LMM<- lmer(as.formula(FORMULA_LMM),
            data = data)

continue_toadd <- TRUE

while(continue_toadd){
  
  toadd <- add1(fit = fit_LMM, 
                regressors = selected_regressors, 
                regs_toadd = candidate_regressors)
  
  index_toadd <- which.min(toadd$AIC)
  var_toadd <- toadd[index_toadd,]$added_regressor
  
  cat(var_toadd, "was added\n")
  
  continue_toadd <- var_toadd != "<none>"
  
  if(continue_toadd){
    candidate_regressors <- candidate_regressors[candidate_regressors != var_toadd]
    selected_regressors <- c(selected_regressors, var_toadd)
    
    new_formula <- paste(paste("Social.well.being","~"), 
                         paste(c(selected_regressors,var_toadd,"(1|CNT)"), collapse = "+"))
    fit_LMM <- update(fit_LMM, new_formula)
  }
}
```

```{r}
fit_LMM

MSE(fit_LMM, data$Social.well.being)
```

# [3] Gestione outliers

## Removing influential points

Leverages

```{r}
fit = lm(FORMULA_SOCIAL,data=data)
summary(fit)

#Punti leva
lev=hatvalues(fit)
sum(lev)

p=fit$rank
n=dim(data)[1]
fs=summary(fit)

#soglia per leverages è 2*p/n

threshold_lev <- 0.012 #rule of thumb 2*p/n = 0.01102435 (seems to low frmo the plot)

watchout_points_lev = lev[ which( lev > threshold_lev) ]
watchout_ids_lev = seq_along( lev )[ which( lev > threshold_lev) ]

plot(fit$fitted.values,lev,pch=16, ylab='lev',xlab='Fitted values',main='Leverages')
abline(h=threshold_lev,col='red')
points( fit$fitted.values[ watchout_ids_lev ], watchout_points_lev, col = 'red', pch = 16 )
#molti valori sopra soglia

sum(lev[lev>threshold_lev])
#somma è 1.65, punti leva pesano 33%
fit2=lm(FORMULA_SOCIAL,data=data,subset=(lev<threshold_lev))
summary(fit2)
#R^2 adj

abs((fit$coefficients-fit2$coefficients)/fit$coefficients)
#Impatto delle leve sui beta
```

Standardized residuals

```{r}
#Residui standardizzati

res_std=fit$res/fs$sigma

plot(fit$fitted.values,res_std, pch=16, main='Standardized Residuals',ylab='res',xlab='Fitted values')

threshold_std_res <- 4

abline(h=c(-threshold_std_res,threshold_std_res),col='red')
#punto influente se res standardizzato >2
watchout_ids_rstd = which( abs( res_std ) > threshold_std_res )
watchout_rstd = res_std[ watchout_ids_rstd ]

points( fit$fitted.values[watchout_ids_rstd],
        res_std[watchout_ids_rstd], col = 'red', pch = 16 )
points( fit$fitted.values[watchout_ids_lev],
        res_std[watchout_ids_lev], col = 'orange', pch = 16 )
#in rosso residui alti, in arancione leve
```

Studentized residuals

```{r}
#Residui studentizzati
stud = rstandard( fit )

threshold_stud_res <- 4

watchout_ids_stud = which( abs( stud ) > threshold_stud_res )
watchout_stud = stud[ watchout_ids_stud ]

plot( fit$fitted.values, stud, ylab = 'res',xlab='Fitted values', main = "Studentized Residuals", pch = 16 )
points( fit$fitted.values[watchout_ids_stud],
        stud[watchout_ids_stud], col = 'pink', pch = 16 )
points( fit$fitted.values[watchout_ids_lev],
        stud[watchout_ids_lev], col = 'orange', pch = 16 )
abline( h = c(-threshold_stud_res,threshold_stud_res), col = 'red' )
#Residui e residui studentizzati sembrano rilevare stessi punti
```

Cook's distance

```{r}
#Distanza di Cook
Cdist = cooks.distance( fit )

threshold_cook_dist <- 0.01 #rule of thumb 4/(n-p)=0.0009237875 seems too low

watchout_ids_Cdist = which( Cdist >threshold_cook_dist )
watchout_Cdist = Cdist[ watchout_ids_Cdist ]

plot( fit$fitted.values, Cdist, pch = 16, xlab = 'Fitted values',
      ylab = 'distance', main = 'Cooks Distance' )
abline(h=threshold_cook_dist, col='red')
points( fit$fitted.values[ watchout_ids_Cdist ], Cdist[ watchout_ids_Cdist ],
        col = 'green', pch = 16 )
points( fit$fitted.values[watchout_ids_stud],
        Cdist[watchout_ids_stud], col = 'pink', pch = 16 )
points( fit$fitted.values[watchout_ids_lev] ,
        Cdist[watchout_ids_lev], col = 'orange', pch = 16 )
```

Removing all the influential poitns

```{r}
#punti da tenere non notevoli
id_to_keep1 = !(1:n %in% watchout_ids_Cdist)
id_to_keep2 = !(1:n %in% watchout_ids_stud)
id_to_keep3 = !(1:n %in% watchout_ids_rstd)
id_to_keep4 = !(1:n %in% watchout_ids_lev)

id_to_keep=id_to_keep1 & id_to_keep2 &id_to_keep3 & id_to_keep4

#tolgo tutte le righe notevoli
dati_soc_woo=subset(data,id_to_keep)

write.csv(dati_soc_woo,"std_scores_data_wo_outliers.csv")

ID_TO_KEEP_SOCIAL = id_to_keep
dim(dati_soc_woo)
#si scende a circa 3964 (circa 91% dei dati)
print(paste("From",dim(data)[1],"obs we moved to",dim(dati_soc_woo)[1]))
print(paste("Percentuale di dati sopravvissuti:",dim(dati_soc_woo)[1]/dim(data)[1]*100,"%"))

fit_fin=lm(FORMULA_SOCIAL, dati_soc_woo)
summary(fit_fin)
```

## Comparing the assumptions

```{r}
suppressWarnings({fit_data = lmer(FORMULA_SOCIAL_LMM,data=data)})
res = residuals(fit_data)
qqnorm(res,main="data", ylim=c(-6,6))
qqline(res,col="red")

suppressWarnings({fit_data_soc_woo = lmer(FORMULA_SOCIAL_LMM,data=dati_soc_woo)})
res = residuals(fit_data_soc_woo)
qqnorm(res,main="data_soc_woo", ylim=c(-6,6))
qqline(res,col="red")
```

# [4] Fitting

## Fitting

## Assumptions

Fixed part

```{r}
#############################
fit = fit_social_lm
df = dati_soc_woo
# fit = fit_psych_lm
# df = data_psych_woo
#############################
res = fit$residuals
alpha = 0.05

shapiro.test(res)$p
print(paste("Normality of residuals?",shapiro.test(res)$p>alpha))
qqnorm(res)
qqline(res,col="red")

# homoschedasticity
boxplot(res ~ df$CNT,las=2)
```

Mixed part

```{r}
#############################
fit = fit_social_lmm
df = dati_soc_woo
# fit = fit_psych_lmm
# df = data_psych_woo
#############################
res = residuals(fit)
raneff = unlist(ranef(fit))
alpha = 0.05

shapiro.test(res)$p
shapiro.test(raneff)$p
print(paste("Normality of residuals?",shapiro.test(res)$p>alpha))
print(paste("Normality of rand effects?",shapiro.test(raneff)$p>alpha))
qqnorm(res)
qqline(res,col="red")
qqnorm(raneff)
qqline(raneff,col="red")

# homoschedasticity
boxplot(res ~ df$CNT,las=2)
```

# [5] Interpretations

## Dotplots

```{r}
#############################
fit = fit_social_lmm
dotplot(ranef(fit, condVar=T))$CNT
ranef(fit)
```

# [6] Analisi outliers

```{r}
data_out_social = subset(data,!ID_TO_KEEP_SOCIAL)
data_out_psych = subset(data,!ID_TO_KEEP_PSYCH)
dim(data_out_social)
dim(data_out_psych) # ok sì sono diversi
```

## Social or Psych

```{r}
###########################
df_out = data_out_social
target = "Social.well.being"

for (i in 1:length(STATES)) { # STATES sta in Utilities
	print(paste(STATES[i],":",sum(df_out$CNT==STATES[i]),"outliers // out of",
		  sum(data$CNT==STATES[i]),"obs originally // %lost =",
					sum(df_out$CNT==STATES[i])/sum(data$CNT==STATES[i])*100))
}
```

*Social*: CZE, FIN, LTU, POL, SVK =\> sono peggiori della medie dei dati "normali" ESP, LUX =\> sono migliori della medie dei dati "normali"

Forse ci sta, già ESP era lo stato migliore secondo il LMM, e LUX è uno stato molto ricco. Quindi magari togliendo gli outliers sono state rimosse scuole fin troppo belle nella già bella SPA, e lo stesso vale per LUX.

*Psych*: Sembra molto più sottile qui la questione. Ma comunque ancora ESP e LUX mostrano un comportamento superiore di poco alla media. Insieme ora anche a Croazia (HRV) e DNK.

```{r}
mean_target_woo = mean(df_out[,target])
boxplot(df_out[,target] ~ df_out[,"CNT"],col = colora(14),las=2,main=target)
abline(h=mean_target_woo)
```
