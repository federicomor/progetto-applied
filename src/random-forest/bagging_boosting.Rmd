---
title: "Bagging & Boosting"
---

# REFERENCES

-   use of caret package: <https://topepo.github.io/caret/model-training-and-tuning.html#custom>

# SETTING UP

```{r}
#loaded librarires
library(dplyr)
library(car) #to apply transformations
library(MVN) #to perform multivariate gaussianity check

library(GGally) #for ggcorr
library(ggplot2)

library(randomForest) #bagging
library(gbm) #boosting

library(caret) #tuner for ML algorithms
```

```{r, setup}
#DIRECTORIES
root_proj_dir = "../../"
dataset_dir = paste(root_proj_dir,"/data/pisa_wPV_grouped_bysch.csv",sep="")
include_dir = paste(root_proj_dir,"/src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_dir)
#IMPORTING THE DATASET
pisa_data <- read.csv(file=dataset_dir)
```

```{r}
#some adjustments on the data
pisa_data$X <- NULL
pisa_data$schID <- NULL
pisa_data$CNT <- as.factor(pisa_data$CNT)
pisa_data$CNTSCHID <- as.factor(pisa_data$CNTSCHID)
pisa_data$CNTSTUID <- as.factor(pisa_data$CNTSTUID)

#standardizing
transformed_data <- as.data.frame(scale(select_if(pisa_data,is.numeric)))
transformed_data$CNT <- pisa_data$CNT #adding CNT column

pisa_data <- transformed_data
rm(transformed_data)
```

```{r}
#variabili finite nel dataset
group_list <- c("tec","psi","clt","fam","tch","sch")
grouped_variables <-list()
#list of grouped variables
grouped_variables[["tec"]] <- c("ICTCLASS","ICTHOME","ICTOUTSIDE","ICTRES","AUTICT","COMPICT","INTICT","ENTUSE","HOMESCH","USESCH", "ICTSCH","RATCMP1")
grouped_variables[["psi"]] <- c("ATTLNACT","EMOSUPS","COMPETE","EUDMO","GFOFAIL","SWBP","RESILIENCE","BELONG","BEINGBULLIED","PERFEED")
grouped_variables[["clt"]] <- c("JOYREAD","CULTPOSS","HEDRES","SCREADCOMP","LMINS","MMINS","STUBEHA")
grouped_variables[["fam"]] <- c("WEALTH","ESCS","HOMEPOS","BFMJ2","BMMJ1","HISCED","HISEI","CULTPOSS","HEDRES","ICTHOME")
grouped_variables[["tch"]] <- c("TEACHINT","TEACHSUP","STIMREAD","PROAT5AB","PROAT5AM","PROAT6","TEACHBEHA")
grouped_variables[["sch"]] <- c("PERCOMP","PERCOOP","ICTSCH","RATCMP1","STRATIO","SCHSIZE","CLSIZE","CREACTIV","EDUSHORT","STAFFSHORT")
```

```{r}
#cross-validation
train <- sample(1:nrow(pisa_data), nrow(pisa_data)/2)
```

# SELECTING THE REGRESSORS

```{r}
#selecting the regressors
included_regressors <- c("ESCS", "ICTCLASS", "COMPICT", "HOMESCH", "RATCMP1", "ICTRES", "ICTHOME", "INTICT", "MMINS")

#print explaination for the included regressors
for(regr in included_regressors){
  cat(regr, "\n") 
  cat(spiega(regr), "\n")
}
#compute the formula
formula_str <- paste("PV1MATH", paste(included_regressors, collapse = "+"), sep ="~")
```

# RANDOM FOREST

## Tuning the hyperparameters

Hyperameters to tune:

-   candidates features for the split at each step while growing a tree:

    -   RANDOM FOREST: mtry = sqrt(p) (rule of thumb) -\> goal: uncorrelate the trees

    -   BAGGING: mtry = p, all the variables are candidates for the split

-   number of trees (ntree)Evaluating the best fit

Tuning mtry

```{r}
# Create model with default paramters
control <- trainControl(method="repeatedcv", 
                        number=10, #number of folder in K-fold CV
                        repeats=3, #number of repetitions
                        verboseIter = TRUE)
set.seed(1)

# grid
tunegrid <- expand.grid(.mtry = c(2,3,4))

# training
rf_training <- train(as.formula(formula_str), 
                     data = pisa_data, 
                     method = "rf", 
                     metric = "Rsquared", 
                     tuneGrid=tunegrid, 
                     trControl=control )

print(rf_training)
```

## Interpretation

```{r}
#contribution of the regressors
importance(rf_training$finalModel)
varImpPlot(rf_training$finalModel)
```

Customizing the training

```{r eval=FALSE, include=FALSE}
customRF <- list(type = "Regression", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes
```

```{r eval=FALSE, include=FALSE}
# train model
control <- trainControl(method="LOOCV", verboseIter = TRUE, number = 1, repeats = 5)
tunegrid <- expand.grid(.mtry= sqrt(length(included_regressors)), .ntree=c(1000, 2500))
set.seed(1)
custom <- train(as.formula(formula_str), 
                data=pisa_data[train,], 
                method=customRF, 
                metric="Rsquared", 
                tuneGrid=tunegrid, 
                trControl=control)
summary(custom)
plot(custom)
```

# BOOSTING

## Tuning the hyperparameters

-   number of trees

-   shrinkage coefficient

-   number of spits while growing the trees

```{r}
# training settings
gbm_control <- trainControl(method="repeatedcv", 
                        number=10, 
                        repeats=1,
                        verboseIter = TRUE)
# setting the grid
gbm_grid <- expand.grid(n.trees = c(1000, 5000),
                        interaction.depth = c(3, 6),
                        shrinkage = c(0.01, 0.001),
                        n.minobsinnode = 20)
                                        
# actual training
gbm_training <- train(as.formula(formula_str), 
                    data=pisa_data, 
                    method="gbm", 
                    metric = "Rsquared", 
                    tuneGrid = gbm_grid, 
                    trControl = gbm_control,
                    verbose = FALSE)

# print the summary
gbm_training
```

## 
