---
title: "Bagging & Boosting"
---

# REFERENCES

-   use of caret package: <https://topepo.github.io/caret/model-training-and-tuning.html#custom>

# SETTING UP

```{r}
#loaded librarires
library(dplyr)
library(car) #to apply transformations
library(MVN) #to perform multivariate gaussianity check

library(GGally) #for ggcorr
library(ggplot2)

library(randomForest) #bagging
library(gbm) #boosting

library(caret) #tuner for ML algorithms
```

```{r, setup}
set.seed(42)
#DIRECTORIES
root_proj_dir = "../../"
dataset_dir = paste(root_proj_dir,"/data/pisa_wPV_grouped_bysch.csv",sep="")
include_dir = paste(root_proj_dir,"/src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_dir)
#IMPORTING THE DATASET
pisa_data <- read.csv(file=dataset_dir)
```

```{r}
#some adjustments on the data
pisa_data$X <- NULL
pisa_data$schID <- NULL
pisa_data$CNT <- as.factor(pisa_data$CNT)
pisa_data$CNTSCHID <- as.factor(pisa_data$CNTSCHID)
pisa_data$CNTSTUID <- as.factor(pisa_data$CNTSTUID)

#standardizing
transformed_data <- as.data.frame(scale(select_if(pisa_data,is.numeric)))
transformed_data$CNT <- pisa_data$CNT #adding CNT column

pisa_data <- transformed_data
rm(transformed_data)
```

```{r}
#variabili finite nel dataset
group_list <- c("tec","psi","clt","fam","tch","sch")
grouped_variables <-list()
#list of grouped variables
grouped_variables[["tec"]] <- c("ICTCLASS","ICTHOME","ICTOUTSIDE","ICTRES","AUTICT","COMPICT","INTICT","ENTUSE","HOMESCH","USESCH", "ICTSCH","RATCMP1")
grouped_variables[["psi"]] <- c("ATTLNACT","EMOSUPS","COMPETE","EUDMO","GFOFAIL","SWBP","RESILIENCE","BELONG","BEINGBULLIED","PERFEED")
grouped_variables[["clt"]] <- c("JOYREAD","CULTPOSS","HEDRES","SCREADCOMP","LMINS","MMINS","STUBEHA")
grouped_variables[["fam"]] <- c("WEALTH","ESCS","HOMEPOS","BFMJ2","BMMJ1","HISCED","HISEI","CULTPOSS","HEDRES","ICTHOME")
grouped_variables[["tch"]] <- c("TEACHINT","TEACHSUP","STIMREAD","PROAT5AB","PROAT5AM","PROAT6","TEACHBEHA")
grouped_variables[["sch"]] <- c("PERCOMP","PERCOOP","ICTSCH","RATCMP1","STRATIO","SCHSIZE","CLSIZE","CREACTIV","EDUSHORT","STAFFSHORT")
```

```{r}
#cross-validation
train <- sample(1:nrow(pisa_data), nrow(pisa_data)/2)
```

# SELECTING THE REGRESSORS

```{r}
#selecting the regressors
included_regressors <- c("ESCS", "ICTCLASS", "COMPICT", "HOMESCH", "RATCMP1", "ICTRES", "ICTHOME", "INTICT", "MMINS")

#print explaination for the included regressors
for(regr in included_regressors){
  cat(regr, "\n") 
  cat(spiega(regr), "\n")
}
#compute the formula
formula_str <- paste("PV1MATH", paste(included_regressors, collapse = "+"), sep ="~")
```

# RANDOM FOREST

## Tuning the hyperparameters

Hyperameters to tune:

-   candidates features for the split at each step while growing a tree:

    -   RANDOM FOREST: mtry = sqrt(p) (rule of thumb) -\> goal: uncorrelate the trees

    -   BAGGING: mtry = p, all the variables are candidates for the split

-   number of trees (ntree)Evaluating the best fit

Tuning mtry:

```{r}
# Create model with default paramters
control <- trainControl(method="repeatedcv", 
                        number=5, #number of folder in K-fold CV
                        repeats=1, #number of repetitions
                        verboseIter = TRUE)

# grid
tunegrid <- expand.grid(.mtry = c(2,3,4))

# training
rf_training <- train(as.formula(formula_str), 
                     data = pisa_data, 
                     method = "rf", 
                     metric = "Rsquared", 
                     tuneGrid=tunegrid, 
                     trControl=control)

#printing the training results
print(rf_training)

#to see what's inside the method used to fit
getModelInfo(model = "rf", regex = FALSE)
```

## Interpretation

```{r}
#contribution of the regressors
importance(rf_training$finalModel)
varImpPlot(rf_training$finalModel)
```

Try to:

-   customize the method (see the caret page)

-   mixed effect random forest <https://www.mate.polimi.it/biblioteca/add/qmox/36-2020.pdf>

```{r}

```

# BOOSTING

## Tuning the hyperparameters

-   number of trees

-   shrinkage coefficient

-   number of spits while growing the trees

```{r}
# training settings
gbm_control <- trainControl(method="repeatedcv", 
                        number=5, 
                        repeats=1,
                        verboseIter = TRUE)
# setting the grid
gbm_grid <- expand.grid(n.trees = c(5000,10000),
                        interaction.depth = c(3, 6),
                        shrinkage = c(0.01, 0.05),
                        n.minobsinnode = 10)
                                        
# actual training
gbm_training <- train(as.formula(formula_str), 
                    data=pisa_data, 
                    method="gbm", 
                    metric = "Rsquared", 
                    tuneGrid = gbm_grid, 
                    trControl = gbm_control,
                    verbose = FALSE)

#to see what's inside the method used to fit
getModelInfo(model = "gbm", regex = FALSE)

# print the summary
gbm_training
```

## 
