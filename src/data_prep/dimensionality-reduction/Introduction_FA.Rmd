---
output: html_document
editor_options: 
  chunk_output_type: inline
---
# Factor Analysis

## Notes and references

-   Exploratory Factor Analysis (EFA) tutorial: <https://rpubs.com/pjmurphy/758265>

-   notes from the textbook "Applied Multivariate Statistical Analysis" in the same directory of this folder

-   reference paper of past year project on Well-Being

-   source: <https://www.geo.fu-berlin.de/en/v/soga/Geodata-analysis/factor-analysis/A-simple-example-of-FA/index.htmlZ>

-   according to this article (<https://journals.sagepub.com/doi/full/10.1177/0095798418771807>) it is better to perform EFA only on variables we suspect to be generated by a latent factor. Thus, it is better to follow the same procedure we used for PCA

## Settings

```{r}
#loaded librarires
library(dplyr)
library(psych) #for KMO test and principal()
library(car) #to apply transformations
library(MVN) #to perform multivariate gaussianity check
library(GGally) #for ggcorr
library(ggplot2)
library(elasticnet) #trying out sparse principal component analysis [spca()]
library(tidyverse)
```

```{r, setup}
#DIRECTORIES
root_proj_dir = "../../../"
dataset_dir = paste(root_proj_dir,"/data/pisa_data_final.csv",sep="")
include_dir = paste(root_proj_dir,"/src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_dir)
#IMPORTING THE DATASET
pisa_data <- read.csv(file=dataset_dir)
colnames(pisa_data)
head(pisa_data)
```

```{r}
#some adjustments on the data
pisa_data$X <- NULL
pisa_data$CNT <- as.factor(pisa_data$CNT)
pisa_data$SCHLTYPE <- as.factor(pisa_data$SCHLTYPE)
head(pisa_data)
```

# Data preprocessing: standardization

Saving plausible values in a separate dataframe. Later we'll add them to the scores dataset

```{r}
#paluesible values dataset
plausible_values <- pisa_data %>% select(starts_with("PV"))
#excluding PV frmo the dataset to compute the factor analysis
pisa_data <- pisa_data %>% select(-starts_with("PV")) #excluding target variables
```

```{r}
#standardize the variables
transformed_data <- as.data.frame(scale(select_if(pisa_data,is.numeric)))
transformed_data$CNT <- pisa_data$CNT #adding CNT column

pisa_data <- transformed_data
rm(transformed_data)
```

Quick look at the data

```{r}
boxplot(select_if(pisa_data,is.numeric),las=2)
```

# Grouping the variables

Grouping variables suspected to be generated by a latent factor (The same variable may appear in more than one group, according to our choices):

```{r}
grouped_variables <-list()
#list of grouped variables

##Technology
#c("ICTCLASS","ICTHOME","ICTOUTSIDE","ICTRES","AUTICT","COMPICT","INTICT","ENTUSE","HOMESCH","USESCH", "ICTSCH","RATCMP1")

grouped_variables[["ICT at home"]] <- c("ICTHOME","ICTRES","ENTUSE","HOMESCH")
grouped_variables[["Relationship with ICT"]] <- c("AUTICT","COMPICT","INTICT","ENTUSE", "HOMESCH", "USESCH")
grouped_variables[["ICT at school"]] <- c("ICTCLASS","ICTOUTSIDE","USESCH", "ICTSCH","RATCMP1")

##Psichology
grouped_variables[["Psychological Well-being"]] <- c("EUDMO","SWBP","GFOFAIL", "RESILIENCE", "COMPETE")
grouped_variables[["Social Well-Being"]] <- c("PERCOOP","PERCOMP","EMOSUPS","BELONG","BEINGBULLIED")

##Culture
grouped_variables[["Cultural possesion"]] <- c("CULTPOSS","HEDRES")
grouped_variables[["Attitude towards school"]] <- c("LMINS","MMINS","STUBEHA","ATTLNACT")
grouped_variables[["Reading"]] <- c("JOYREAD", "SCREADCOMP")

##Family
grouped_variables[["Family economic status"]] <- c("WEALTH","ESCS","HOMEPOS","BFMJ2","BMMJ1","HISCED","HISEI")

### consider only ESCS
grouped_variables[["Family economic status"]] <- "ESCS"


##Teaching
grouped_variables[["Teachers' degree"]] <- c("PROAT5AB","PROAT5AM","PROAT6")
grouped_variables[["Teacher support"]] <- c("TEACHINT","TEACHSUP","STIMREAD","TEACHBEHA")

##School
grouped_variables[["School size"]] <- c("STRATIO","SCHSIZE","CLSIZE")
grouped_variables[["Equipment of the school"]] <- c("EDUSHORT","STAFFSHORT")

#groups
group_list <- names(grouped_variables)
```

Some graphs to validate the choice made while grouping the variables

```{r}
p_ <- GGally::print_if_interactive

#overall correlation overview
ggcorr(select_if(pisa_data, is.numeric),
       label = TRUE,
       label_size = 0.5)

for(group_name in group_list){
  
  p_(ggcorr(pisa_data[,grouped_variables[[group_name]]],
         label=TRUE,label_size = 2) +
    ggtitle(group_name))
}
```

# Performing EFA

## Preliminary tests

-   cortest.bartlett(): test to see if the variables in the dataframe are uncorrellated. H0: correlation matrix sigma = Identity matrix (i.e. uncorrelated variables)

-   KMO: ratio of correlations and partial correlations that reflects the extent to which correlations are a function of the variance shared across all variables rather than the variance shared by particular pairs of variables

-   multivariate normality to perform factor analysis with the maximum likelihood estimation method

```{r}
for(group in names(grouped_variables)){
  cat("\nGroup: ", group,"\n\n")
  print(KMO(pisa_data[,grouped_variables[[group]]]))
}
```

A desirable value for KMO is \>=0.70, below 0.5 unacceptable and we cannot apply EFA

## Selecting the number of factors

the following methods are useful to select the initial number of factors. The final choice should be made based on the portion of variance explained

(Note that scree plots are contained in parallel analysis)

```{r}
nfactors <- list()

for(group in group_list){
  #scree plot
  #scree(pisa_data[,grouped_variables[[group]]], 
  #      pc=TRUE, 
  #      main = paste("Scree plot:", group))
  
  #parallel analysis
  parallel.an <- fa.parallel(pisa_data[,grouped_variables[[group]]], 
              main = paste("Parallel Analysis Scree Plots:",group))
  
  nfactors[[group]]$FA <- parallel.an$nfact
  nfactors[[group]]$PC <- parallel.an$ncomp
  
  #a method from psych package with many more criteria
  #nfactors(cor(pisa_data[,grouped_variables[[group]]]),
  #         title = paste("Number of factors", group),
  #         plot=FALSE)
}
```

Fix on number of factors/components

```{r}

```

## Fitting the models

Options:

-   fa(): performs various types of EFA, we'll use the MinRes method which make no normal assumpition (as opposed to ML method) [package psych]

-   principal(): performs PCA in a more sophisticated way, using rotations to inrease the interpretability of the components. See the documentation in this folder [package psych]

```{r}
#TRIED methods:

# FA with principal factor method
fit_FA_MinRes <- list() 
# rotated PCA
fit_PCA_rotated <- list()

#fitting the models
for(group in group_list){
  
  #FA with MinRes
  fit_FA_MinRes[[group]] <- fa(r = cor(pisa_data[,grouped_variables[[group]]]),
                            nfactors = nfactors[[group]]$FA,
                            rotate = "promax")
  #advanced PCA
  fit_PCA_rotated[[group]] <- principal(r = cor(pisa_data[,grouped_variables[[group]]]),
                               nfactors = nfactors[[group]]$PC, 
                               rotate = "promax")
}
```

**A note on rotation:** loading matrix (L) is unique up to rotation made through orthogonal matrices. Thus, we have can provide as an option to this functions a rotation method to ease the interpretability of the loadings. There a two kind of rotations:

-   orthogonal rotation methods: if we assume that the extracted factors are independent

    -   "varimax": optimized to reduce cross loadings and to minimize smaller loading values, making factor models clearer

    -   "quartimax": works to reduce the number of variables needed to explain a factor, making interpretation easier

    -   "equamax": compromise between varimax and quartimax

-   oblique rotation methods: if we assume that the extracted factors are not independent

    -   "promax": Promax rotation is popular for its ability to handle large datasets efficiently. The approach also tends to result in greater correlation values between factors.

    -   The direct "oblimin" rotation approach is somewhat less efficient with large datasets, but can produce a simpler factor structure.

[reference: <https://rpubs.com/pjmurphy/758265> for]

# GOF

Overall overview of all the methods

Metrics used:

-   alpha: I did not include this in the end. It should be used to evaluate the internal consistency of the factors. To do this, we should use the variables significantly (loading \> threshold) associated with a factor and evaluate their correlation.

-   communalities: percentage of the variability of a single variable in the original dataframe explained by the common factors

-   fit: $\Psi$ residual matrix $$
    1 - \frac{\sum_{i,j}{(\Psi\Psi^T)_{i,j}}}{\sum_{i,j}{(\text{R}\text{R}^T)_{i,j}}}
    $$

-   variance explained (it is the average of the communalities)

```{r}
for(group in group_list){
  cat("\n-----------Group:", group, "-----------\n")
  
  #alpha score
  #cat("\nalpha:\n")
  #alpha <- psych::alpha(pisa_data[,grouped_variables[[group]]], check.keys = T)
  #cat(alpha$total[1]$raw_alpha)
  
  #number of factors and components
  cat("\n\nNumbr of factors: ", nfactors[[group]]$FA)
  cat("\nNumbr of components: ", nfactors[[group]]$PC)
  
  #MinRes factor analysis
  cat("\n\n### MinRes Factor Analysis ###\n")
  #communalities
  cat("-> Communalities:\n")
  print(fit_FA_MinRes[[group]]$communality)
  cat("-> Unique variance:\n")
  print(fit_FA_MinRes[[group]]$uniquenesses)
  
  #fit score
  cat("-> Fit of the model\n")
  print(factor.fit(r = cor(select_if(pisa_data[,grouped_variables[[group]]],is.numeric)),
             fit_FA_MinRes[[group]]))
  
  #variance explained
  cat("-> summary:\n")
  print(fit_FA_MinRes[[group]]$Vaccounted)
  
  
  #rotated PCA
  cat("\n\n### PCA ###\n")
  cat("-> fit: ", fit_PCA_rotated[[group]]$fit,"\n")
  cat("-> summary:\n")
  print(fit_PCA_rotated[[group]]$Vaccounted)
  
  
}
```

# Loadings

## MinRes FA 

Visualizing the loadings:

```{r}
#Factor Analysis with method "MinRes"
for(group in group_list){
  fa.diagram(fit_FA_MinRes[[group]])
  for(i in 1:dim(fit_FA_MinRes[[group]]$loadings)[2]){
    barplot(fit_FA_MinRes[[group]]$loadings[,i], main = paste("factor",group,i),las=2,cex.names=0.7)
  }
}
```

## Rotated PCA

Visualize the loadings:

```{r}
#Principal Component method
for(group in group_list){
  for(i in 1:nfactors[[group]]$PC){
    barplot(fit_PCA_rotated[[group]]$loadings[,i], main = paste("factor",group,i),las=2,cex.names=0.7)
  }
}
```

# Thoughts

-   Family Economic status: all the variables are highly correlated with ESCS (some of them are even used to compute ESCS), we can keep only ESCS and discard all the others, since they are not carrying additional information

-   Is it really necessary to reduce dimensionality for all the ICT variables? Are they possibly generated by a latent factor? Maybe it can be useful too have many measurements of different aspects and then perform variable selection later on, while constructing a predictive model

-   The same goes for the school variables. Note that school variables and some of the ICT variables are the only ones that a policy maker can control (e.g. we cannot control the economic status of a student, that is given, but we can implement policies at school level. For example we can decide how much schools should invest in technology). Thus, if we want to make an analysis useful for policy making we should keep as many variables of this type as possible.

-   For the psychological variables a synthetic score is much needed, especially if we want to use it as a target of our analysis

-   Also if we want to use other targets, we may use PCA or FA to synthetized a suitable score

# Writing the dataset of scores

Utility to compite the scores

```{r}
scores <- function(fit,data){
  return(as.data.frame(as.matrix(data)%*%fit$loadings))
}

colnames(pisa_data)

scores_data = list()
for(group in group_list){
    scores_data[group] = list(scores(fit_PCA_rotated[[group]],pisa_data[,grouped_variables[[group]]]))
}

scores_data=data.frame(scores_data)
```
