# Reducing dimensionality with PCA

## Settings

Libraries

```{r}
#loaded librarires
library(dplyr)
library(psych) #for KMO test and principal()
library(car) #to apply transformations
library(MVN) #to perform multivariate gaussianity check
library(GGally) #for ggcorr
library(ggplot2)
library(tidyverse)
```

Directories

```{r, setup}
#DIRECTORIES
root_proj_dir = "../../../"
dataset_dir = paste(root_proj_dir,"/data/pisa_data_final.csv",sep="")
include_dir = paste(root_proj_dir,"/src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_dir)
#IMPORTING THE DATASET
pisa_data <- read.csv(file=dataset_dir)
colnames(pisa_data)
head(pisa_data)
```

Adjustment on the dataset

```{r}
pisa_data$X <- NULL
pisa_data$CNT <- as.factor(pisa_data$CNT)
pisa_data$SCHLTYPE <- as.factor(pisa_data$SCHLTYPE)
head(pisa_data)
# remove PV
pisa_data <- pisa_data %>% select(-(starts_with("PV") & !contains("1")))
# LMINS and MMINS averaged
pisa_data$LM_MINS = (pisa_data$LMINS+pisa_data$MMINS)/2

head(pisa_data)
```

## Data preprocessing 

-\> standardize the variables

```{r}
transformed_data <- as.data.frame(scale(select_if(pisa_data,is.numeric)))
data <- transformed_data

col_names_pisa = colnames(pisa_data)
col_names_data = colnames(data)
factor_vars <- setdiff(col_names_pisa,col_names_data)
factor_vars # those needed to be added later
```

-\> remove variables that we don't want to reduce

-   Psychological variables are used to compute the well-being index that we want to predict

-   Variables related to the economic status of the student's family are synthesized in ESCS

```{r}
#variables that we won't use anymore
remove_vars = c("EUDMO","SWBP","GFOFAIL", "RESILIENCE", "COMPETE",  #Psychological Well-being
                  "PERCOOP","PERCOMP","EMOSUPS","BELONG","BEINGBULLIED", #Social Well-Being
                  "WEALTH","HOMEPOS","BFMJ2","BMMJ1","HISCED","HISEI", # Family
                  "LMINS","MMINS" #just the average (Total learning time)
)

#original variables kept
original_vars = c("ESCS",
                  "ENTUSE",
                  "ICTRES",
                  "HEDRES",
                  "CULTPOSS",
                  "STUBEHA","ATTLNACT","JOYREAD","LM_MINS", #interest in studying
                  "PROAT5AB","PROAT5AM","PROAT6", #Teachers' level of education
                  "TEACHBEHA",
                  "STRATIO","CLSIZE","STAFFSHORT","EDUSHORT","RATCMP1","ICTSCH","CREACTIV", 
                  "SCHSIZE", 
                  "PV1MATH","PV1READ" #performances in pisa test
                  )
```

# Rotated PCA

## Groupings

```{r}
grouped_variables <-list()
#list of grouped variables

#Perceived Competence and Interest in ICT
grouped_variables[["CompInt.ICT"]] <- c("AUTICT","COMPICT","INTICT")
#Overall use of ICT related to school activities
grouped_variables[["SchoolUse.ICT"]] <- c("HOMESCH","USESCH","ICTCLASS","ICTOUTSIDE")
#Teaching ability
grouped_variables[["Teacher.skills"]] <- c("TEACHINT","TEACHSUP","STIMREAD","PERFEED") 

#groups
group_list <- names(grouped_variables)

names_grouped =c( (unlist(grouped_variables)),original_vars,factor_vars,remove_vars)
col_names_data = colnames(pisa_data)
difference1 <- setdiff(col_names_data,names_grouped)
difference1
```

### Correlation within groups

```{r}
p_ <- GGally::print_if_interactive

for(group_name in group_list){
  
  p_(ggcorr(data[,grouped_variables[[group_name]]],
         label=TRUE,label_size = 2) +
    ggtitle(group_name))
}

```

## Selecting the number of factors per group

The following methods are useful to select the initial number of factors. The final choice should be made based on the portion of variance explained

(Note that scree plots are contained in parallel analysis)

```{r,eval=F}
nfactors <- list()

for(group in group_list){
  #scree plot
  #scree(pisa_data[,grouped_variables[[group]]], 
  #      pc=TRUE, 
  #      main = paste("Scree plot:", group))
  
  #parallel analysis
  parallel.an <- fa.parallel(data[,grouped_variables[[group]]], 
              main = paste("Parallel Analysis Scree Plots:",group))
  
  nfactors[[group]] <- parallel.an$ncomp
  
  #a method from psych package with many more criteria
  #nfactors(cor(pisa_data[,grouped_variables[[group]]]),
  #         title = paste("Number of factors", group),
  #         plot=FALSE)
}
```

```{r}
nfactors_2 <- list()

for(group in group_list){
 nfactors_2[[group]] <- 1
}
```

## Fitting the models

-   principal(): performs PCA in a more sophisticated way, using rotations to inrease the interpretability of the components. See the documentation in this folder [package psych]

what the next chunk does is - define a "soglia" of explained variance for every group - perform a first pca with the previously chosen number of components - increase by one the number of components until every group has an explained variance that reaches the "soglia"

```{r}
soglia = 0.40
nfactors = nfactors_2

iter = 1
cum_var = list()
while(iter<5){  # it would be better to have a better check, but it's fast and works

  fit_PCA_rotated <- list()
  #fitting the models
  for(group in group_list){
    
    #advanced PCA
    fit_PCA_rotated[[group]] <- principal(r = cor(data[,grouped_variables[[group]]]),
                                 nfactors = nfactors[[group]], 
                                 rotate = "promax")
    cum_var[[group]]=sum(fit_PCA_rotated[[group]]$Vaccounted["Proportion Var",])
  }

  bool = cum_var<soglia
  groups_low_cumvar = names(nfactors[bool])
  for(k in groups_low_cumvar )
    nfactors[[k]] =nfactors[[k]] +1

  iter = iter +1
}
```

Results

```{r}
# results
for(group in group_list){
  cat("\n-----------Group:", group, "-----------\n")
  
  #number of factors and components
  cat("\nNumbr of components: ", nfactors[[group]])
  cat("\n-> fit: ", fit_PCA_rotated[[group]]$fit,"\n")
  cat("-> cumulative variance explained:\n", cum_var[[group]],"\n")
}
```

## Loadings

```{r}
for(group in group_list){
  for(i in 1:nfactors[[group]]){
    barplot(fit_PCA_rotated[[group]]$loadings[,i], main = paste("factor",group,i),las=2,cex.names=0.7)
  }
}
```

# Compute the scores dataset

## Well-being indeces

```{r}
include_dir2 = paste(root_proj_dir,"/src/include/computing_WBindex.R",sep="")
source(include_dir2) ###!!!including new file!!!###
```

Recall to scale the dataset

```{r}
#standardize the variables
transformed_data <- as.data.frame(scale(select_if(pisa_data,is.numeric)))
transformed_data$CNT <- pisa_data$CNT #adding CNT column
transformed_data$SCHLTYPE <- pisa_data$SCHLTYPE #adding CNT column

pisa_data <- transformed_data
rm(transformed_data)
```

Actually computing WB index

```{r}
scores_WB <- computing_scoresWB(pisa_data, root_proj_dir)
names_s = names(scores_WB)
scores_WB = data.frame(scores_WB)
colnames(scores_WB)=names_s
head(scores_WB)
```

## PCA results & original variables

```{r}
#(1) addinng original vars
original_vars= c(original_vars,factor_vars)

scores_original = list()
for (var in original_vars)
  scores_original[[var]] = pisa_data[[var]]

scores_original = data.frame(scores_original)
head(scores_original)

#(2) adding PCA scores
scores <- function(fit,data){
  return(as.data.frame(as.matrix(data)%*%fit$loadings))
}

scores_data = list()

for(group in group_list){
    scores_data[[group]] = scores(fit_PCA_rotated[[group]],
                                  pisa_data[,grouped_variables[[group]]])
}

scores_data=data.frame(scores_data)
colnames(scores_data)=group_list
head(scores_data)

scores_data_final = cbind(scores_data,scores_original)
head(scores_data_final)
```

## Final dataset

```{r}
scores_data_final= cbind(scores_WB,scores_data_final)
head(scores_data_final)
```

Save Dataframe of scores

```{r}
saving_path <- paste(root_proj_dir,"src/random-forest/std_scores_data.csv",sep="")

write.csv(scores_data_final,saving_path)
```
