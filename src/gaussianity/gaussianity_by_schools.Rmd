---
title: "gaussianity"
output: 
date: "2023-04-07"
editor_options: 
  chunk_output_type: inline
---

# NOTES

-   for MANOVA we need multivariate gaussianity for each group

-   for ANOVA we need univariate gaussianity for each group

-   we do not need multivariate gaussianity for the whole dataset since we do not want to perform tests on the mean for the whole dataset

# REFERENCES

-   stats.stackechange post that tells that it is highly improbable to have gaussianity with a large number of observations: <https://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless> (confirmed by one of Masci mails)

# PRELIMINARY STUFF

```{r, setup}
#DIRECTORIES
root_proj_dir = "../../"
dataset_dir = paste(root_proj_dir,"/data/pisa-woNA_school_final.csv",sep="")
include_dir = paste(root_proj_dir,"/src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_dir)
#IMPORTING THE DATASET
pisa_data <- read.csv(file=dataset_dir)
```

```{r}
#some adjustments on the data
pisa_data$X <- NULL
pisa_data$schID <- as.factor(pisa_data$schID)
```

# CHECKING GAUSSIANITY

Reference: `Lab_4.R`

From `Lab_4.R` we have many options to test normality of data:

-   **Henze-Zikler's test:** giving subset = CNT as an option since we want to check normality in the groups

-   **Shapiro-Wilk test family**: limited to 5000 observations

-   We can reject Gaussianity also with univariate tests like the ANDERSON-DARLING test. However, we can only reject gaussianity and not accept it, since gaussianity of the components is a necessary (but not sufficient) condition for the multivariate gaussianity.

# TESTING NORMALITY

In `result` we can see also the results of the univariate tests on gaussianity

```{r}
library(MVN)
library(dplyr)
#Heinze-Zikler to test multivariate normality
result <- mvn(data = subset(pisa_data,select = -schID), subset = "CNT", mvnTest = "hz",univariateTest = "SW")
#With the dataset grouped by schools we can perform also the multivariate version of shapiro test
library(mvnormtest)
for(){
  
}

#non capisco cosa sia questo
#ks.test(select_if(pisa_data,is.numeric),"pnorm")
# https://it.wikipedia.org/wiki/Test_di_Kolmogorov-Smirnov#Descrizione_del_test_a_due_code_-_un_campione
# sembra un test per studiare se un insieme di dati hanno una certa distribuzione
# / H0: F(x) = F0(X), tipo legge normale
# \ H1: ecc
```

# QQPLOT

```{r}
qq_plot_gaussianity <- function(data){
  # Compute the quantiles of the data
  quantiles <- quantile(data, probs = seq(0, 1, by = 0.01))

  # Compute the theoretical quantiles of a normal distribution
  norm_quantiles <- qnorm(seq(0, 1, by = 0.01), mean = mean(data), sd = sd(data))

  # Create the QQ-plot
  plot(norm_quantiles, quantiles, main = "QQ-plot", xlab = "Theoretical quantiles", ylab = "Sample quantiles")
  abline(0, 1, col = "red")  # add a reference line
}

# Call the QQ-plot function with the dataset
for(i in 4:dim(pisa_data)[2]){
  qq_plot_gaussianity(pisa_data[,i])
}
```

# REMOVING OUTLIERS

## Problems

-   Singular estimated covariance matrix S: according to chatGPT one of the causes of singular covariance matrices could be the presence of highly correlated variables. As shown in \`exploratory-analysis/exploring-data-by-schools.Rmd\` it is also our case. The problem maybe solve by PCA by groups, since we're transforming our data to obtain uncorrelated variables (i.e. Principal Components)

```{r}
#plotting the correlation matrix between variables top have an insight on the
library(GGally)
library(ggplot2)
library(dplyr)
ggcorr(select_if(pisa_data,is.numeric))

#removing/modifying variables
pisa_data$X <- NULL
pisa_data$schID <- as.factor(pisa_data$schID)

#computing the determinant of the covariance matrix
det(cov(select_if(subset(pisa_data,select = -c(LMINS,MMINS,BFMJ2,BMMJ1,HISEI)),is.numeric)))
image(cov(select_if(subset(pisa_data,select = -c(LMINS,MMINS,BFMJ2,BMMJ1,HISEI)),is.numeric)))
```

```{r}
pisa_data$schID<-as.factor(pisa_data$schID)
M <- colMeans(select_if(pisa_data,is.numeric))
S <- cov(select_if(pisa_data,is.numeric))

#computing mahlanobis distance
d2 <- matrix(mahalanobis(select_if(pisa_data,is.numeric), M, S))
#removing over a certain distance from the mean
pisa_wo_outliers<- pisa_data[which(d2 <= 8), ] #too many observations removed with 8
dim(pisa_data)
dim(pisa_wo_outliers)
#testing again on the dataset wo outliers
result_wo_ouliers <- mvn(data = pisa_wo_outliers, subset = "CNT", mvnTest = "hz",univariateTest = "AD")
```

# TRANSFORMING

```{r}
library(car)
bcn_param <- powerTransform(select_if(pisa_data,is.numeric), family = "bcnPower") #take long
trasformed_data <- bcnPower(U=select_if(pisa_wo_outliers,is.numeric),lambda=bcn_param$lambda, gamma=bcn_param$gamma)
#add the CNT column
transformed_data["CNT"] <- pisa_wo_outliers$CNT

#testing on transformed data
result_wo_ouliers <- mvn(data = transformed_data, subset = "CNT", mvnTest = "hz",univariateTest = "AD")
```
