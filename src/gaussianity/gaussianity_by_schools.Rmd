---
title: "gaussianity"
output: 
date: "2023-04-07"
editor_options: 
  chunk_output_type: inline
---

# REFERENCES AND NOTES

-   for MANOVA we need multivariate gaussianity for each group, for ANOVA we need univariate gaussianity for each group
-   stats.stackechange post that tells that it is highly improbable to have gaussianity with a large number of observations: <https://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless> (confirmed by one of Masci mails)

# PRELIMINARY STUFF

```{r, setup}
#DIRECTORIES
root_proj_dir = "../../"
dataset_dir = paste(root_proj_dir,"/data/pisa-woNA_school_final.csv",sep="")
include_dir = paste(root_proj_dir,"/src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_dir)
#IMPORTING THE DATASET
pisa_data <- read.csv(file=dataset_dir)
```

```{r}
#some adjustments on the data
pisa_data$X <- NULL
pisa_data$schID <- as.factor(pisa_data$schID)
pisa_data$CNT <- as.factor(pisa_data$CNT)
```

# TESTING NORMALITY

Reference: `Lab_4.R`

From `Lab_4.R` we have many options to test normality of data:

-   **Henze-Zikler's test:** giving subset = CNT as an option we check normality in the groups

-   **Shapiro-Wilk test family**: limited to 5000 observations

-   We can reject Gaussianity also with univariate tests like the ANDERSON-DARLING or SHAPIRO-WILK test. However, we can only reject gaussianity and not accept it, since gaussianity of the components is a necessary (but not sufficient) condition for the multivariate gaussianity.

Normality in the full dataset highly improbable (almost 5000 obs, -\>see the stats.stackexchange post in the references)

```{r}
library(MVN)
library(dplyr)
#Heinze-Zikler: in result we can see also the results of the univariate tests on gaussianity
result_hz <- mvn(data = select_if(pisa_data,is.numeric), mvnTest = "hz",univariateTest = "SW")
#multivariate shapiro on the whole dataset
result_shapiro <- mshapiro.test(t(select_if(pisa_data,is.numeric)))
```

Normality in the groups (MANOVA assumption)

```{r}
#Heinze-Zikler: in result we can see also the results of the univariate tests on gaussianity
result_hz_cntry <- mvn(data = subset(pisa_data,select = -schID), subset = "CNT", mvnTest = "hz",univariateTest = "SW")

#Multivariate Shapiro-test
library(mvnormtest)
result_shapiro_cntry <- list()

for(cntry in unique(pisa_data$CNT)){
  result_shapiro_cntry[[cntry]] <- mshapiro.test(t(select_if(pisa_data[pisa_data$CNT==cntry,],is.numeric)))
}
```

```{r}
#ks.test(select_if(pisa_data,is.numeric),"pnorm")
# https://it.wikipedia.org/wiki/Test_di_Kolmogorov-Smirnov#Descrizione_del_test_a_due_code_-_un_campione
# sembra un test per studiare se un insieme di dati hanno una certa distribuzione
# / H0: F(x) = F0(X), tipo legge normale
# \ H1: ecc
```

# QQPLOTS

Normality in the whole dataset for each variable

```{r}
qq_plot_gaussianity <- function(data,variable_name){
  # Compute the quantiles of the data
  quantiles <- quantile(data, probs = seq(0, 1, by = 0.01))
  # Compute the theoretical quantiles of a normal distribution
  norm_quantiles <- qnorm(seq(0, 1, by = 0.01), mean = mean(data), sd = sd(data))
  # Create the QQ-plot
  plot(norm_quantiles, quantiles, main = variable_name, xlab = "Theoretical quantiles", ylab = "Sample quantiles")
  abline(0, 1, col = "red")  # add a reference line
}

#Call the QQ-plot function with the full dataset
for(i in 4:dim(pisa_data)[2]){
  qq_plot_gaussianity(pisa_data[,i],colnames(pisa_data)[i])
}
```

Plotting normality by groups for each variables

-   the plots are quite. I suspect the presence of some bug in my code

```{r}
palette <- colorRampPalette(rainbow(length(unique(pisa_data$CNT))))
palette <- palette(length(unique(pisa_data$CNT)))
for(variable in colnames(pisa_data)){
  if(variable != "schID" && variable != "CNT"){
    #calling the plot
    plot(1, type = "n", xlab = "Theoretical quantiles", ylab = "Sample quantiles", main = variable)
    #looping over countries
    idx <- 1
    quantiles <- list()
    norm_quantiles <- list()
    for(cntry in unique(pisa_data$CNT)){
      # Compute the quantiles of the data
      quantiles[[cntry]]<- quantile(pisa_data[pisa_data$CNT==cntry, variable], probs = seq(0, 1, by = 0.01))
      # Compute the theoretical quantiles of a normal distribution
      norm_quantiles[[cntry]] <- qnorm(seq(0, 1, by = 0.01), mean = mean(pisa_data[pisa_data$CNT==cntry, variable]), sd = sd(pisa_data[pisa_data$CNT==cntry,variable]))
      #plotting
      points(norm_quantiles[[cntry]], quantiles[[cntry]], col = palette[idx])
      idx <- idx + 1
    }
    abline(0, 1, col = "black")
    legend("bottomright", legend = unique(pisa_data$CNT), col = palette, pch = 1, cex = 0.75)
  }
}
```

# REMOVING OUTLIERS

```{r}
#computing estimated mean and covariance
M <- colMeans(select_if(pisa_data,is.numeric))
S <- cov(select_if(pisa_data,is.numeric))
#chack that S is non-singular
det(S)
#computing mahlanobis distance from the mean for each observation
d2 <- matrix(mahalanobis(select_if(pisa_data,is.numeric), M, S))

#removing over a certain distance from the mean:
threshold_dist <- 50 #we need to tune this parameter
pisa_wo_outliers<- pisa_data[which(d2 <= threshold_dist), ] 
```

```{r}
#testing again on the dataset wo outliers
result_wo_ouliers <- mvn(data = subset(pisa_wo_outliers,select = -schID), subset = "CNT", mvnTest = "hz",univariateTest = "SW")
```

# TRANSFORMING

```{r}
library(car)
#must be strictly positive 
transformed_data <- select_if(pisa_wo_outliers,is.numeric)
transformed_data <- transformed_data+abs(min(transformed_data)) + 1e-5 #1e-5 epsilon per ottenere disuguaglianza stretta
lambda <- powerTransform(transformed_data)  

for(i in length(lambda$lambda)){
   transformed_data[,i]<- bcPower(U=transformed_data[,i],lambda=lambda$lambda[i])
}

#add the CNT and schID columns
transformed_data["CNT"] <- pisa_wo_outliers$CNT
transformed_data["schID"] <- pisa_wo_outliers$schID
```

```{r}
#testing again on transformed data
result_trans_wo_ouliers <- mvn(data = subset(transformed_data,select = -schID), subset = "CNT", mvnTest = "hz",univariateTest = "SW")
```

# ISSUES LOGBOOK

-   Singular estimated covariance matrix S

    -   according to chatGPT one of the causes of singular covariance matrices could be the presence of highly correlated variables. As shown in \`exploratory-analysis/exploring-data-by-schools.Rmd\` it is also our case. The problem may be solveD by PCA by groups, since we're transforming our data to obtain uncorrelated variables (i.e. Principal Components)

    -   Anyway, it was not our case this time, since the singular covariance matrix was due to some variable included in the dataset
