---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---
# TO DO
- transforming the dataset (if necessary)
- Ridge and Lasso (?)
- Plots (if number of variables permits it)
- introduction of groups (for LMM)

```{r, setup}
#DIRECTORIES
rm(list=ls())
graphics.off()
root_proj_dir = "../../"
dataset_path = paste(root_proj_dir,"data/df_pca_scores.csv",sep="")
include_path = paste(root_proj_dir,"src/include/Utilities.R",sep="")
#INCLUDING UTILITIES
source(include_path)
#IMPORTING THE DATASET
data <- read.csv(file=dataset_path)
head(data)
library(mvtnorm)
library(MASS)
library(car)
library(rgl)
library(leaps)
library(ISLR)
library(glmnet)
library(lme4)
library(nlmeU) ## --> for the dataset
library(nlme)  ## --> for models implementation

library(corrplot)
library(lattice)
library(plot.matrix)


library(insight)

library(ggplot2)
```

```{r}
data$X <- NULL
data$CNT <- as.factor(data$CNT)
#data <- data[,23:74] #excluding target variables
head(data)
```

# choice of the model

total_model = c("ATTLNACT","EMOSUPS","COMPETE","GFOFAIL","EUDMO","RESILIENCE","BELONG","BEINGBULLIED",
                "PERFEED","CREACTIV","STRATIO","SCHSIZE","CLSIZE","EDUSHORT","STAFFSHORT","STUBEHA","TMINS",
                "JOYREAD","CULTPOSS","HEDRES","SCREADCOMP","COMPICT","ICTCLASS","ICTHOME","ICTOUTSIDE","ICTRES",
                "ENTUSE","HOMESCH","USESCH","INTICT","AUTICT")
response_variable_total = "SWBP"

########
tech <- c("ICTCLASS","ICTHOME","ICTOUTSIDE","ICTRES","ENTUSE","HOMESCH","USESCH","INTICT","AUTICT")
tech2 <- c("AUTICT","INTICT")
response_variable_tech = "COMPICT"
########

culture = c("JOYREAD","CULTPOSS","HEDRES","SCREADCOMP")
TMINS=data$MMINS+data$LMINS
response_variable_culture = "TMINS"
########

psychology = c("ATTLNACT","EMOSUPS","COMPETE","EUDMO","GFOFAIL","RESILIENCE","BELONG","BEINGBULLIED","PERFEED")
response_variable_psy = "SWBP"
########

school_prof = c("STRATIO","SCHSIZE","CLSIZE","EDUSHORT","STAFFSHORT","STUBEHA")
response_variable_school = "CREACTIV"
########

school_prof_2 = c("SCHSIZE","CLSIZE","STAFFSHORT")
response_variable_school_2 = "STRATIO"
########################


```{r}
all_vars = colnames(data)

tech <- colnames(data[grep("tec", all_vars)])
tech
psic <- colnames(data[grep("psi", all_vars)])
psic
culture <- colnames(data[grep("clt", all_vars)])
culture
family <- colnames(data[grep("fam", all_vars)])
family
teach <- colnames(data[grep("tch", all_vars)])
teach
school <- colnames(data[grep("sch", all_vars)])
school
```


```{r}
response_variable = psic[6]
response_variable
vars = all_vars
vars = vars[vars!=response_variable]
vars= vars[vars!="CNT"]
vars

not_signific_vars = c("col1_clt","col3_tch","col4_tch","col3_sch")
for(i in 1:length(not_signific_vars)){
  spiega(not_signific_vars[i])
  print("")
}

vars= vars[!(vars %in% not_signific_vars )]
vars
number_of_covariates = length(vars)
number_of_covariates
```

# run the model
```{r}
formula <- paste(paste(response_variable,"~"), paste(vars, collapse = "+"),paste("+(1|CNT)"))
formula_classic <- paste(paste(response_variable,"~"), paste(vars, collapse = "+"))
formula
linear_model_classic <- lm(formula_classic,data)
summary(linear_model_classic)
```


```{r}
linear_model <- lmer(formula,data)
summary(linear_model)
print("######################### VIF ##########################")
vif(linear_model)
# save.image("from_lmm.RData")
```

## Combination of variables with beta coefficients
```{r}
##??
df=data
# coefficienti tratti dal summary
tec_beta_comb = 
    (-0.008862) * df$col1_tec+
    ( 0.029719) * df$col2_tec+
    ( 0.011752) * df$col3_tec+
    (-0.005932) * df$col4_tec+
    ( 0.015692) * df$col5_tec

psi_beta_comb = ( 
  0.282455) * df$col1_psi+
    (-0.137583) * df$col2_psi+
    ( 0.190003) * df$col3_psi+
    ( 0.238503) * df$col4_psi+
    ( 0.239341) * df$col5_psi+
    (-0.038022) * df$col7_psi

clt_beta_comb =
    (-0.028202) * df$col2_clt+
    ( 0.005754) * df$col3_clt

fam_beta_comb = (-0.008007) * df$col1_fam

tch_beta_comb = 
    ( 0.034128) * df$col1_tch+
    (-0.068108) * df$col2_tch

sch_beta_comb = 
    ( 0.007906) * df$col1_sch+
    (-0.005847) * df$col2_sch+
    (-0.005905) * df$col4_sch

##??
df_tec=df
boxplot(tec_beta_comb ~ df_tec$CNT, col=fun_colori(length(unique(df$CNT))),
        las=2,main = "tec combined with betas")
boxplot(psi_beta_comb ~ df_tec$CNT, col=fun_colori(length(unique(df$CNT))),
        las=2,main = "psi combined with betas")
boxplot(clt_beta_comb ~ df_tec$CNT, col=fun_colori(length(unique(df$CNT))),
        las=2,main = "clt combined with betas")
boxplot(fam_beta_comb ~ df_tec$CNT, col=fun_colori(length(unique(df$CNT))),
        las=2,main = "fam combined with betas")
boxplot(tch_beta_comb ~ df_tec$CNT, col=fun_colori(length(unique(df$CNT))),
        las=2,main = "tch combined with betas")
boxplot(sch_beta_comb ~ df_tec$CNT, col=fun_colori(length(unique(df$CNT))),
        las=2,main = "sch combined with betas")

```

## Clustering sulle combination?
Sembra ci siano divisioni per contributi migliori/peggiori, ma all'interno delle
scuole dei vari stati, non tra i singoli stati uno contro l'altro.


```{r}
cols=colori_fun(14,52)
breaks=c(0,which((df$CNT[1:length(df$CNT)-1]==df$CNT[2:length(df$CNT)])==FALSE))+181
```


```{r}
val_cur = tec_beta_comb

plot(val_cur, col=cols[as.numeric(as.factor(df$CNT))])
abline(v=breaks, lty=2,col="gray")
text(unique(df$CNT),x=breaks,y=1,cex=0.5)

val_clust = hclust(dist(val_cur,method="euclidean"),method="complete")
plot(val_clust,hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
clusts = cutree(val_clust,k=3)

plot(val_cur, col=cols[as.numeric(as.factor(df$CNT))],pch=clusts)
abline(v=breaks, lty=2,col="gray")
text(unique(df$CNT),x=breaks,y=-0.2,cex=0.5)
```

```{r}
val_cur = psi_beta_comb

plot(val_cur, col=cols[as.numeric(as.factor(df$CNT))])
abline(v=breaks, lty=2,col="gray")
text(unique(df$CNT),x=breaks,y=1,cex=0.5)

val_clust = hclust(dist(val_cur,method="euclidean"),method="complete")
plot(val_clust,hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
clusts = cutree(val_clust,k=3)

plot(val_cur, col=cols[as.numeric(as.factor(df$CNT))],pch=clusts)
abline(v=breaks, lty=2,col="gray")
text(unique(df$CNT),x=breaks,y=1,cex=0.5)
```

```{r}
val_cur = clt_beta_comb

plot(val_cur, col=cols[as.numeric(as.factor(df$CNT))])
abline(v=breaks, lty=2,col="gray")
text(unique(df$CNT),x=breaks,y=1,cex=0.5)

val_clust = hclust(dist(val_cur,method="euclidean"),method="complete")
plot(val_clust,hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
clusts = cutree(val_clust,k=3)

plot(val_cur, col=cols[as.numeric(as.factor(df$CNT))],pch=clusts)
abline(v=breaks, lty=2,col="gray")
text(unique(df$CNT),x=breaks,y=-0.15,cex=0.5)
```




# RANDOM EFFECTS

questo pu? darci un'idea del contributo del paese per il well being
```{r}
rand_eff<-ranef(linear_model)
dotplot(rand_eff)
```

```{r}
qqnorm(resid(linear_model))
qqline(resid(linear_model))

qqnorm(unlist(ranef(linear_model)$CNT))
qqline(unlist(ranef(linear_model)$CNT))

hist(resid(linear_model))
```


#  Verify assumptions (used for inference and estimate of parameters)
```{r, warning=FALSE, message=FALSE}
# * Gaussianity
# * Homoschedasticity
plot(linear_model,col=data$CNT)
boxplot(linear_model_classic$residuals ~ data$CNT, col=unique(data$CNT),
        xlab='countrys', ylab='Residuals') 
abline(h=0)


# Comment
#   1. We want to see no pattern: a cloud around the zero
#   2. We want to see a good fit on the line
#   3. Again, we want to see no pattern
#   4. We have the iso-lines of the Cook distance: we can identify the outliers

shapiro.test(residuals(linear_model))

```


# Variable selection

## **1.** Best Subset Selection (exhaustive search) 
```{r, warning=FALSE, message=FALSE}
# Best Subset Selection
formula_reg = formula(paste(response_variable,"~."))
regfit.full = regsubsets(formula_reg, data=data[,!(colnames(data)=="CNT")],really.big = T)
summary(regfit.full)
```


```{r, warning=FALSE, message=FALSE}
# Best Subset Selection: we say when we stop
nv_max = number_of_covariates
regfit.full = regsubsets(formula_reg, data=data, nvmax=nv_max,really.big = T)
summary(regfit.full)

reg.summary = summary(regfit.full)

# Which one we choose:
reg.summary$which

# R-squared
reg.summary$rsq

# R.adj^2
reg.summary$adjr2

# SSres (residual sum of squares)
reg.summary$rss

# Plots
par(mfrow=c(1,3))
plot(reg.summary$rsq, xlab="Number of Variables", ylab="R-squared", type="b")
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="b")

# We want the model with max r.adj^2 so we extract the coefficients of that model
# Note: ind = how many coefficients has the model
ind = which.max(reg.summary$adjr2)
coef(regfit.full, ind)

# Graphical table of best results
par(mfrow=c(1,2))
plot(regfit.full, scale="r2", main="Exhaustive search")
plot(regfit.full, scale="adjr2", main="Exhaustive search")

```

## **2.** Forward and Backward Stepwise Selection
```{r, warning=FALSE, message=FALSE}
# Forward
regfit.fwd = regsubsets(formula_reg,data=data,nvmax=nv_max,method="forward")
summary(regfit.fwd)

# Plot
par(mfrow=c(1,3))
plot(summary(regfit.fwd)$rsq, xlab="Number of Variables", ylab="R-squared", type="b")
plot(summary(regfit.fwd)$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
plot(summary(regfit.fwd)$rss, xlab="Number of Variables", ylab="RSS", type="b")

par(mfrow=c(1,2))
plot(regfit.fwd,scale="r2",main="Forward Stepwise Selection")
plot(regfit.fwd,scale="adjr2",main="Forward Stepwise Selection")

# Backward
regfit.bwd = regsubsets(formula_reg,data=data,nvmax=nv_max,method="backward")
summary(regfit.bwd)

# Plot
par(mfrow=c(1,3))
plot(summary(regfit.bwd)$rsq, xlab="Number of Variables", ylab="R-squared", type="b")
plot(summary(regfit.bwd)$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="b")
plot(summary(regfit.bwd)$rss, xlab="Number of Variables", ylab="RSS", type="b")

par(mfrow=c(1,2))
plot(regfit.bwd,scale="r2",main="Backward Stepwise Selection")
plot(regfit.bwd,scale="adjr2",main="Backward Stepwise Selection")

```

## **3.** Comparison
```{r, warning=FALSE, message=FALSE}
coef(regfit.full,7) # Exhaustive search
coef(regfit.fwd,7) # Forward Stepwise Selection
coef(regfit.bwd,7) # Backward Stepwise Selection
```

## **4.** K-fold-cross-validation (exhaustive search)
```{r, warning=FALSE, message=FALSE}
k = 10
folds = sample(1:k,nrow(data),replace=TRUE)
table(folds)

# Function that performs the prediction for regsubsets
predict.regsubsets = function(object,newdata,id){
  form  = as.formula(object$call[[2]])
  mat   = model.matrix(form,newdata)
  coefi = coef(object,id=id)
  xvars = names(coefi)
  mat[,xvars]%*%coefi
}
p = number_of_covariates
cv.errors = matrix(NA,k,p, dimnames=list(NULL, paste(1:number_of_covariates)))

for(j in 1:k){
  best.fit = regsubsets(formula_reg,data=data[folds!=j,],nvmax=number_of_covariates,really.big=T)
  for(i in 1:p){
    pred = predict(best.fit,data[folds==j,],id=i)
    cv.errors[j,i] = mean( (data$Y[folds==j]-pred)^2 )
  }
}

cv.errors

root.mean.cv.errors = sqrt(apply(cv.errors,2,mean)) # average over the columns
root.mean.cv.errors

# Plot
plot(root.mean.cv.errors,type='b')
points(which.min(root.mean.cv.errors),
       root.mean.cv.errors[which.min(root.mean.cv.errors)], col='red',pch=19)
which.min(root.mean.cv.errors)

# Estimation on the full dataset
reg.best = regsubsets(Y~.,data=data, nvmax=number_of_covariates)
coef(reg.best,10)

```

# Inference on Betas
```{r, warning=FALSE, message=FALSE}
## Assuption: Eps ~ N(0, sigma^2)
## Test (Fisher):
##    H0: (beta1, beta2) == (0, 0)
##    H1: (beta1, beta2) != (0, 0)
r = linear_model_classic$rank - 1  # number of regressors

# needs to be specified
kk=9
linearHypothesis(linear_model, cbind(rep(0,kk),diag(kk)), rep(0,kk))
linear_model
# Comment
#   Pr(>F) = final p-value in summary(fm)
```


#  Bonferroni Intervals 
```{r, warning=FALSE, message=FALSE}
alpha = 0.05
n = dim(data)[1]
qT = qt(1-alpha/(2*p), n-(r+1))

vals = 1:20
C = diag(20)
Bf = c()
for(j in vals){
  Bf = rbind(Bf,
         c( (C %*%coefficients(linear_model_classic))[j]-sqrt((C %*%vcov(linear_model_classic) %*% t(C))[j,j])*qT,
            (C %*%coefficients(linear_model_classic))[j]+sqrt((C %*%vcov(linear_model_classic) %*% t(C))[j,j])*qT))
}

Bf = data.frame(Bf)
all(Bf[,1]*Bf[,2]>0)

# Generic beta_j (p, r, n, alpha generici)
# beta_j = c(coefficients(fm)[j]-sqrt(vcov(fm)[j,j])*qt(1-alpha/(2*p), n-(r+1)),
#            coefficients(fm)[j]+sqrt(vcov(fm)[j,j])*qt(1-alpha/(2*p), n-(r+1)))

# Alternatively: Bonferroni's correction
confint(linear_model_classic, level= 1-alpha/p)[vals,]
```




# Inferenza sui beta classic model
```{r}
## Assuption: Eps ~ N(0, sigma^2)
## Test (Fisher):
##    H0: (beta1, beta2) == (0, 0)
##    H1: (beta1, beta2) != (0, 0)
r = linear_model_classic$rank - 1  # number of regressors

# needs to be specified
kk=19
linearHypothesis(linear_model_classic, cbind(rep(0,kk),diag(kk)), rep(0,kk))
linear_model_classic

```

# Ridge regression classic model
```{r}
#try a range of lambda values, then choose the one that minimizes generalized cross validation  
lambda.c <- seq(30,40,0.01)
#varie prove di intervalli (zoom su quello contenente lambda opt)

#fit.ridge <- lm.ridge(response_variable ~ data$col1_tec+data$col2_tec+data$col3_tec+data$col4_tec+data$col5_tec+data$col1_psi+data$col2_psi+data$col3_psi+data$col4_psi+data$col5_psi+ data$col7_psi+data$col1_clt+data$col2_clt+data$col3_clt+data$col1_fam+data$col1_tch+data$col2_tch+data$col3_tch+data$col4_tch+data$col1_sch+data$col2_sch+data$col3_sch +data$col4_sch,lambda=lambda.c)
fit.ridge <- lm.ridge(formula_classic,data,lambda=lambda.c)

lambda.opt <- lambda.c[which.min(fit.ridge$GCV)]
lambda.opt  
#molto dritte, sembrano quasi delle rette (?)

for (i in 1:number_of_covariates) {
  plot(lambda.c,coef(fit.ridge)[,i], type='l', xlab=expression(lambda),
     ylab=expression(beta[i-1]))
  abline(v=lambda.opt, col=2, lty=2)
}

coef.ridge <- coef(fit.ridge)[which.min(fit.ridge$GCV),]
coef.ridge


#lambdaopt=33.95
#              col1_tec    col2_tec    col3_tec    col4_tec    col5_tec    col1_psi    col2_psi    col3_psi    col4_psi    col5_psi    col7_psi    col2_clt    col3_clt 
# 0.06927790 -0.02416874  0.06091557  0.03385272  0.02213204  0.04015382  0.43701278 -0.08521387  0.16889777  0.26573159  0.21113793 -0.10879213  0.04765208  0.03400456 
#   col1_fam    col1_tch    col2_tch    col1_sch    col2_sch    col4_sch 
#-0.02785968  0.05312325 -0.19340668  0.04877980 -0.03586436 -0.04417708 

#non identici ma molto simili a coefficienti precedenti

```

# Lasso regression classic model
```{r}
#predictors matrix and response vector
x <- model.matrix(as.formula(formula_classic),data)[,-1]
y <- data$col6_psi

lambda.grid <- 10^seq(5,-3,length=100)
fit.lasso <- glmnet(x,y, lambda = lambda.grid) 

plot(fit.lasso,xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
legend('topright', dimnames(x)[[2]], col =  rainbow(dim(x)[2]), lty=1, cex=1)

#cross validation for lambda
cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid)

bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso

plot(cv.lasso)
abline(v=log(bestlam.lasso), lty=1)


coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')[1:20,]
coef.lasso 

# (Intercept) data$col1_tec data$col2_tec data$col3_tec data$col4_tec data$col5_tec data$col1_psi data$col2_psi data$col3_psi data$col4_psi data$col5_psi data$col7_psi 
#   0.06853704   -0.02285816    0.05934161    0.03282803    0.02167061    0.03967517    0.44227522   -0.08174265    0.16678511    0.26215337    0.20793069   -0.10538030 
#data$col2_clt data$col3_clt data$col1_fam data$col1_tch data$col2_tch data$col1_sch data$col2_sch data$col4_sch 
#   0.04682972    0.03334747   -0.02698860    0.05121214   -0.19250747    0.04835701   -0.03490630   -0.04370465 
#nessuna componente va a 0
#ancora coefficienti molto simili ai precedenti

```
















# Provare con gli alberi
```{r}
attach(data)


tree.tec <- tree(COMPICT ~ ICTCLASS+ICTHOME+ICTOUTSIDE+ICTRES+ENTUSE+AUTICT,data)
summary(tree.tec)

plot(tree.tec)
text(tree.tec,pretty=0)

cv.tec <- cv.tree(tree.tec)
plot(cv.tec$size,cv.tec$dev,type='b',xlab='size',ylab='deviance')

#pruning per ridurre albero anche se solo una variabile considerata
prune.tec <- prune.tree(tree.tec,best=4)
plot(prune.tec)
text(prune.tec,pretty=0)

#considera solo autict 

detach(data)

```

