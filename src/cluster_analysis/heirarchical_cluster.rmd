---
title: 'Unsupervised learning: Hierarchical, K-means clustering'
output: html_document
editor_options: 
  chunk_output_type: inline
---

## **0.** Settings
```{r setup}
rm( list = ls() )

list.files()
graphics.off() # chiude tutti i device grafici
cat("\014") #pulisci console
library(mvtnorm)
library(MVN)
library(rgl)
library(car)
library(dbscan)
library(cluster)
library(fields)

#directories
dataset_dir = "../../data/"
include_dir = "../include/Utilities.R"
#including utilities
#including utilities
source(include_dir)
#importing the dataset
pisa_data <- read.csv(file=paste(dataset_dir,"pisa-woNA_school_final_wo_Outl_ClassicData_PCA_SCORES.csv",sep=""))
head(pisa_data)

categories_variables = list(tec,psi,clt,fam,tch,sch)
categories_variables_names = c("tec","psi","clt","fam","tch","sch")
data_grezzi = pisa_data[,c("CNT",tec)]
head(data_grezzi)
# Dimensions
n = dim(data_grezzi)[1]
p = dim(data_grezzi)[2]


meas_labels<-c(2:p)
measures = data_grezzi[,meas_labels]
## note: rescale the variables if there's a high order of magnitude

```
# **0.** Exploration: choice of measure and linkage
```{r}

for(var in categories_variables){
  
  data_grezzi = pisa_data[,c("CNT",var)]
  meas_labels<-c(2: dim(data_grezzi)[2])
  measures = data_grezzi[,meas_labels]

data.e = dist(measures, method="euclidean")
data.m = dist(measures, method="manhattan")
data.c = dist(measures, method="canberra")

distances = list(data.e,data.m,data.c)
names = c("euclidean","manhattan","canberra")


i=1
for(dist_chosen in distances ){
data_grezzi = pisa_data[,c("CNT",tec)]
name_dist_chosen <- names[i]
i=i+1

# linkages: "single", "average", "complete", "ward","ward.D2"

data.ds = hclust(dist_chosen, method='single')
data.da = hclust(dist_chosen, method='average')
data.dc = hclust(dist_chosen, method='complete')
data.dw = hclust(dist_chosen, method="ward")
data.dw2 = hclust(dist_chosen, method="ward.D2")

par(mfrow=c(2,2))
plot(data.dc, main=paste(name_dist_chosen,'complete'), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.da, main=paste(name_dist_chosen,'average'), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.dw, main=paste(name_dist_chosen,'ward'), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.dw2, main=paste(name_dist_chosen,'wardD2'), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')


# Cophenetic Matrices

coph.dc <- cophenetic(data.dc)
coph.da <- cophenetic(data.da)
coph.dw <- cophenetic(data.dw)
coph.dw2 <- cophenetic(data.dw2)

# Cophenetic Coefficients

dc = cor(data.e, coph.dc)
da = cor(data.e, coph.da)
dw = cor(data.e, coph.dw)
dw2 = cor(data.e, coph.dw2)

print(paste(categories_variables_names[i],c(dc,da,dw,dw2)))
}
}
# euclidian average : 0.6, but ward seems so much better from the dendogram
# manhatta average 0.55 but ward seems so much better from the dendogram
# terrible canberra
```





### **1.1.** Compute dissimilarities
```{r, warning=FALSE, message=FALSE}
# methods: "euclidean", "manhattan", "canberra"
name_dist_chosen <- "manhattan"
# linkages: "single", "average", "complete", "ward", "ward.D2"
name_linkage_chosen <- 'ward.D2'

data.dist = dist(measures, method=name_dist_chosen)


#image(1:n,1:n,as.matrix(data.dist), main=paste('metrics:, name_dist_chosen'), asp=1, xlab='i', ylab='j' )

# Comment
#   light colors = small values
#   dark colors  = large values
```

### **1.2.** Hierarchical clustering
```{r message=FALSE, warning=FALSE}
data.hclust = hclust(data.dist, method=name_linkage_chosen)

# Order of aggregation
#data.hclust$merge
# Comment

# [128,] -119 80 (e.g.)
# It means that at the step 128 the unity 119 has been aggregated to the cluster
# produced at step 80. If both values are positive, e.g. [140,] 90 95, then we are
# aggregating the clusters produced at the step 90 and 95. If both are negative then
# we are creating a new cluster.

# Distance at which we have aggregations
#data.hclust$height

# Ordering that allows to avoid intersection in the dendrogram
#data.hclust$order
```

### **1.3.** Plot of the dendrograms
```{r, warning=FALSE, message=FALSE}
plot(data.hclust, main=paste(name_dist_chosen,name_linkage_chosen), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
```

### **1.4.** Cutting the dendrogram 
```{r, warning=FALSE, message=FALSE}
k_chosen = 3
plot(data.hclust, main=paste(name_dist_chosen,name_linkage_chosen), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(data.hclust, k=k_chosen)


# How to cut a dendrogram? 
# We generate vectors of labels through the command cutree()

cluster.cutree = cutree(data.hclust, k=k_chosen)
table(cluster.cutree)
```

### **1.5.** How good is the clustering?
```{r, warning=FALSE, message=FALSE}
# Did it aggregate coherently with the dissimilarity matrix or not?

# Cophenetic Matrices
coph.mat <- cophenetic(data.hclust)

#image(as.matrix(coph.mat), main=name_dist_chosen, asp=1 )
# Cophenetic Coefficients
coph.coeff = cor(data.dist, coph.mat)
coph.coeff


```

## **2.** Save data to test the mean
```{r}
data1 = measures[cluster.cutree==1,]
data2 = measures[cluster.cutree==2,]
data3 = measures[cluster.cutree==3,]
data4 = measures[cluster.cutree==4,]
```
