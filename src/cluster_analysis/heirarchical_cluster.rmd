---
title: 'Unsupervised learning: Hierarchical, K-means clustering'
output: html_document
editor_options:
  chunk_output_type: console
---

## **0.** Settings
```{r setup}
rm( list = ls() )

list.files()
graphics.off() # chiude tutti i device grafici
cat("\014") #pulisci console
library(mvtnorm)
library(MVN)
library(rgl)
library(car)
library(dbscan)
library(cluster)
library(fields)
library(ggplot2)

#directories
dataset_dir = "../../data/"
include_dir = "../include/Utilities.R"
#including utilities
#including utilities
source(include_dir)
#importing the dataset
pisa_data <- read.csv(file=paste(dataset_dir,"pisa_school_final_wo_Outl_PCA_SCORES.csv",sep=""))
head(pisa_data)
p = dim(pisa_data)[2]


categories_variables = list(tec,psi,clt,fam,tch,sch)
cat_var_names = c("tec","psi","clt","fam","tch","sch")


data_grezzi = pisa_data
data_clustered<-pisa_data

head(data_grezzi)
# Dimensions
n = dim(data_grezzi)[1]
p = dim(data_grezzi)[2]


meas_labels<-c(2:p)
data_grezzi = data_grezzi[,meas_labels]
head(data_grezzi)
## note: rescale the variables if there's a high order of magnitude

```
# **0.** Exploration: choice of measure and linkage

k=0
for(var in categories_variables){
  k=k+1
  data_grezzi = pisa_data[,c("CNT",var)]
  meas_labels<-c(2: dim(data_grezzi)[2])
  measures = data_grezzi[,meas_labels]

data.e = dist(measures, method="euclidean")
data.m = dist(measures, method="manhattan")
data.c = dist(measures, method="canberra")

distances = list(data.e,data.m,data.c)
names = c("euclidean","manhattan","canberra")


i=1
for(dist_chosen in distances ){
data_grezzi = pisa_data[,c("CNT",tec)]
name_dist_chosen <- names[i]
i=i+1

# linkages: "single", "average", "complete", "ward","ward.D2"

data.ds = hclust(dist_chosen, method='single')
data.da = hclust(dist_chosen, method='average')
data.dc = hclust(dist_chosen, method='complete')
data.dw = hclust(dist_chosen, method="ward")
data.dw2 = hclust(dist_chosen, method="ward.D2")

par(mfrow=c(2,2))
plot(data.dc, main=paste(name_dist_chosen,'complete',cat_var_names[k]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.da, main=paste(name_dist_chosen,'average',cat_var_names[k]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.dw, main=paste(name_dist_chosen,'ward',cat_var_names[k]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.dw2, main=paste(name_dist_chosen,'wardD2',cat_var_names[k]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')


# Cophenetic Matrices

coph.dc <- cophenetic(data.dc)
coph.da <- cophenetic(data.da)
coph.dw <- cophenetic(data.dw)
coph.dw2 <- cophenetic(data.dw2)

# Cophenetic Coefficients

dc = cor(data.e, coph.dc)
da = cor(data.e, coph.da)
dw = cor(data.e, coph.dw)
dw2 = cor(data.e, coph.dw2)

#print(cat_var_names[k],c(dc,da,dw,dw2))
}
}
# euclidian average : 0.6, but ward seems so much better from the dendogram
# manhatta average 0.55 but ward seems so much better from the dendogram
# terrible canberra






### **1.1.** Compute dissimilarities
```{r, warning=FALSE, message=FALSE}
counter_names = 1
for (k in categories_variables){
measures = data_grezzi[,k]
head(measures)

# methods: "euclidean", "manhattan", "canberra"
name_dist_chosen <- "manhattan"
# linkages: "single", "average", "complete", "ward", "ward.D2"
name_linkage_chosen <- 'ward.D2'

data.dist = dist(measures, method=name_dist_chosen)

data.hclust = hclust(data.dist, method=name_linkage_chosen)

# Order of aggregation
#data.hclust$merge
# Distance at which we have aggregations
#data.hclust$height
# Ordering that allows to avoid intersection in the dendrogram
#data.hclust$order

plot(data.hclust, main=paste(name_dist_chosen,name_linkage_chosen), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')



### **1.2.** Cutting the dendrogram
k_chosen = 3
plot(data.hclust, main=paste(name_dist_chosen,name_linkage_chosen), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(data.hclust, k=k_chosen)


# How to cut a dendrogram?
# We generate vectors of labels through the command cutree()

cluster.cutree = cutree(data.hclust, k=k_chosen)
table(cluster.cutree)




## How good is the clustering?
# Cophenetic Matrices
coph.mat <- cophenetic(data.hclust)
# Cophenetic Coefficients
coph.coeff = cor(data.dist, coph.mat)
coph.coeff


#plot(measures[,1],measures[,3],col=cluster.cutree+1)
#plot3d(measures, size=3, col=cluster.cutree+1, aspect = F)

## **2.** Saving the clustering as factors

data_clustered[,cat_var_names[counter_names]] = as.factor(cluster.cutree)
counter_names = counter_names+ 1


}
head(data_clustered)
```
## **2.1** Exploring the dataset
```{r}
country_names <- unique(data_clustered$CNT)
g=16
indeces = list()
for (jj in 1:g){
    indeces[jj] <- list(which(data_clustered$CNT == country_names[jj]))
}
n_list = list()
for(j in 1:g) {
  n_list[j] = list(length(indeces[[j]]))
}

####
# per ogni cluster voglio vedere quale percentuale di scuole della stessa nazione ricadono nello stesso
percent_list = list()
counter = 0


for(k in cat_var_names){ # categories
  counter = counter +1  
  percent_list_mid = list()
  counter2=0
  for(i in 1:k_chosen){ # clusters
    counter2 = counter2 +1
  percent = c()
    for(j in 1:g){ # countrys 
      temp = length(data_clustered[which(data_clustered[,k]==i & data_clustered$CNT==country_names[j]),1])
      temp = temp/n_list[[j]]
      percent = c(percent,temp)
    }
    percent_list_mid[counter2]=list(percent)

  }
  percent_list[counter] = list(percent_list_mid)
}


```

```{r}

#for(kk in 1:6){
# Create sample data
group_percentages <- percent_list[[kk]]


# Convert percentages to data frame
groups_names = c("A", "B", "C")
df <- data.frame(
  country = rep(country_names, each = 3),
  group = rep(groups_names, times = length(country_names)),
  percentage = rep(c(0,0,0),times = length(country_names))
)


for(i in country_names){
  for(j in groups_names){
    df[which((df$group==j) & (df$country==i)),3] = group_percentages[[which(groups_names==j)]][which(countrys==i)]*100
  }
}
  
x11()
# Create stacked bar chart
ggplot(df, aes(x = country, y = percentage, fill = group)) +
  geom_bar(stat = "identity") +
  labs(title = "Proportion of Schools in Each Group by Country",
       x = "Country",
       y = "Proportion") +
  scale_fill_manual(values = c("#E69F00", "#56B4E9", "#009E73"),
                    labels = c("Group A", "Group B", "Group C")) +
  geom_text(aes(label = paste0(round(percentage,2), "%")), 
            position = position_stack(vjust = 0.5))

#}

```


## **3.** DBSCAN
### **3.1** choice of hyperparameters
```{r}
data_to_cluster = measures[,c(clt)]
# Rule of thumb, minPts = dimensionality + 1 = 3 here
# How to choose eps from minPts?
# Plot of the distances to the minPts nearest neighbor
k_chosen_2=dim(data_to_cluster)[2]+1
kNNdistplot(as.matrix(data_to_cluster), k = k_chosen_2)
eps_chosen = 0.5
abline(h = eps_chosen, col = "red", lty = 2)

# Run the dbscan
dbs <- dbscan(data_to_cluster, eps = eps_chosen, minPts = p+1)
dbs
plot(data_to_cluster, col = dbs$cluster + 1L, pch=19)
```
### **3.2** Silhouette
```{r}
# Let's compute the silhouette score on the clustering performed before
# WARNING (specific to DBSCAN): We need to remove the noise points as they do
# not belong to a cluster, before computing the silhouette score
clustered_index <- which(dbs$cluster != 0) # Index of non noise points
clustered_points <- measures[clustered_index,] # only clustered points
clustered_labels <- dbs$cluster[clustered_index] # corresponding labels

sil <- silhouette(clustered_labels, dist(clustered_points))
summary(sil)

sil_score <- function(labels, dist) {
  # Compute the average of the silhouette widths
  sil <- silhouette(labels, dist)
  sil_widths <- sil[,"sil_width"]
  mean(sil_widths)
}

sil_score(clustered_labels, dist(clustered_points))
```
