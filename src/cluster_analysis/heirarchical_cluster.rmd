---
title: 'Unsupervised learning: Hierarchical, K-means clustering'
output: html_document
editor_options: 
  chunk_output_type: inline
---

## **0.** Settings
```{r setup}
rm( list = ls() )

list.files()
graphics.off() # chiude tutti i device grafici
cat("\014") #pulisci console
library(mvtnorm)
library(MVN)
library(rgl)
library(car)
library(dbscan)
library(cluster)
library(fields)

#directories
dataset_dir = "../../data/"
include_dir = "../include/Utilities.R"
#including utilities
#including utilities
source(include_dir)
#importing the dataset
pisa_data <- read.csv(file=paste(dataset_dir,"pisa-woNA_school_final_wo_Outl_ClassicData_PCA_SCORES.csv",sep=""))
head(pisa_data)
p = dim(pisa_data)[2]


categories_variables = list(tec,psi,clt,fam,tch,sch)
cat_var_names = c("tec","psi","clt","fam","tch","sch")


data_grezzi = pisa_data
head(data_grezzi)
# Dimensions
n = dim(data_grezzi)[1]
p = dim(data_grezzi)[2]


meas_labels<-c(2:p)
measures = data_grezzi[,meas_labels]
head(measures)
## note: rescale the variables if there's a high order of magnitude

```
# **0.** Exploration: choice of measure and linkage
```{r}
k=0
for(var in categories_variables){
  k=k+1
  data_grezzi = pisa_data[,c("CNT",var)]
  meas_labels<-c(2: dim(data_grezzi)[2])
  measures = data_grezzi[,meas_labels]

data.e = dist(measures, method="euclidean")
data.m = dist(measures, method="manhattan")
data.c = dist(measures, method="canberra")

distances = list(data.e,data.m,data.c)
names = c("euclidean","manhattan","canberra")


i=1
for(dist_chosen in distances ){
data_grezzi = pisa_data[,c("CNT",tec)]
name_dist_chosen <- names[i]
i=i+1

# linkages: "single", "average", "complete", "ward","ward.D2"

data.ds = hclust(dist_chosen, method='single')
data.da = hclust(dist_chosen, method='average')
data.dc = hclust(dist_chosen, method='complete')
data.dw = hclust(dist_chosen, method="ward")
data.dw2 = hclust(dist_chosen, method="ward.D2")

par(mfrow=c(2,2))
plot(data.dc, main=paste(name_dist_chosen,'complete',cat_var_names[k]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.da, main=paste(name_dist_chosen,'average',cat_var_names[k]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.dw, main=paste(name_dist_chosen,'ward',cat_var_names[k]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
plot(data.dw2, main=paste(name_dist_chosen,'wardD2',cat_var_names[k]), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')


# Cophenetic Matrices

coph.dc <- cophenetic(data.dc)
coph.da <- cophenetic(data.da)
coph.dw <- cophenetic(data.dw)
coph.dw2 <- cophenetic(data.dw2)

# Cophenetic Coefficients

dc = cor(data.e, coph.dc)
da = cor(data.e, coph.da)
dw = cor(data.e, coph.dw)
dw2 = cor(data.e, coph.dw2)

#print(cat_var_names[k],c(dc,da,dw,dw2))
}
}
# euclidian average : 0.6, but ward seems so much better from the dendogram
# manhatta average 0.55 but ward seems so much better from the dendogram
# terrible canberra
```





### **1.1.** Compute dissimilarities
```{r, warning=FALSE, message=FALSE}
measures = measures[,sch]
head(measures)

# methods: "euclidean", "manhattan", "canberra"
name_dist_chosen <- "manhattan"
# linkages: "single", "average", "complete", "ward", "ward.D2"
name_linkage_chosen <- 'ward.D2'

data.dist = dist(measures, method=name_dist_chosen)


#image(1:n,1:n,as.matrix(data.dist), main=paste('metrics:, name_dist_chosen'), asp=1, xlab='i', ylab='j' )

# Comment
#   light colors = small values
#   dark colors  = large values
```

### **1.2.** Hierarchical clustering
```{r message=FALSE, warning=FALSE}
data.hclust = hclust(data.dist, method=name_linkage_chosen)

# Order of aggregation
#data.hclust$merge
# Comment

# [128,] -119 80 (e.g.)
# It means that at the step 128 the unity 119 has been aggregated to the cluster
# produced at step 80. If both values are positive, e.g. [140,] 90 95, then we are
# aggregating the clusters produced at the step 90 and 95. If both are negative then
# we are creating a new cluster.

# Distance at which we have aggregations
#data.hclust$height

# Ordering that allows to avoid intersection in the dendrogram
#data.hclust$order
```

### **1.3.** Plot of the dendrograms
```{r, warning=FALSE, message=FALSE}
plot(data.hclust, main=paste(name_dist_chosen,name_linkage_chosen), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
```

### **1.4.** Cutting the dendrogram 
```{r, warning=FALSE, message=FALSE}
k_chosen = 3
plot(data.hclust, main=paste(name_dist_chosen,name_linkage_chosen), hang=-0.1, xlab='', labels=F, cex=0.6, sub='')
rect.hclust(data.hclust, k=k_chosen)


# How to cut a dendrogram? 
# We generate vectors of labels through the command cutree()

cluster.cutree = cutree(data.hclust, k=k_chosen)
table(cluster.cutree)
measures = cbind(measures,cluster.cutree)
measures$cluster.cutree = as.factor(measures$cluster.cutree)

```

### **1.5.** How good is the clustering?
```{r, warning=FALSE, message=FALSE}
# Did it aggregate coherently with the dissimilarity matrix or not?

# Cophenetic Matrices
coph.mat <- cophenetic(data.hclust)

#image(as.matrix(coph.mat), main=name_dist_chosen, asp=1 )
# Cophenetic Coefficients
coph.coeff = cor(data.dist, coph.mat)
coph.coeff


plot(measures[,1],measures[,2],col=cluster.cutree+1)
plot3d(measures, size=3, col=cluster.cutree+1, aspect = F)

```

## **2.** Save data to test the mean
```{r}
data1 = measures[cluster.cutree==1,]
data2 = measures[cluster.cutree==2,]
data3 = measures[cluster.cutree==3,]
data4 = measures[cluster.cutree==4,]
```

## **3.** DBSCAN
### **3.1** choice of hyperparameters 
```{r}
data_to_cluster = measures[,c(clt)]
# Rule of thumb, minPts = dimensionality + 1 = 3 here
# How to choose eps from minPts?
# Plot of the distances to the minPts nearest neighbor
k_chosen_2=dim(data_to_cluster)[2]+1
kNNdistplot(as.matrix(data_to_cluster), k = k_chosen_2) 
eps_chosen = 0.5
abline(h = eps_chosen, col = "red", lty = 2)

# Run the dbscan
dbs <- dbscan(data_to_cluster, eps = eps_chosen, minPts = p+1)
dbs
plot(data_to_cluster, col = dbs$cluster + 1L, pch=19)
```
### **3.2** Silhouette
```{r}
# Let's compute the silhouette score on the clustering performed before
# WARNING (specific to DBSCAN): We need to remove the noise points as they do
# not belong to a cluster, before computing the silhouette score
clustered_index <- which(dbs$cluster != 0) # Index of non noise points
clustered_points <- measures[clustered_index,] # only clustered points
clustered_labels <- dbs$cluster[clustered_index] # corresponding labels

sil <- silhouette(clustered_labels, dist(clustered_points))
summary(sil)

sil_score <- function(labels, dist) {
  # Compute the average of the silhouette widths
  sil <- silhouette(labels, dist)
  sil_widths <- sil[,"sil_width"]
  mean(sil_widths)
}

sil_score(clustered_labels, dist(clustered_points))
```
