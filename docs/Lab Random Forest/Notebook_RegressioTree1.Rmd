---
title: "Regression Trees"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

## **0.** Settings
```{r setup}
knitr::opts_knit$set(root.dir = normalizePath("G:/Il mio Drive/UNIVERSITA/APPLIED STATISTICS/LABS/Lab Random Forest")) 

rm( list = ls() )

list.files()
graphics.off() # chiude tutti i device grafici
cat("\014") #pulisci console
library(tree)
library(ISLR)
library(MASS)
```


##### Regression Trees: Boston housing dataset ####
```{r}
# Housing Values in Suburbs of Boston
help(Boston)
dim(Boston)
names(Boston)


set.seed(02091991)


plot(Boston[,c(14,1,2,6,8,13)])

# We are only plotting the following variables
# medv: median value of owner-occupied homes in $1000s.
# crim: per capita crime rate by town.
# zn: proportion of residential land zoned for lots over 25,000 sq.ft.
# rm: average number of rooms per dwelling.
# lstat: lower status of the population (percent).


train <- sample(1:nrow(Boston), nrow(Boston)/2)

help(tree)
# tree function in tree package
# INPUT:
# - formula: as in lm function, with the ~ syntax
#           specify the y and the X variables of the model
# - data: dataframe the formula is apply to
# - subset: specify the observations to consider 
#          in the fitting
# - na.action: what to do with missing values (by default are dropped)
# - ... other parameters such as: 
#       mincut=5, minimum number of observation in the terminal nodes
#       mindev: with-in node deviance
# OUTPUT:
# - where: the node each observation is assigned to
# ...

tree.boston <- tree(medv~.,Boston,subset=train)
summary(tree.boston)

# In the summary we get the "Variables actually used in tree construction"
# the number of terminal nodes and the residuals position indicators


plot(tree.boston)
text(tree.boston,pretty=0)

# Interpretation: the most impacting variable
# on the value of the house is the lower status of the population.
# Secondly, the number of rooms.
# The lower the status of the population the lower the value
# The higher the number of rooms the higher the value.


# Prediction on the test set with the unpruned tree
# predict function in the tree package:
# INPUT:
# - tree model object
# - newdata: new data to apply the model to

yhat <- predict(tree.boston,newdata=Boston[-train,])
boston.test <- Boston[-train,"medv"]


plot(boston.test,yhat)
abline(0,1)

mean((yhat-boston.test)^2) # test set MSE
```


# Pruning and Cross-Validation
```{r}
# We want to do pruning to find out the optimal
# number of splits in the tree
# prune.tree
# INPUT:
# - tree object
# - k: cost-complexity parameter
# - best: size of the tree, i.e number of nodes
# - method: method used to compute the error
# ...
# OUTPUT:
# - tree object that minimize the method
# - size: number of terminal nodes  each tree in the cost-complexity pruning sequence
# - deviance: total deviance of each tree in the cost-complexity pruning sequence
# ...

# Suppose we want to find the best tree of size 7
# such that minimize the deviance
prune.boston <- prune.tree(tree.boston,best=7)
prune.boston

# plot the optimal tree of size 7

plot(prune.boston)
text(prune.boston,pretty=0)
```


# cv.tree:
 INPUT:
 - tree object
 - FUN: function to do the pruning
 - K: number of Folds
 OUTPUT:
 - output of the called FUN
 by default it is calling the function prune.tree if regression or
 prune.misclass if classification
```{r}
cv.boston <- cv.tree(tree.boston,FUN=prune.tree)

# Let's plot how the deviance of the terminal nodes change as the 
# dimension of the tree change:


plot(cv.boston$size,cv.boston$dev,type='b')

# Once we have decided the optimal size of the tree
# we prune the tree:

prune.boston <- prune.tree(tree.boston,best=3)


plot(prune.boston)
text(prune.boston,pretty=0)

# So the significant variables in determining the mdev are:


plot(Boston[,c(14,6,13)])

# Predict with the best tree:
yhat <- predict(prune.boston,newdata=Boston[-train,])
boston.test <- Boston[-train,"medv"]


plot(boston.test,yhat)
abline(0,1)

mean((yhat-boston.test)^2) # test set MSE

# or
prune.boston <- prune.tree(tree.boston,best=7)


plot(prune.boston)
text(prune.boston,pretty=0)
```


# Predict with the best tree:
```{r}
yhat <- predict(prune.boston,newdata=Boston[-train,])
boston.test <- Boston[-train,"medv"]


plot(boston.test,yhat)
abline(0,1)

mean((yhat-boston.test)^2) # test set MSE
```


##### Regression Trees: Rain Dataset
```{r}
# The rain file contain the data about rain in London and Milan:
# - rain.day: number of rainy days
# - rain.mm: millimiters of rain
# - umbrella: delta of the umbrella sold wrt the previous year [10000 unit]


# The goal is to predict the millimiters of rain from
# the number of days and the number of umbrellas,
# in both London and Milan

rain<-read.table("rain.txt")
names(rain)
dim(rain)

set.seed(02091991)

train <- sample(1:nrow(rain), nrow(rain)/2)

tree.rain <- tree(rain.mm~.,rain)#,subset=train)
summary(tree.rain)


plot(tree.rain)
text(tree.rain,pretty=0)


# Prediction on the test set with the unpruned tree
yhat <- predict(tree.rain,newdata=rain[-train,])
rain.test <- rain[-train,"rain.mm"]

color<-rep('red',length(rain.test))
color[which(rain[-train,"city"]=="London")]<-'green'

plot(rain.test,yhat,col=color)
abline(0,1)


boxplot(rain$rain.mm~rain$city,col=c('green','red'))

mean((yhat-rain.test)^2) # test set MSE

# Pruning and Cross-Validation
cv.rain <- cv.tree(tree.rain)


plot(cv.rain$size,cv.rain$dev,type='b')

prune.rain <- prune.tree(tree.rain,best=2)


plot(prune.rain)
text(prune.rain,pretty=0)

# Seems that the more impacting information is the city!


```

