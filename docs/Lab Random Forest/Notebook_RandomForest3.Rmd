---
title: "Tree-based Ensambled Methods"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---


## **0.** Settings
```{r setup}
knitr::opts_knit$set(root.dir = normalizePath("G:/Il mio Drive/UNIVERSITA/APPLIED STATISTICS/LABS/Lab Random Forest")) 

rm( list = ls() )

list.files()
graphics.off() # chiude tutti i device grafici
cat("\014") #pulisci console
library(randomForest)
library(gbm)
library(MASS)
library(tree)
```


#### Bagging, Random Forests and Boosting
Boston housing dataset
```{r}
help(Boston)
dim(Boston)

set.seed(1)

train <- sample(1:nrow(Boston), nrow(Boston)/2)
boston.test <- Boston[-train,"medv"]
```


#### Algorithm 1: Regression Tree 
```{r}
tree.boston<- tree(medv~.,data=Boston,subset=train)


plot(tree.boston)
text(tree.boston,pretty=0)

# Logical Plot
tree.boston
# Note: all the logic conditions are reported. With * we underline the terminal nodes
```


# PARAMETERS: 
number of terminal nodes (See Lab 8 A and B for details)



### Algorithm 2: Bagging (bootstrap aggregation) 
 randomForest:
 INPUT:
 - formula: as in lm function, with the ~ syntax
           specify the y and the X variables of the model
 - data: dataframe containg variables of the formula
 - subset: specify the observations to consider 
          in the fitting
 - na.action: what to do with missing values (by default are dropped)
 - xtest, ytest: testset
 - ntree: number of trees to grow
 - mtry: number of variables selected at each split
 - nodesize: minimum size of terminal nodes
 - maxnodes: maximum number of terminal nodes (subjected to nodesize of course)
 ...
 OUTPUT:
 - call, type: info about the inputs
 - mse: vector of mean square errors
 - confusion: confusion matrix for prediction (class only)
 - test: if test is given, contains info about test

 Using the randomForest function but holding always
 all the predictors in building the trees
```{r}
help(randomForest)

dim(Boston)
# set mtry=13 to perform bagging (using all features)

bag.boston <- randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston

# Evaluation of the performances of this bagged model
yhat.bag <- predict(bag.boston,newdata=Boston[-train,])

# Plot of the estimations with bagging

plot(yhat.bag, boston.test)
abline(0,1)

# Computing the test MSE
mean((yhat.bag-boston.test)^2)
```


# PARAMETER SELECTION: 
```{r}
# number of trees (ntree)

# Choosing the number of trees, for example we go for 25 trees
bag.boston <- randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
bag.boston
yhat.bag <- predict(bag.boston,newdata=Boston[-train,])

# the error increased comparing to the results obtained with 500 trees
mean((yhat.bag-boston.test)^2)

# Plot the error (in this regression problem the mse)

plot(bag.boston)

# The higher the number of trees computed the smaller the MSE,
# but the higher the computational cost.
```


#### Algorithm 3: Random Forests ####
```{r}
# To run the random forest we use the same function 
# but decreasing the number of variables selected to built each tree

# decreasing mtry from 13 variables to 6
set.seed(1)
rf.boston <- randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
rf.boston

# Prediction on the test set
yhat.rf <- predict(rf.boston,newdata=Boston[-train,])

# Mean Square Prediction Error
mean((yhat.rf-boston.test)^2)

# importance (): function receiving a RF output and computing the increase
# of purity of terminal node and the increase in MSE when a certain variable is 
# considered in the tree
importance(rf.boston)

# Note: the MSE is computed on permuting OOB data
# For regression, impurity is computed with RSS, 
# while for classification with Gini index.

# By plotting the matrix we can rank the variables
```


# PARAMETERS SELECTION: 
```{r}
varImpPlot(rf.boston)

# number of trees (ntree), number of variables (mtry)

# mtry default (p/3 for regression and sqrt(p) for classification)
rf.boston <- randomForest(medv~.,data=Boston,subset=train,importance=TRUE)
rf.boston

yhat.rf <- predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)

importance(rf.boston)

varImpPlot(rf.boston)


# Varying mtry we save the OOB error and the test error
# to find the optimal number of variables
# Note: if no test set is specified, the fit$mse is the 
# prediction mean square error computed on the OOB set
oob.err <- double(13)
test.err <- double(13)
for(mtry in 1:13){
  # fit the model on training set with the current value of mtry
  fit <- randomForest(medv~.,data=Boston,subset=train,mtry=mtry,ntree=400)
  
  # save the OOB error
  oob.err[mtry] <- fit$mse[400]
  
  # predict the test set value for medv
  pred <- predict(fit,Boston[-train,])
  
  # save the test prediction mean square error
  test.err[mtry] <- with(Boston[-train,],mean((medv-pred)^2))
  
  # plot the step
  cat(mtry," ")  
}


matplot(1:mtry,cbind(test.err,oob.err),pch=19,col=c('red','blue'),type='b',ylab="Mean Squared Error")
legend('topright',legend=c("Test","OOB"),pch=19,col=c('red','blue'))

# We go for mtry=6
fit.best <- randomForest(medv~.,data=Boston,subset=train,mtry=6,ntree=400)
# prediction and mse
mean((Boston[-train,'medv']-predict(fit,Boston[-train,]))^2)

# Homework: try a 2-D grid parameter evaluation by changing both 
# ntree and mtry
```


#### Algorithm 4: Boosting
gbm:
INPUTS:
...
- n.trees: number of trees to fit
- shrinkage: learning rate when adding new trees
- interaction.depth: level of variables interactions, i.e. the number of
split in each tree
....
```{r}
set.seed(1)

help(gbm)

boost.boston <- gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=10000,interaction.depth=4)

# Plotting the relative influence of the variables,
# i.e. average among trees of the empirical improvement 
# gained by splitting on certain varaible


summary(boost.boston,cex.names=0.8)

# Partial dependence plots
# (marginal effect of the selected variables 
# on the response after integrating out the other variables)
# 
# par(mfrow=c(1,2))
# plot(boost.boston,i="rm")
# plot(boost.boston,i="lstat")

# use boosted model to predict medv on the test set
yhat.boost <- predict(boost.boston,newdata=Boston[-train,],n.trees=10000)
mean((yhat.boost-boston.test)^2) # test MSE
```


# PARAMETERS:
 number of trees (n.trees), 
 number of splits (interaction.depth), 
 shrinkage parameter (shrinkage)
```{r}
# Setting the value of the shrinkage parameter
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",
                 n.trees=10000,interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=10000)
mean((yhat.boost-boston.test)^2)

# Changing the number of trees with predict function:
# set the number of trees n.trees: in the prediction only 
# the first n.trees of the model are used.
n.trees=seq(100,10000,by=100)
predmat <- predict(boost.boston,newdata=Boston[-train,],n.trees=n.trees)
dim(predmat)
dim(Boston[-train,])
length(n.trees)

berr <- with(Boston[-train,],apply((predmat-medv)^2,2,mean))

plot(n.trees,berr,pch=19,ylab="Mean Squared Error", xlab="# Trees",main="Boosting Test Error")
abline(h=min(test.err),col='red') # Random Forests minimum error

# Note: grid parameters evaluation should be run, by considering
# different number of trees, shrinkage parameters and other params
# at the same time
```

