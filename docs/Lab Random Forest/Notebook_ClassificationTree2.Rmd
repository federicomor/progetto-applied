---
title: "Classification Trees"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

## **0.** Settings
```{r setup}
knitr::opts_knit$set(root.dir = normalizePath("G:/Il mio Drive/UNIVERSITA/APPLIED STATISTICS/LABS/Lab Random Forest")) 

rm( list = ls() )

list.files()
graphics.off() # chiude tutti i device grafici
cat("\014") #pulisci console
library(glmnet)
library(tree)
library(ISLR)
library(mvtnorm)
library(MASS)
library(car)
library(mvtnorm)
library(rpart)
```

##### Classification Trees: Carseats dataset ####
```{r}
attach(Carseats)

help(Carseats)
dim(Carseats)
names(Carseats)

# We would like to built a classification tree for Sales:

hist(Sales)
abline(v=8,lwd=3,col='red')

# we built a categorical variable from numeric variable
High <- ifelse(Sales<=8,"No","Yes")
table(High)
# No if less or equal than 8000 units sold per location
# Yes if more than 8000 units
```


```{r}
# new dataframe
Carseats <- data.frame(Carseats,High)

set.seed(02091991)
# Tree with all the other variables
# Default splitting method for class trees: gini
tree.carseats <- tree(High~.-Sales,Carseats)
summary(tree.carseats)

# As management engineers might know, the location in 
# the supermarket seems to impact the sales the most


plot(tree.carseats)
text(tree.carseats,pretty=0)

# logic scheme:
tree.carseats


# create a training and a test set to estimate the test error
set.seed(1)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train,]
High.test <- High[-train]

# Built a tree on training set
tree.carseats <- tree(High~.-Sales,Carseats,subset=train)


plot(tree.carseats)
text(tree.carseats,pretty=0)

# Predict the test set
tree.pred <- predict(tree.carseats,Carseats.test,type="class")

# Misclassification table:
table(tree.pred,High.test)
# Right:
(84+44)/200
# Wrong:
(35+37)/200

# Simplest classifier:
table(High[train])
# Proportion of yes
83/200

# Pruning and Cross-Validation:
# use cross validation to prune the tree optimally
set.seed(3)

# cv.tree with prune.misclass function (default missclass function)
help("prune.misclass")
cv.carseats <- cv.tree(tree.carseats,FUN=prune.misclass)

names(cv.carseats)
cv.carseats

# plot the cross validation error rate as a function of size tree


plot(cv.carseats$size,cv.carseats$dev,type="b")

# or just:
# 
# plot(cv.carseats) 

# Once we select the best size via Cross-Validation we prune the classification tree
# with the best number of terminal nodes with prune.misclass
prune.carseats <- prune.misclass(tree.carseats,best=5)
prune.carseats

# Plot of the best pruned tree

plot(prune.carseats)
text(prune.carseats,pretty=0)

tree.pred <- predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)

# wrongly classified
(33+32)/200

detach(Carseats)
```


## Classification Trees: Iris dataset
```{r}
# Goal: classify the specie of a flower knowing petal and
# sepal length
help(iris)
head(iris)
names(iris)

# Y:
species.name <- iris$Species
# X: (only petal measurments)
iris2 <- iris[,3:4]

i1 <- which(species.name=='setosa')
i2 <- which(species.name=='versicolor')
i3 <- which(species.name=='virginica')

n1 <- length(i1)
n2 <- length(i2)
n3 <- length(i3)
n <- n1+n2+n3


# Plot of original data

plot(iris2, main='Iris Petal', xlab='Petal.Length', ylab='Petal.Width', pch=19)
points(iris2[i1,], col='red', pch=19)
points(iris2[i2,], col='green', pch=19)
points(iris2[i3,], col='blue', pch=19)
legend("topleft", legend=levels(species.name), fill=c('red','green','blue'))

# Jittering
iris3 <- iris2 + cbind(rnorm(150, sd=0.025))    

plot(iris3, main='Iris Petal', xlab='Petal.Length', ylab='Petal.Width', pch=19)
points(iris3[i1,], col='red', pch=19)
points(iris3[i2,], col='green', pch=19)
points(iris3[i3,], col='blue', pch=19)
legend("topleft", legend=levels(species.name), fill=c('red','green','blue'))

# Initialize the dataframe:
iris.df <- data.frame(iris2,species.name)

# Training and Test Set
set.seed(02091991)
train <- sample(1:nrow(iris.df), 100)
iris.test <- iris.df[-train,]

# Classification Tree on Species
tree.iris <- tree(species.name ~., iris.df[train,])
summary(tree.iris)

# Plot of the result

plot(tree.iris)
text(tree.iris,pretty=0)

# Cross-validation
set.seed(3)

cv.iris <- cv.tree(tree.iris,FUN=prune.misclass)

names(cv.iris)
cv.iris

# Plot of the misclassification error as a function of size

plot(cv.iris$size,cv.iris$dev,type="b")

# Best size seems to be 3, so we prune the tree until we get best 4 terminal nodes tree
prune.iris <- prune.misclass(tree.iris,best=3)


plot(prune.iris)
text(prune.iris,pretty=0)

# Logical structure
prune.iris
summary(prune.iris)

# Let's plot the regions on the X space

plot(iris2, main='Iris Petal', xlab='Petal.Length', ylab='Petal.Width', pch=19)
points(iris2[i1,], col='red', pch=19)
points(iris2[i2,], col='green', pch=19)
points(iris2[i3,], col='blue', pch=19)
legend("topleft", legend=levels(species.name), fill=c('red','green','blue'))
abline(v=2.47339)
x <- c(2.47339,8)
points(x, rep(1.69941,length(x)), type='l')
y <- c(-1,1.69941)
points(rep(5.03084,length(y)), y, type='l')

# Prediction:
tree.pred <- predict(prune.iris,iris.test,type="class")
table(tree.pred,iris.test$species.name)
errort <- (tree.pred != iris.test$species.name)
errort

AERt   <- sum(errort)/length(iris.test$species.name)
AERt


# Comparison with qda (have a look at LAB 4 B to refresh)
# Create same training and test as before
iris.train<-iris.df[train,1:2]
species.train<-iris.df[train,3]
iris.test<-iris.df[-train,1:2]
species.test<-iris.df[-train,3]

# QDA model
qda.iris <- qda(iris.train,species.train)
qda.iris
# prediction of iris2
Qda.iris <- predict(qda.iris, iris.test)
Qda.iris$class
species.name
table(True=species.test, Estimated=Qda.iris$class)

erroriq <- (Qda.iris$class != species.test)
erroriq

AERq   <- sum(erroriq)/length(species.test)
AERq

AERt
```


#### Simulated data: Variables Selection 
```{r}
# Idea: generate two variables correlated with the response.
# The rest are uncorrelated to the response. Test the pruning power of tree algorithm
# Ridge and Lasso

set.seed(123)
# 100 observation and p variables
n <- 100
p <- 50

covariates <- array(0, dim=c(n,p))

# generate 3 variables, second and third correlated with the first
# correlation matrix:
matrix(data=c(1,0.7,0.7,0.7,1,0,0.7,0,1),nrow = 3,ncol = 3)

temp <- rmvnorm(n, mean = rep(0, 3), sigma = matrix(data=c(1,0.7,0.7,0.7,1,0,0.7,0,1),nrow = 3,ncol = 3))
cor(temp[,1],temp[,2])
cor(temp[,1],temp[,3])
# 1st is response
resp <- temp[,1]
# 2nd and 3rd covariates
covariates[,1] <- temp[,2]
covariates[,2] <- temp[,3]

# generate other p indipendent covariates
for(i in 3:p)
  covariates[,i] <- rnorm(n)

# Plot

par(mfrow=c(3,3))
for(i in 1:9)
  plot(covariates[,i],resp, pch=19)
```


```{r}
correlations <- NULL
for(i in 1:p)
  correlations <- c(correlations,cor(resp,covariates[,i]))

plot(1:p,correlations, pch=19)

data <- data.frame(cbind(resp,covariates))

names(data)
for(i in 1:p)
  names(data)[i+1] <- paste('V',i,sep='')
names(data)

# Tree: 

tree.complete <- tree(resp ~., data)
summary(tree.complete)


plot(tree.complete)
text(tree.complete,pretty=0)

# The more impacting variables on response are V1,V2

# Let's run cv to select best number of terminal nodes
set.seed(3)

cv <- cv.tree(tree.complete)

names(cv)
cv


plot(cv$size,cv$dev,type="b")

prune <- prune.tree(tree.complete,best=4)


plot(prune)
text(prune,pretty=0)
# As expected only 1st and 2nd covariate are selected


# Ridge and Lasso

# Define model matrix
x <- model.matrix(resp~.,data)[,-1] # predictor matrix
y <- data$resp # response

# grid of parameters lambda
grid <- 10^seq(5,-3,length=100)

# Ridge
ridge.mod <- glmnet(x,y,alpha=0,lambda=grid)


plot(ridge.mod,xvar='lambda',label=TRUE)

cv.out <- cv.glmnet(x,y,alpha=0,lambda=grid) # default: 10-fold cross validation

plot(cv.out)

# Select best lambda
bestlam <- cv.out$lambda.min
bestlam
log(bestlam)


plot(ridge.mod,xvar='lambda',label=TRUE)
abline(v=log(bestlam))

# Lasso
lasso.mod <- glmnet(x,y,alpha=1,lambda=grid)


plot(lasso.mod,xvar='lambda',label=TRUE)

cv.out <- cv.glmnet(x,y,alpha=1,lambda=grid) # default: 10-fold cross validation

plot(cv.out)

bestlam <- cv.out$lambda.1se
bestlam
log(bestlam)


plot(lasso.mod,xvar='lambda',label=TRUE)
abline(v=log(bestlam))

predict(lasso.mod,s=bestlam,type="coefficients")

# Even with Lasso, more than two coefficients are different form zero



# Note: there are several packages and algorithm to build
# classification and regression trees, such as rpart:
set.seed(02091991)

tree.carseats_rpart <- rpart(High~.-Sales,Carseats)

# Plot:


par(mar=c(0,0,0,0))
plot(tree.carseats_rpart)
text(tree.carseats_rpart,pretty=0,use.n=TRUE)
```
